<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.System Administration and IT Infrastructure Services</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,400;0,700;1,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <header class="header">
      <div class="header__top">
        <div class="container">
          <h2 class="header__title">IT Support Course</h2>
        </div>
      </div>
      <div class="container">
        <nav>
          <ul class="header__list">
            <li>
              <a href="index.html" class="header__link"
                >1.IT Support Funtdamentals</a
              >
            </li>
            <li>
              <a href="index2.html" class="header__link"
                >2.The Bits and Bytes of Computer Networking</a
              >
            </li>
            <li>
              <a href="index3.html" class="header__link"
                >3.Operating Systems and You: Becoming a Power User</a
              >
            </li>
            <li>
              <a href="#" class="header__link header__link_active"
                >4.System Administration and IT Infrastructure Services</a
              >
            </li>
            <li>
              <a href="index5.html" class="header__link"
                >5.IT Security: Defense against the digital dark arts</a
              >
            </li>
          </ul>
        </nav>
        <h1>4.System Administration and IT Infrastructure Services</h1>
      </div>
    </header>

    <main>
      <section class="section">
        <header class="section__header">
          <div class="container">
            <h2 class="section__title">Module 1</h2>
          </div>
        </header>
        <article class="article">
          <div class="container">
            <h3 class="article__title">System Administration</h3>
            <div class="article__part">
              <h4>What is Systems Administration?</h4>
              <p>
                Before we can get into the nitty-gritty of what systems
                administration is. We need to talk about what these systems are.
                Organizations don't just run on their own. Employees need
                computers along with access to the Internet to reach out to
                clients. The organization websites needs to be up and running, a
                files have to be shared back and forth, and so much more. All of these requirements make up the IT infrastructure of an
              organization.
              </p>
              <img src="image/4/1/it-infrustructure.jpeg" alt="it infrastructure" />
              <div class="important">
                IT infrastructure encompasses the software, the hardware, and
                network and services required for an organization to operate in
                an enterprise IT environment. Without an IT infrastructure,
                employees wouldn't be able to do their jobs and the whole
                company will crumble before it even gets started.
              </div>
              <p>
                <b>Organizations employ the help of someone like a systems
                administrator to manage the companies IT infrastructure. System
                administrators</b>, or as we like to call them, sysadmins are the
                unsung heroes in an organization. They work in the background to
                make sure a companies IT infrastructure is always working.
                Constantly fighting to prevent IT disasters from happening. Notice
                all of the really hard work that sysadmins put in. Show a little
                appreciation for your sysadmin by celebrating System Administrator
                Appreciation Day worldwide. Yes, that's a real thing. In all
                seriousness, sysadmins have a lot of different responsibilities.
              </p>
              <p>
                Any company that has an IT presence needs a sysadmin or someone
                who handles those responsibilities. The role of a sysadmin can
                vary depending on the size of an organization. As an organization
                gets bigger, you need teams of sysadmins. Their responsibilities
                may be separated out into different roles with job titles like
                network administrators and database administrators. Companies like
                Facebook and Apple don't have a single person running the IT show.
              </p>
              <p>
                But in smaller companies, it's usually a single person who manages
                an entire company's IT infrastructure. In this course, we'll focus
                on how just one person, you, can single-handedly manage an IT
                infrastructure. You learn the skills you need to manage an
                organization of less than 100 people as a sole IT person. As you
                start to scale up to large organizations, you'll also need to
                level up your knowledge of systems administration. You need to
                pick up skills that allow you to automate workflows, and managed
                configurations, or computer settings automatically.
              </p>
              <p>
                Right now,
                let's focus on systems administration in a small organization. In
                the next couple of lessons, we're going to talk in detail about
                the responsibilities of a sysadmin and how that relates to a role
                of IT support specialist who handles system administration.
              </p>
            </div>
            <div class="article__part">
              <h4>Servers Revisited</h4>
              <p>
                Basically, a <b>sys admin is responsible for their company's IT
                services</b>. Employees need these IT services so that they can be
                productive. This includes things like email, file storage, running
                a website and more. These services have to be stored somewhere.
                They don't just appear out of nowhere. Any thoughts on where
                they're stored? If the answer is servers, you're correct.
              </p>
              <p>
                We talked about service in an earlier course and you've learned that
                the term service can have multiple meanings. In one course, we
                discussed how service have web content that they serve to other
                computers. In another course, we talked about how service can be
                software that perform a certain function. In this video, we're
                going to talk about service more in depth because in many cases,
                sys admins are responsible for maintaining all of the company's
                servers. If you're working as an IT support specialist and have
                systems administration responsibilities, these tasks could be
                something you will perform.
              </p>
              <p class="important">
                A server is essentially software or a
                machine that provides services to other software or machines. For
                example, a web server stores and serves content to clients through
                the internet.
              </p>
              <p>
                <b>You can access the web server through a domain name
                like google.com</b>. An email server provides email service to other
                machines and an SSH server provides SSH services to other machines
                and so on and so forth. <b>We call the machines that use the services
                provided by a server, clients</b>. Clients request the services from a
                server and in turn, the servers respond with these services. A
                server can provide services to multiple clients at once, and a
                client can use multiple servers.
              </p>
              <img src="image/4/1/servers.jpeg" alt="servers and clients" />
              <p>
                <b>Any computer can be a server</b>. I can start up a web server on my
                home computer that would be able to serve my own personal website
                on the internet for me. But I don't really want to do that because
                I have to leave my computer on all the time in order for my
                website to be available all the time. Industry standard servers
                are typically running 24/7, and they don't run on dinky little
                hardware like my home laptop. They run on a really powerful and
                reliable hardware.
              </p>
              <p>
                Server hardware can come in lots of different
                forms. There can be <b>towers</b> that sit upright that look very similar
                to the desktops we've seen. Those towers can be put in a closet or
                can sit on the table if you want them to.
              </p>
              <img src="image/4/1/tower-server.jpeg" alt="tower server" />
              <p>
                But what if you needed to have 10 servers? The towers would start
                taking up way too much space. Instead, you can use <b>rack servers</b>
                which lay flat and are usually mounted in a 90 inch wide server
                rack. If you needed even more space, you could use <b>blade servers</b>
                that are even slimmer than racks.
              </p>
              <img src="image/4/1/rack-blade-server.jpeg" alt="rack and blade server" />
              <p>
                There are other types of form factors for servers, but these are
                the most common ones. You can also customize the hardware on your
                service depending on the services. For example, on a file server,
                you'll want more storage resources so that you can store more
                files. What about connecting to our servers? Working in a small IT
                organization, you could potentially deal with a handful of
                servers. You don't want to have a monitor keyboard and a mouse for
                each of these servers, do you? Fortunately, you don't have to
                thanks to something we learned in an earlier course. We can
                remotely connect to them with something like <b>SSH</b>. Even so, you
                should always have a monitor keyboard on hand.
              </p>
              <img src="image/4/1/small-company-servers.jpeg" alt="small company servers" />
              <p>
                Sometimes when you're working, your network might be having issues
                and SSH won't be an option. The common industry practice is to use
                something known as a <b>KVM switch</b>. KVM stands for keyboard, video
                and mouse. A KVM switch looks like a hub that you can connect
                multiple computers to and control them using one keyboard, mouse
                and monitor.
              </p>
            </div>
            <div class="article__part">
              <h4>The Cloud</h4>
              <p>
                The Cloud, the magical wonderful Cloud that you hear about in the
                news that moves data across the white fluffy winds in the sky. The
                magical Cloud disperse bits of data across in the world in itty
                bitty raindrops. No, that's not how the Cloud works at all. But
                you'd be surprised how many people believe that. There's no doubt
                you've heard the term Cloud in the news or from other people. Your
                photos are stored in the Cloud, your email is stored in the Cloud.
                Cloud computing is the concept that you can access your data, use
                applications, store files, etc., from anywhere in the world as
                long as you have an Internet connection.
              </p>
              <p class="important">
                But the Cloud isn't a
                magical thing, it's just a network of servers that store and
                process our data. You might have heard the word datacenter before.
              </p>
              <p class="important">
                A datacenter is a facility that stores hundreds, if not thousands
                of servers. Companies with large amounts of data have to keep
                their information stored in places like datacenters. Large
                companies like Google and Facebook usually own their own data
                centers because they have billions of users that need access to
                their data at all times.
                Smaller companies could do this but
                usually rent out parts of a data center for their needs.
              </p>
                When you
              use the Cloud service, this data is typically stored in the data
              center or multiple data centers, anywhere that's large enough to
              hold the information of millions, maybe even billions of users.
              It's easy to see why the Cloud has become a popular way of
              computing in the last few years. Now, instead of holding onto
              terabytes of storage space on your laptop, you can upload that
              data to a file storage service like Dropbox, which stores that
              data in a managed location like a datacenter.
              <img src="image/4/1/file-storage.jpeg" alt="" />
              The same goes for your organization. Instead of managing your own
              servers, you can use Internet services that handle everything for
              you, including security updates, server hardware, routine software
              updates, and more.
              <img src="image/4/1/datacenter.jpeg" alt="" />
              But with each of these options come a few drawbacks. The first is
              cost. When you buy a server, you pay upfront for the hardware.
              That way you can set up your services like a false storage at
              potentially very little cost because you're the one managing it.
              When you use Internet services like Box, Dropbox that offer fast
              storage online, the starting cost may be smaller, but in the long
              term, costs could add up since you're paying a fixed amount every
              month. When comparing the cost of services, always keep in mind
              what a subscription could cost you for every user in your
              organization. Weigh that against maintaining your own hardware in
              the long term, and then make the decision that works best for your
              organization. The second drawback is dependency. Your data is
              beholden to these platforms. If there's an issue with the service,
              someone other than you is responsible for getting it up and
              running again. That could cost your company precious loss of
              productivity and data. No matter what method you choose, remember
              that you're still responsible for the problems that arise when
              there's an issue. If Dropbox is having an issue with your
              important user data, it's still your problem and you have to get
              it working again no matter what. To prevent a situation like that
              from popping up, you might consider backing up some critical data
              in the Cloud and on a physical desk. That way, if one system goes
              down, you have another way to solve the problem. Whether you
              choose to maintain physical servers or use cloud services, these
              are the type of things you need to think about when providing
              services to your company. Required en ​
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">System Administration Tasks</h3>
            <div class="article__part">
              <h4>Organizational Policies</h4>

              In a small company it's usually a sys admin's responsibility to
              decide what computer policies to use. In larger companies with
              hundreds of employees or more. This responsibility usually falls
              under the chief security officer. But in smaller businesses or
              shops as the IT lingo goes, the sys admin has to think carefully
              about computer security and whether or not to allow access to
              certain users. There are a few common policy questions that come
              up in most IT settings that you should know. Should users be
              allowed to install software? Probably not, you could run the risk
              of having a user accidentally install malicious software. Should
              users have complex passwords with certain requirements? It's
              definitely a good rule of thumb to create a complex password that
              has symbols, random numbers and letters. A good guideline for a
              password length is to make sure it has a minimum of eight
              characters that make it more difficult for someone to crack.
              Should users be able to view non-work related websites like
              Facebook? That's a personal call, some organizations prefer that
              their employees only use their work computer and network strictly
              for business. But many allow other users, so their employee can
              promote their business or goods on social media platforms, stay up
              to date on current events and so on. It would definitely be a
              policy that you and your organization's leaders can work out
              together. If you hand out a company phone to an employee, should
              you set a device password? Absolutely people lose their mobile
              devices all the time. If a device is lost or stolen, it should be
              password protected at the very least, so that someone else can't
              easily view company emails. These are just a few of the policy
              questions that can come up. Whatever policies are decided upon
              have to be documented somewhere. If you're managing systems,
              you'll be responsible for documenting your company's policies,
              routine procedures and more. You can store this documentation on
              internal Wiki site, file server software, wherever. The takeaway
              here is that having documentation of policies readily available to
              your employees will help them learn and maintain those policies.
            </div>
            <div class="article__part">
              <h4>User and Hardware Provisioning</h4>

              Another responsibility sysadmins have is managing users and
              hardware. Sysadmins have to be able to create new users, and give
              them access to their company's resources. On the flip side of
              that, they also have to remove users from an IT infrastructure if
              users leave the company, it's not just user accounts they have to
              worry about. Sysadmins are also responsible for user machines,
              they have to make sure a user is able to log in, and that the
              computer has the necessary software that a user needs to be
              productive. Sysadmin also have to ensure that the hardware their
              provisioning or setting up for users, is standardized in some way.
              We talked in an earlier course about imaging a machine with the
              same image, this practice is industry standard with dealing with
              multiple user environments. Not only do sysadmins have to
              standardize settings on a machine, they have to figure out the
              hardware life cycle of a machine. They often think of the hardware
              life cycle of a machine in the literal way. When was it built,
              when was it first used, did the organization buy it brand new or
              was it used? Who maintained it before, how many users have used it
              in the current organization? What happens to this machine if
              someone needs a new one, these are all good questions to ask when
              thinking about an organization's technology. Sysadmins don't want
              to keep a ten year-old computer, in their organization. Or maybe
              they do, even that's something they might have to make a decision
              on. There are four main stages the hardware lifecycle.
              Procurement, this is the stage where hardware is purchased or we
              use for an employee. Deployment, this is where hardware is set up
              so that the employee can do their job. Maintenance, this is the
              stage where software is updated and hardware issues are fixed, if
              and when they occur. Retirement, in this final stage, hardware
              becomes unusable or no longer needed. And it needs to be properly
              removed from the fleet. In a small organization, a typical
              hardware life cycle might go something like this. First, a new
              employee is hired by the company, human resources tells you to
              provision a computer for them and set up their user account. Next,
              you allocate a computer you have from your inventory or you order
              a new one if you need it. When you allocate hardware, you may need
              to tag the machine with a sticker so that you can keep track of
              which inventory belongs, to the organization. Next, you image the
              computer with the base image, next, you name the computer with the
              standardized host name. This helps with managing machines, in
              regards to the name itself, we talked about using a format such as
              user name, dash location, but other hosting status can be used.
              After that, you install software, the user needs on their machine.
              Then, the new employee starts and you streamline the setup process
              for them by providing instructions on how to log into their new
              machine, get email etc. Eventually, if the computer sees a
              hardware issue of failure, you look into it, and think through the
              next steps. If it's getting too old, you have to figure out where
              to recycle it and where to get new hardware. Finally, if a user
              leaves the company, you'll also have to remove their access from
              IT resources and wipe the machine. So that you can eventually
              reallocate it to someone else. Installing software and configuring
              settings on a new computer can get a little time-consuming. In a
              small company, you don't do it often enough where it makes much of
              a difference. But in a larger company, a time-consuming process
              just won't cut it, you have to learn automated ways to provision
              new machines, so that you only spend minutes on this and not
              hours.
            </div>
            <div class="article__part">
              <h4>User and Hardware Provisioning</h4>
              Another responsibility sysadmins have is managing users and
              hardware. Sysadmins have to be able to create new users, and give
              them access to their company's resources. On the flip side of
              that, they also have to remove users from an IT infrastructure if
              users leave the company, it's not just user accounts they have to
              worry about. Sysadmins are also responsible for user machines,
              they have to make sure a user is able to log in, and that the
              computer has the necessary software that a user needs to be
              productive. Sysadmin also have to ensure that the hardware their
              provisioning or setting up for users, is standardized in some way.
              We talked in an earlier course about imaging a machine with the
              same image, this practice is industry standard with dealing with
              multiple user environments. Not only do sysadmins have to
              standardize settings on a machine, they have to figure out the
              hardware life cycle of a machine. They often think of the hardware
              life cycle of a machine in the literal way. When was it built,
              when was it first used, did the organization buy it brand new or
              was it used? Who maintained it before, how many users have used it
              in the current organization? What happens to this machine if
              someone needs a new one, these are all good questions to ask when
              thinking about an organization's technology. Sysadmins don't want
              to keep a ten year-old computer, in their organization. Or maybe
              they do, even that's something they might have to make a decision
              on. There are four main stages the hardware lifecycle.
              Procurement, this is the stage where hardware is purchased or we
              use for an employee. Deployment, this is where hardware is set up
              so that the employee can do their job. Maintenance, this is the
              stage where software is updated and hardware issues are fixed, if
              and when they occur. Retirement, in this final stage, hardware
              becomes unusable or no longer needed.
              <img src="image/4/1/hardware-provisioning.jpeg" alt="" />
              And it needs to be properly removed from the fleet. In a small
              organization, a typical hardware life cycle might go something
              like this. First, a new employee is hired by the company, human
              resources tells you to provision a computer for them and set up
              their user account. Next, you allocate a computer you have from
              your inventory or you order a new one if you need it. When you
              allocate hardware, you may need to tag the machine with a sticker
              so that you can keep track of which inventory belongs, to the
              organization. Next, you image the computer with the base image,
              next, you name the computer with the standardized host name. This
              helps with managing machines, in regards to the name itself, we
              talked about using a format such as user name, dash location, but
              other hosting status can be used. After that, you install
              software, the user needs on their machine. Then, the new employee
              starts and you streamline the setup process for them by providing
              instructions on how to log into their new machine, get email etc.
              Eventually, if the computer sees a hardware issue of failure, you
              look into it, and think through the next steps. If it's getting
              too old, you have to figure out where to recycle it and where to
              get new hardware. Finally, if a user leaves the company, you'll
              also have to remove their access from IT resources and wipe the
              machine. So that you can eventually reallocate it to someone else.
              Installing software and configuring settings on a new computer can
              get a little time-consuming. In a small company, you don't do it
              often enough where it makes much of a difference. But in a larger
              company, a time-consuming process just won't cut it, you have to
              learn automated ways to provision new machines, so that you only
              spend minutes on this and not hours. Another responsibility
              sysadmins have is managing users and hardware. Sysadmins have to
              be able to create new users, and give them access to their
              company's resources. On the flip side of that, they also have to
              remove users from an IT infrastructure if users leave the company,
              it's not just user accounts they have to worry about. Sysadmins
              are also responsible for user machines, they have to make sure a
              user is able to log in, and that the computer has the necessary
              software that a user needs to be productive. Sysadmin also have to
              ensure that the hardware their provisioning or setting up for
              users, is standardized in some way. We talked in an earlier course
              about imaging a machine with the same image, this practice is
              industry standard with dealing with multiple user environments.
              Not only do sysadmins have to standardize settings on a machine,
              they have to figure out the hardware life cycle of a machine. They
              often think of the hardware life cycle of a machine in the literal
              way. When was it built, when was it first used, did the
              organization buy it brand new or was it used? Who maintained it
              before, how many users have used it in the current organization?
              What happens to this machine if someone needs a new one, these are
              all good questions to ask when thinking about an organization's
              technology. Sysadmins don't want to keep a ten year-old computer,
              in their organization. Or maybe they do, even that's something
              they might have to make a decision on. There are four main stages
              the hardware lifecycle. Procurement, this is the stage where
              hardware is purchased or we use for an employee. Deployment, this
              is where hardware is set up so that the employee can do their job.
              Maintenance, this is the stage where software is updated and
              hardware issues are fixed, if and when they occur. Retirement, in
              this final stage, hardware becomes unusable or no longer needed.
              And it needs to be properly removed from the fleet. In a small
              organization, a typical hardware life cycle might go something
              like this. First, a new employee is hired by the company, human
              resources tells you to provision a computer for them and set up
              their user account. Next, you allocate a computer you have from
              your inventory or you order a new one if you need it. When you
              allocate hardware, you may need to tag the machine with a sticker
              so that you can keep track of which inventory belongs, to the
              organization. Next, you image the computer with the base image,
              next, you name the computer with the standardized host name. This
              helps with managing machines, in regards to the name itself, we
              talked about using a format such as user name, dash location, but
              other hosting status can be used. After that, you install
              software, the user needs on their machine. Then, the new employee
              starts and you streamline the setup process for them by providing
              instructions on how to log into their new machine, get email etc.
              Eventually, if the computer sees a hardware issue of failure, you
              look into it, and think through the next steps. If it's getting
              too old, you have to figure out where to recycle it and where to
              get new hardware. Finally, if a user leaves the company, you'll
              also have to remove their access from IT resources and wipe the
              machine. So that you can eventually reallocate it to someone else.
              Installing software and configuring settings on a new computer can
              get a little time-consuming. In a small company, you don't do it
              often enough where it makes much of a difference. But in a larger
              company, a time-consuming process just won't cut it, you have to
              learn automated ways to provision new machines, so that you only
              spend minutes on this and not hours.: aggiunto alla selezione.
              Premi [CTRL + S] per salvare come nota. Required en ​
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Applying Changes</h3>
            <div class="article__part">
              <h4>With Great Power Comes Great Responsibility</h4>

              When you have administrative rights for something, whether it's
              one machine, a fleet of 100 machines, or a cloud service with
              thousands of users, you need to be careful that you use these
              rights responsibly. The most important thing is to avoid using
              administrative rights for tasks that don't require them. For
              example, you shouldn't browse the web as an administrator user.
              Try to minimize the time spent in an administrative session. Do
              whatever you need to do and once you're done, close the session.
              In Linux systems, we usually use the sudo command to execute
              commands as an administrator. When you execute sudo for the first
              time in a machine, you get a message like this. These principles
              apply to any administrator rights regardless of the operating
              system or service you're in charge of. Let's take a deeper dive
              into what this all means. Respect the privacy of others. Don't use
              your administrator rights to access private information that you
              have no business accessing. Having filesystem access to the
              information stored in a user's home directory doesn't mean you
              should be looking at their personal files. Being an administrator
              of an email server, it doesn't mean you should read someone else's
              email. Just because you can, doesn't mean you should. Even if you
              have a business reason to access a certain piece of information,
              make sure you follow the appropriate process or policies to access
              it. You shouldn't use your administrator rights to bypass any
              roles. Think before you type. When you use your administrator
              rights, your actions can have much greater consequences than when
              you're acting as a normal user. Think through what you're doing
              and don't rush. Mistakes like deleting the wrong set of files,
              rebooting the wrong machine, or breaking the connection that
              you're using to manage a remote machine can all happen if you're
              not careful. You can train yourself to do this by writing out the
              steps you plan to take before doing them. This helps in two ways.
              It allows you to plan ahead and serves as documentation of what
              you did. Documenting what you did is crucial when using
              administrator rights. Listing the commands you executed lets you
              repeat the same exact process in the future and fix any problems
              that may come up later. In Linux, there is a command called
              script. We can use it to record a group of commands as they're
              being issued along with their output. In Windows PowerShell,
              there's an equivalent command called Start-Transcript. The output
              of these tools is useful for automating procedures. Similarly, we
              can use the record my desktop tool to record the interaction with
              the graphical application. The last of the sudo command principle
              is, with great power, comes great responsibility. Similar cheeky,
              but the point is serious. The more you can do with the
              administrator rights, the more you can mess up. You can minimize
              the impact of any mistake and some mistakes I notable by making
              sure you can quickly revert your changes if something goes wrong.
              You can do this by making a copy of the state before changing it.
              By keeping your configuration in a version control system, or by
              documenting what steps you need to take in order to go back to the
              previous state. Reverting to the previous state is called a
              rollback. Some commands are easy to rollback than others. For
              example, if you change the configuration setting from true to
              false, the rollback is to set it back to true. But if you're
              deleting a file in a way that it doesn't keep a backup copy, the
              rollback could be tough or even impossible. Before you make a
              change, take a moment to think about what a rollback would look
              like and make sure you have copies of any information that can be
              lost. Required en ​
            </div>
            <div class="article__part">
              <h4>Recording Your Actions</h4>
              <p>
                When you are going to make changes in a machine, it’s very
                important to have a clear plan of what you are going to do and
                to store the actions that you actually took.
              </p>

              <p>
                A common practice for system administrators that work with bug
                queues or ticketing systems is to include the commands executed
                and the output obtained in the corresponding bug or ticket. This
                is recommended if the commands that need to be executed are few
                and straightforward.
              </p>

              <p>
                However, there are situations where you don’t yet know which
                commands exactly you’ll need to execute because there’s some
                investigation that needs to happen. In cases like that, it can
                be helpful to use a command like script for Linux or
                Start-Transcript for Windows.
              </p>
              <h6>script</h6>

              <p>In the case of script, you can call it like this:</p>

              <p>script session.log</p>

              <p>
                This will write the contents of your session to the session.log
                file. When you want to stop recording, you can write exit or
                press Ctrl-D. The generated file will be in ANSI format which
                includes the colors that were displayed on screen. In order to
                read them, you can use commands like ansi2txt or ansi2html to
                convert it to plain text or HTML respectively.
              </p>

              <h6>Start-Transcript</h6>

              <p>In the case of Start-Transcript, you can call it like this:</p>

              <p>Start-Transcript -Path C:\Transcript.txt</p>

              <p>
                This will write the contents of the session to
                C:\Transcript.txt. When you want to stop recording you need to
                call Stop-Transcript. The file created is a plain text file
                where the commands executed and their outputs are stored.
              </p>
              <h6>Recording Graphical Sessions</h6>

              <p>
                Performing system administration actions through a Graphical
                user interface is less common (as it’s harder to automate and to
                perform remotely), but it’s still something that may happen
                sometimes.
              </p>

              <p>
                If you are going to be performing an action that needs to be
                done graphically and you can document what you are doing, you
                can use a specialized tool like recordMyDesktop for Linux, or
                general video tools like OBS or VLC .
              </p>
            </div>
            <div class="article__part">
              <h4>Never Test in Production</h4>

              Let's start by defining what we mean by production. In an
              infrastructure contexts, we call the parts of the infrastructure
              where certain service is executed and serve to its users
              production. If you host a website, the service that deliver the
              website content to the users are the production service. Inside
              your company, the service that validate users' passwords are the
              production authentication services. You get the idea. Let's say
              you need to make an important change in your production
              infrastructure. It could be adding a new service, changing the
              configuration of an existing service, updating the operating
              system, or maybe shutting down the service that's currently
              running. How do you go about doing that? The key to safely making
              these changes is to always run them through a test environment
              first. The test environment is usually a virtual machine running
              the same configuration as a production environment, but isn't
              actually serving any users of the service. This way, if there's a
              problem when deploying the change, you'd be able to fix it without
              the user seeing it. If you're in charge of an important service
              that you need to keep running during a configuration change, we
              recommend that you have a secondary or a stand-by machine. This
              machine will be exactly the same as a production machine, but
              won't receive any traffic from actual users until you enable it to
              do so. In this case, once you've tested your changes in the test
              environment and are ready to deploy them to production, first,
              apply the changes to the secondary machine. Once the changes have
              been applied, make the stand-by machine and primary machine, and
              then apply the changes to the other machine. For even bigger
              services when you have lots of service providing the service, you
              may want to have canaries. Similar to the canary of coal miners
              carry to detect toxic gases when entering the minds, you'll use a
              small group of servers to detect any potential issues in the
              larger changes you want to push out in the system. Once you verify
              that it works correctly on those machines, you then deploy the
              change to the rest of the fleet. That way, if there's an issue
              with the change, only a subset of the users get exposed to it. You
              can roll it back before it hits everyone. Now let's say you need
              to make a minor change in your production infrastructure. Should
              you just go ahead and make the change in production? No, you
              should always try it in your test infrastructure first. It doesn't
              matter how minor the change may seem, it's always something that
              could go wrong. Whether the infrastructure needs primary and
              secondary machines or a group of canaries depends on how big the
              service is and how important it is that it doesn't go down. Even
              for the smallest services, you should never make changes directly
              in production. Always use a test instance first and only deploy
              the change to production after verifying that it works.
            </div>
            <div class="article__part">
              <h4>Fixing Things the Right Way</h4>

              As an IT support specialist, you will often be in charge of fixing
              problems. The problem could be in a user's computer, a server in
              your own infrastructure, a piece of code running in the Cloud or
              somewhere in between. So how do you go about fixing it? Let's say
              you're dealing with a problem either because you've found it or
              someone reported to you. Before you start fixing it, make sure you
              can recreate the error as you'll want to test your solution to
              make sure the problem is gone after you apply the fix. This is
              called a reproduction case and means you are creating a roadmap to
              retrace the steps that led the user to an unexpected outcome, like
              reaching an error page. When looking for a reproduction case,
              there are three questions you will need to answer. What steps did
              you take to get to this point? What's the unexpected or bad
              result? What's the expected result? Let's say that you're trying
              to fix a problem where a user can't access a website page. The
              reproduction case would be navigating to the failing site with a
              web browser. The bad result is an error page, while the expected
              result is a visible website. Once you have the steps you need to
              recreate the unexpected result and you know what the right results
              should be, you can try to fix the underlying issue. Remember,
              always do this in your test instance, never in production. Make
              sure you document all your steps and any findings. Having this
              documentation may prove invaluable if you ever had to deal with
              similar issues again. Your future self will thank you. After
              applying your fix, retrace the same steps that took you to the bad
              experience. If you're fix worked, the expected experience should
              now take place. For our page example, you can verify your solution
              by visiting the site. Once you apply the necessary fix, you should
              see the website content instead of the error page. Wow, we've come
              a long way and covered lots of different aspects of the
              responsibilities of the SysAdmin, including how they apply changes
              safely with the right amount of effort. Next, you'll practice
              these concepts using Qwiklabs.
            </div>
          </div>
        </article>
      </section>

      <section class="section">
        <header class="section__header">
          <div class="container">
            <h2 class="section__title">Module 2</h2>
          </div>
        </header>
        <article class="article">
          <div class="container">
            <h3 class="article__title">Intro to IT Infrastructure Services</h3>
            <div class="article__part">
              <h4>The Role of IT Infrastructure Services in SysAdmin</h4>
              There are lots of IT infrastructure services that keep a company
              running. In a smaller company, a single person could be
              responsible for all the services. In larger companies, teams of
              sysadmins might manage just one service. In this course, we're
              going to discuss what you'll need to set up these services as the
              sole IT person in a company. We'll also give you an overview of
              some of the Cloud services that you can utilize if you wanted
              another company to run your services. Reminder, as we mentioned
              before, Cloud services are services that are accessed through the
              internet like Gmail. We can access our Gmail accounts from any
              computing device as long as we're connected to the Internet. By
              the end of this module you should be well versed in what services
              you will need to have a functioning IT infrastructure for your
              company.
            </div>
            <div class="article__part">
              <h4>Types of IT Infrastructure Services</h4>
              We talked about physical infrastructure, components of an IT
              environment in an earlier lesson. Remember that you can set up
              different servers to run your services on like a server to run
              your file storage service. You can buy or rent hardware for these
              servers and set up and store them either on site or at another
              location essentially you manage these servers end to end. There's
              another option, if you don't want to be responsible for managing
              the hardware tasks and updating your server operating systems with
              security patches and updates. You can use the cloud alternative to
              maintain your own infrastructure which is called infrastructure as
              a service or IaaS. IaaS providers give you pre-configured virtual
              machines that you can use just as if you had a physical server.
              <img src="image/4/2/iaas.jpeg" alt="" />
              Some popular IaaS providers are Amazon web services and their
              elastic compute cloud or EC2 instances, Linode which rents out
              virtual servers. Windows Azure and Google Compute Engine which
              you've been using throughout this course. Your company's internal
              network is going to be like your network at home, you're going to
              have multiple computers that need to be on a certain sub net. You
              have to sign them IP addresses statically or using DHCP, the
              networking hardware has to be set up. Wireless internet will
              probably need to be available. DNS needs to be working etcetera.
              If your company is large networking is usually taken care of by a
              dedicated team but in smaller companies you probably be
              responsible for setting up the network. Network can be integrated
              in an IaaS provider, but in recent years it's also been branched
              off into its own cloud service Networking as a Service or NaaS.
              NaaS allows companies to offshore their networking services so
              that they don't have to deal with the expensive networking
              hardware. Companies also won't have to set up their own network
              security, manage their own routing, set up a one and private
              internets and so on.
              <img src="image/4/2/naas.jpeg" alt="" />
              Let's talk about the software that your company might want to use.
              Do you need to type out wear documents use an email client,
              communicate with other people, use operating systems, process
              spreadsheets or have any other software needed to run a business
              that, yes. The right software has to be available to your
              company's users. We've already discussed how to install and
              maintain software machines. You have to deal with things like
              licenses, security updates and maintenance for each machine. The
              cloud alternative to maintaining your own software is known as
              Software as a Service or SaaS.
              <img src="image/4/2/saas.jpeg" alt="" />
              Instead of installing a word processor on every machine, you can
              use Microsoft Office 365 or Google G Suite. These are both
              services that you can purchase that allow you to edit word
              documents, process spreadsheets, make presentations and more all
              from a web browser. Some companies have a product built around a
              software application. In this case there's some things that
              software developers need to be able to code, build and ship their
              software. First, specific applications have to be installed for
              their programming development environment then depending on the
              product, they might need a database to store information. Finally,
              if they're serving web content like a website, they'll need to
              publish their product on the internet. If you're building this
              entire pipeline yourself, you may need to set up a database and a
              web server. The programming development environment will also have
              to be installed on every machine that needs it. If you want an all
              in one solution to building and deploying a web application, you
              can use something called Platform as a Service or PaaS. This
              includes an entire platform that allows you to build code, store
              information in a database and serve the application from a single
              platform.
              <img src="image/4/2/paas.jpeg" alt="" />
              Popular options for PaaS are Heroku, Windows Azure and Google App
              Engine. The last IT infrastructure service will discuss is the
              management of users access and authorization. A directory service
              centralizes your organization's users and computers in one
              location so that you can add, update and remove users and
              computers. Some popular directory services that you can set up our
              Windows Active Directory and OpenLDAP. Directory services can also
              be deployed in the cloud using Directory as a Service or DaaS
              providers. There's a general overview of the most common IT
              Infrastructure services you'll encounter when handling system
              administration tasks. While cloud services are a great option it's
              super important that you understand how a service works and how to
              maintain before you employ the help of a cloud service. Even
              though cloud services are widely used in the industry and have a
              lot of pros, there are also some cons. These include recurring
              costs and the need to depend on the provider's service.
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Physical Infrastructure Services</h3>
            <div class="article__part">
              <h4>Server Operating Systems</h4>
              When you want to set up a server, you essentially install a
              service or application on that server like a file storage service,
              then that server will provide those services to the machines that
              request it. Maybe you thought you'd install services on a use
              operating system like Windows 10. Well, that's an option.
              Typically in an organization, you'll want to install your services
              on a server operating system. Server operating systems are regular
              operating systems that are optimized for server functionality.
              This includes functions like allowing more network connections and
              more RAM capacity. Most operating systems have version
              specifically made for servers. In Windows, you have Windows
              Server. In Linux, many distributions come with server counterparts
              like Ubuntu Server, which is optimized for server use. Server
              operating systems are usually more secure and come with additional
              services already built in. You don't have to set up these services
              separately.
            </div>
            <div class="article__part">
              <h4>Virtualization</h4>

              We discussed virtual machines in the last course, and covered how
              to set up a virtual machine on a personal computer. In this
              lesson, we're going to talk about why virtualization can be an
              important part of infrastructure services and systems
              administration. There are two ways you can run your services,
              either on dedicated hardware or on a virtualized instance on a
              server. When you virtualize a server, you're putting lots of
              virtual instances on one server. Each instance contains a service.
              There are a bunch of pros and cons to running your services on
              either of these platforms. Here's the rundown, performance, a
              service running on a dedicated hardware will have better
              performance than service running in a virtualized environment.
              This is because you only have one service using one machine as
              opposed to many services using one machine. Cost, server hardware
              can be pretty expensive. If you put a service on one piece of
              dedicated hardware and had to do that for nine other services it
              starts to add up. One of the huge benefits to virtualizing your
              service is that you can have 10 services running on 10 different
              virtual instances, all on one physical server. Here's another way
              to think about, this in a typical server, if you only have one
              service running, it's probably only taking up 10 to 20% of your
              CPU utilization. The rest of the hardware isn't being utilized.
              You could have add plenty more services to the physical server and
              still have a good threshold for resource utilization. It's cheaper
              to run several services on one machine than it is to run many
              services on multiple machines. Maintenance, service require
              hardware maintenance and routine operating system updates.
              Sometimes you need to take the service offline to do that
              maintenance. With virtualized service you can quickly stop your
              service or migrate them to another physical server. Then take as
              much time as you need for maintenance. Virtualized servers make
              server maintenance much easier to do. Points of failure, when you
              put a service on one physical machine, and that machine has
              issues, you're entering a world of trouble. With virtualized
              servers, you can easily move services off a physical machine and
              spin up the same service on a different machine as a backup. You
              can also do this with the physical server, but that could become
              costly if you count from multiple service. Pro tip, you can
              prevent a single point of failure on a physical machine if you
              have a redundant servers set up, meaning you have duplicate
              servers as a backup. As you can see, there are lots of benefits to
              using virtualized servers. Just make sure to weigh the pros and
              cons of virtualizing your servers and using dedicated server
              hardware. That way, you can make the right choice for your company
            </div>
            <div class="article__part">
              <h4>Remote Access Revisited</h4>

              Another important part of physical infrastructure services is the
              ability to connect to your infrastructure from anywhere in the
              world. In this lesson, we're going to discuss what's needed to set
              up for remote access for small organization. As a systems
              administrator or as anyone in IT support, you will want to be able
              to remotely access another server or users machines so that you
              can troubleshoot an issue or do maintenance from wherever you may
              be. In Linux, the most popular remote access tool is OpenSSH.
              We've already learned how to SSH into a remote computer in the
              last course. We talked a bit about what's needed to set up SSH,
              but we'll quickly show you how to do this. To SSH into another
              machine, you need to install an SSH client on the machine you're
              connecting from. Then install an SSH server on the machine you're
              connecting to. What you're going to do, is always go to my client
              machine and simply run this command, sudo apt-get install
              openssh-client. Let me see. Going, downloading package. Perfect.
              It looks like my client has been installed. Next, you need to
              install the openssh server on the machine you want to access.
              Remember, the SSH server is just a process that listens for
              incoming SSH connections. Let's go to the server and install the
              OpenSSH server. I'm going to do sudo app-get install
              openssh-server. Perfect. It looks like my server is up and
              running. Let's go back to the client and do a test. I do SSH into
              my server IP address with my username, ask for a password, which
              is a good thing. Perfect. As you can see, I'm connected to my
              server. One true way to test this is if I go into my desktop of my
              server, let me create a folder. Now if I go back to my server,
              which is on this window, I list the files. You can see the folder
              test and that's it. We are able to SSH into a machine from another
              machine. Not too complicated. Windows has similar tools that you
              can use. A popular tool to access the CLI remotely is WinRM or
              Putty. RDP is also popular if you want to access the GUI remotely.
              The takeaway here is that when you manage IT infrastructure, you
              can utilize tools like remote access to work on your physical
              infrastructure. You need to do a little bit of setup beforehand,
              like installing the SSH client, SSH servers and allowing remote
              desktop connections, etc. But it'll be worth it in the long run.
            </div>
            <div class="article__part">
              <h4>Remote Connections</h4>
              <p>
                Previously, you learned about the fundamentals of remote access.
                In this reading, you will learn about various methods and tools
                for connecting remotely. You will also learn about some of the
                security risks related to using remote connections.
              </p>

              <p>
                Remote connections can be used by IT Support professionals to
                troubleshoot remote systems. Remote systems may include laptops,
                PCs, workstations, servers, data center machines, and other IT
                equipment that supports remote access. Additionally, remote
                connections can be used for file transfers and terminal
                emulations. IT Support professionals often use remote access
                software to save time by eliminating the need to travel to the
                computer system’s location.
              </p>

              <p>
                Remote access software can also be used for remote and flexible
                work arrangements, which have been increasing in popularity in
                recent years. Numerous organizations have developed remote,
                hybrid, and flexible work opportunities to give employees the
                option to work from home. Through these arrangements, employers
                and employees have discovered the benefits of remote work.
                Employees save time and money by avoiding the commute to work.
                Employees also report an improvement in their work-life balance.
                Employers can save on the costs of maintaining physical offices.
                Employers can opt to expand their hiring pool far beyond their
                physical locations by hiring talent in other cities, regions,
                states, or even countries.
              </p>

              <p>
                Multiple surveys have revealed that up to 95% of employers and
                employees in the United States would like to keep remote,
                hybrid, and/or flexible work options permanently. Recently,
                Microsoft reported that 66% of employers around the world are
                adapting their workplaces to support hybrid work models (see the
                Resources section at the bottom of this reading for more
                information). Given this workplace transformation, organizations
                are likely to ask IT Support professionals to design, configure,
                manage, and/or troubleshoot remote connections for business
                networks.
              </p>
              <h5>Remote access software for IT management</h5>
              <p>
                Unlike RDP and VPN, there are some types of remote access
                software that are typically used only by IT management and other
                computer support professionals. These remote applications help
                IT Support teams manage and monitor large networks more
                efficiently.
              </p>
              <ul>
                <li>
                  <b>Secure Shell or Secure Socket Shell (SSH):</b> SSH is a
                  network protocol and suite of tools that can be used to
                  establish a secure connection between a computer and a private
                  network over the internet. SSH is included with Linux/Unix and
                  Mac Server operating systems. SSH provides identity and access
                  management protocols through robust password authentication
                  and public key authentication. SSH also encrypts data
                  transmissions over the internet. Sessions are established by
                  using an SSH client application to connect to an SSH server.
                  For security, SSH keys are used to provide single sign-on
                  (SSO) services and to automate access to servers for running
                  scripts, backups, and configuration tools. SSH is primarily
                  used by IT Support professionals to remotely manage file
                  transfers and terminal emulators on Linux/Unix systems. For
                  example, IT Support staff can use the SSH network protocol
                  tool to establish an encrypted tunnel from their computer to a
                  remote server over a network. The SSH file transfer tool can
                  then be used to transfer a file, like a firmware update
                  package, to the remote server. Finally, the SSH terminal
                  emulator can be used to issue command lines to install the
                  firmware onto the remote server.
                </li>

                <li>
                  <b>Remote Monitoring and Management (RMM):</b> RMM is used by
                  IT Support professionals to remotely monitor and manage
                  information systems. Implementing RMM involves installing an
                  RMM agent on each endpoint within a network, including
                  servers, workstations, and mobile devices. The agents then
                  send periodic status reports about the health of each endpoint
                  to IT Support staff. RMM tools also help IT Support
                  professionals proactively maintain the network by facilitating
                  the remote installation of security patches and updates. If a
                  problem occurs on an endpoint, the RMM agent will create a
                  ticket, classify the problem type and severity, and then
                  forward the ticket to IT Support staff. RMM systems enable IT
                  Support providers to improve efficiency in information systems
                  management. IT Support providers can manage and even automate
                  routine maintenance for multiple endpoints simultaneously
                  through a unified RMM dashboard.
                </li>
              </ul>
              <h5>Remote access software</h5>
              <p>
                End user remote connections to business networks can be
                established using remote access software. IT professional can
                also use this software to manage business networks remotely.
                There are multiple options available for remote access software,
                each with their own benefits and disadvantages. The following
                list provides a few options for various uses, workforce sizes,
                and network environments:
              </p>
              <ul>
                <li>
                  <p>
                    Remote Desktop Protocol (RDP): RDP is a remote protocol
                    developed by Microsoft. It is compatible with most Windows
                    and Mac operating systems. An RDP solution may work well for
                    flexible or hybrid work environments where employees split
                    their work schedule between being physically in the office
                    and working remotely. With RDP, end users can remotely
                    access the physical computers housed at their offices, in
                    addition to the desktop, software, files, and network access
                    available to those systems. IT Support professionals can
                    also use RDP software to troubleshoot, repair, patch and
                    update end user computers without needing to be in the same
                    room as the PCs.
                  </p>
                  <p>
                    RDP works by encrypting and transmitting the user’s desktop,
                    data, keystrokes, and mouse movements over the internet.
                    Users may notice delayed responses to their keystrokes and
                    mouse activity during the transmission process. RDP creates
                    a dedicated network channel and uses network port 3389 to
                    transmit this information using the TCP/IP protocol
                    standard. Unfortunately, using a single dedicated port
                    creates a security weakness that cybercriminals can target
                    for on-path attacks. Further, RDP does not enforce strong
                    sign-in credentials, which leaves RDP systems vulnerable to
                    stolen credential and brute force attacks.
                  </p>
                </li>
                <li>
                  Virtual Private Network (VPN): VPNs are often described as
                  private tunnels through the public internet. Organizations can
                  use VPNs to create encrypted connections over the internet
                  between remote computers or mobile devices and the
                  organizations’ networks. VPNs can be implemented as software
                  running on networked servers or on network routers with VPN
                  features enabled. When the employees remotely connect to their
                  VPN, they are able to access their organization’s network as
                  though they were physically in the office, eliminating the
                  need to travel to the office in person. VPNs work well for
                  small to medium sized organizations, but may not be adequate
                  for large enterprises. Additionally, VPNs might not be the
                  right solution for organizations that need to provide
                  restricted levels of network access to groups like contractors
                  or vendors.
                </li>
              </ul>
              <h5>Third party tools</h5>
              <ul>
                <li>
                  Integrated video conferencing, screen sharing, and desktop
                  management apps: Video conferencing apps like Google Meet,
                  Zoom, Microsoft Teams, Skype, etc. are growing in popularity
                  as remote work tools. Video conferencing allows two or more
                  people to meet “face-to-face” in a virtual environment. Some
                  video conferencing apps also offer screen sharing tools,
                  remote desktop control, polling tools, text messaging, meeting
                  transcripts, webinar management options, the ability to record
                  meetings, and more. The growing popularity of these tools for
                  remote work has also invited an increase in related security
                  attacks. Fortunately, the major providers of video
                  conferencing software continuously update and patch their
                  applications in response to these attacks.
                </li>
                <li>
                  File sharing and transfer platforms: Cloud storage platforms,
                  like Google Drive, Microsoft OneDrive, and Dropbox, have
                  largely replaced file transfer protocol (FTP) tools. File
                  sharing through a cloud platform provides the benefits of
                  asynchronous file transfers, file transfer and data
                  encryption, customizable security and authentication settings,
                  and the ability to file share with multiple users
                  simultaneously. File owners can share individual files,
                  folders, or entire drives. However, cloud storage might not be
                  an appropriate option for organizations affected by certain
                  privacy laws, regulations, or other security concerns. These
                  organizations can still use FTP applications based on SSH or
                  HTTPS protocols for secure file transfers over the internet.
                </li>
              </ul>
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Network Services</h3>
            <div class="article__part">
              <h4>FTP, SFTP, and TFTP</h4>

              Now that we're a little more familiar with some of the common
              aspects of physical infrastructure, let's move on to network
              services. A network service that's commonly used in organization
              is a file transfer service. Why would you want to have a service
              dedicated file transfer? Sure, you could probably carry around a
              flash drive and copy files to each machine you work on, or even
              using remote copy tools we learned in the last course. Or you
              could essentially store huge files and transfer files from one
              computer to another using the Internet. There are a few different
              file transfer protocol services that are used today. Let's take a
              quick rundown of what's out there and what they do. FTP. It's a
              legacy way to transfer files from one computer to another over the
              Internet, and it's still in use today. It's not a super secure way
              to transfer data because it doesn't handle data encryption.
              <img src="image/4/2/ftp-service.jpeg" alt="" />
              The FTP service works much like our SSH service. Clients that want
              to access an FTP server have to install an FTP client. On the FTP
              server, we install the software that allows us to share
              information located in a directory on that server. FTP is
              primarily used today to share web content. If you use a website
              host provider, you might see that they have an FTP connection
              already available for use. They can easily copy files to and from
              your website. SFTP is a secure version of FTP, so it makes sense
              to choose this option over FTP. During this SFTP process, data is
              sent through SSH and is encrypted.
              <img src="image/4/2/sftp.jpeg" alt="" />
              TFTP stands for trivial FTP. Is a simpler way to transfer files
              than using FTP. TFTP doesn't require user authentication like FTP,
              so any files that you store here should be generic and not need to
              be secure. A popular use of TFTP is to host installation files.
              <img src="image/4/2/tftp.jpeg" alt="" />
              One method of booting a computer that we haven't discussed yet is
              PXE or pixie boot, which stands for pre boot execution. This
              allows you to boot into a software that's available over the
              network. A common use case for organization that wants to install
              software over a network is to keep operating system installation
              files, but TFTP server. That way, when you perform a network boot,
              you can be automatically launched into the installer. This is a
              lot more efficient than having to carry around a USB with an
              operating system image. Depending on your usage of file
              transferring services, you might want to weigh the option we
              mentioned. If you just want to share files between your computers
              in a secure way and have a nice directory where you can access all
              the shared files instead of transferring them to your machine,
              you'll want to look at network file storage services instead.
            </div>
            <div class="article__part">
              <h4>NTP</h4>
              One of the oldest internet protocols in use today is the network
              time protocol or NTP. it's used to keep the clock synchronized on
              machines connected to a network. You've probably seen NTP
              implemented in your personal life if you've ever been in an
              airport. Airports utilized synchronized clock systems and many of
              the systems use NTP. This is because the information that you see
              on your departure and arrival screen, has to match the time that
              the air traffic controller team seems for their airplanes. If only
              NTP could solve for airport delays, in the IT world machines need
              to have accurate time across the network for a lot of reasons.
              There are some security service like Kerberos, and network
              authentication protocol that depend on the time being consistent
              across the network to work.
              <img src="image/4/2/ntp.jpeg" alt="" />
              It is important to keep the time consistent and accurate across
              your company's fleet. You can't depend on the hardware itself to
              keep consistent time, so you might want to set up an NTP server.
              There are different ways that an IT support specialist or sysadmin
              can do this for an organization. You can use a local NTP server or
              a public NTP server. To set up a local NTP server, you can install
              NTP server software on your managed server. Then, you install NTP
              clients on your machines and tell those computers which NTP
              servers to sync their time to. This is a great option, because you
              can then manage the entire process from end to end. The other way
              to set up NTP is to use a public NTP server? Public NTP servers
              are managed by other organizations that your client machines
              connect to in order to get synchronized time. This is an awesome
              way to utilize NTP without having to run a dedicated NTP server.
              But if you have a large fleet of thousands of machines, it's a
              better etiquette to be running your own NTP service. Another good
              practice is to run your own NTP server, then have that point to a
              public NTP server. This makes it so that you don't connect all
              your clients to a public NTP server, and you don't have to measure
              time synchronization. Whether you run your own NTP server or use a
              public one, NTP is an important network service that you should
              definitely integrate into your own fleet.
            </div>
            <div class="article__part">
              <h4>Network Support Services Revisited</h4>
              There are a few networks services that are used internally in an
              IT enterprise environment to improve employee productivity,
              privacy, and security. While they're pretty common, you might not
              encounter them in small organizations. We're sure that you
              encounter them at some point in your IT career. There are
              Intranets and proxy servers. An Intranet is an internal network
              inside a company, it's accessible if you're on a company's
              network. Intranets can provide a wide range of information and are
              meant to improve productivity by giving employees a greater medium
              to share information. Think of it like the company's website
              that's only accessible to people on the company network. On this
              site, documentation can be centrally located, teams can post news
              updates, employees can write two forms and start discussions and
              more. Intranets are most commonly seen in large enterprises and
              can be incredibly valuable tool for employee productivity. Another
              internal support service that's widely used is a proxy server.
              Proxy servers acts as an intermediary between a company's network
              and the Internet. They receive network traffic and relay that
              information to the company network. This way, company network
              traffic is kept private from the Internet. The internet gets
              traffic through a proxy server, but it doesn't know where it
              originally came from. It only knows the proxy. Proxy servers can
              also be used to monitor and log internal company network activity.
              They can be configured so certain websites are filtered from being
              accessed. Proxy servers are useful for providing privacy and
              security on the Internet and regulating access inside the company.
            </div>
            <div class="article__part">
              <h4>DNS</h4>
              As a super quick recap, DNS is what maps human understandable
              names to IP addresses. It's an important network service to set up
              and maintain when managing a company's IT infrastructure. You
              don't set it up correctly. No one will be able to access websites
              by their names. We don't really have to think about DNS on our
              personal computers. When you connect a brand new machine to the
              Internet and start typing in the web address, it just works
              automatically. You don't have to type in IP address or anything,
              but something is happening in the background. When you connect to
              a network, you're using the DNS server address that was provided
              by the router you connected to. It updates your network setting to
              use that DNS server address, which is usually your ISP's DNS
              server. From there, you're able to access pretty much any website.
              Why do you need to set up your own DNS servers? The DNS just works
              out of the box. Well, there's two reasons. First, if you're
              running a web service like a website, you want to be able to tell
              the Internet what IP address to reach your website. To do that,
              you need to set up DNS. The second reason is that you probably
              want to work on your server or user machines remotely. In theory,
              you could remote access into them through an IP address, but you
              could also just use an easy to remember host name. To do that, you
              need DNS to map the IP address to the host name.
            </div>
            <div class="article__part">
              <h4>DNS for Web Servers</h4>
              You might remember that we can use a web server to store and serve
              content to clients that request our services. We'll probably want
              to store website content on our web server. If clients want to
              reach our website, we need to set up DNS so that they can just
              type a URL to find us. So let's talk about how DNS gets set up for
              a website. First, we need a domain name. We can buy domain names
              like SettingUpDNSIsFun.example.com. We can purchase domain names
              like this from companies called Domain registrars like Godaddy.com
              or BlueHost.com. Once we have our domain name, we want to point
              our website files to this domain name. Our website files can be
              stored on a cloud hosting provider or we can decide to control
              this ourselves and store it on our own servers. Typically, domain
              registrars also provide cloud hosting services but they can charge
              you a monthly fee to host your web files for you.
              <img src="image/4/2/setting-dns.jpeg" alt="" />
              Pros tip, if you don't want to utilize cloud hosting services, you
              can just run your own web server. Don't forget, there are always
              pros and cons to hosting a service yourself or offshoring it
              somewhere else. Well if you're the sole IT support specialist for
              an organization, make sure to weigh all your options before
              committing to an infrastructure service. Let's assume that we do
              want to host our website files ourselves. From here, we still need
              to point our new domain name to where our web content is located.
              We can do this in two ways. Most domain registrars can provide you
              with DNS settings and you can give the IP address of where your
              content is stored. If you decide not to use your domain registrar
              to host DNS for you, then you have to set up an authoritative DNS
              server for your website.
              <img src="image/4/2/setting-dns2.jpeg" alt="" />
              Since we own the domain name and host our web content ourselves,
              it makes sense for us to have the DNS servers that know that
              information.
            </div>
            <div class="article__part">
              <h4>DNS for Internal Networks</h4>

              The other reason we might want our own DNS servers is so we can
              map our internal computers to IP addresses. That way, we can
              reference a computer by name instead of IP address. There are a
              few ways we can do this. One is using a local host file, which
              contains static IP addresses to host name mappings. Let's take a
              look at an example of this. Remember that we learned that host
              files in networking allow us to map IP addresses to host names
              manually. In Linux, our host file is called, etc/hosts. It has an
              IP address that points to 127.0.0.1, which points to a name called
              local host. This just references back to the computer. Local host
              is commonly used as a way to access a local web server. If I
              change this IP address mapping to ww.google.com, then save and
              open a web browser and type ww.google.com, it won't take me there.
              Let me show you that. I'm going to go ahead and change my local
              host to www.google.com. We're going to save this, open my web
              browser to ww.google.com. As you can see, it didn't take me
              anywhere. It just takes me back to my local computer. This is
              because a DNS query first checks our local host file, then our
              local DNS servers. If there's an entry for google.com in my host
              file, it will go to that IP address instead. Let's say, I wanted
              to access Natalie's computer at 192.168.1.5, and her host name is
              catlady.examplecompany.com. I would have to enter this in my host
              file for every single computer in my fleet. That's definitely not
              a scalable option. What's our next choice? We can set up a local
              DNS server that contains all the organization's computer names
              mapped to their IP addresses. This is a more central storage
              location for this information. Then we change our network settings
              for all our computers to use this DNS server instead of the one
              given to us by our ISP. Finally, let's look at one of the last DNS
              options we can use from internal network. It can be integrated
              with a directory service, which handles user and machine
              information in a central location like Active Directory and LDAP.
              Once we set up DNS and our directory service, it will
              automatically populate with machine to IP address mappings.
              There's no need to enter this information manually.
            </div>
            <div class="article__part">
              <h4>DHCP</h4>

              Another network service that will make your job in IT support
              easier is DHCP, a Dynamic Host Configuration Protocol. Either
              refresh on DHCP, just check out the DHCP lessons, the networking
              course. When managing IT infrastructure and you want to connect a
              computer on a network you have two options. You can grant a static
              IP address or give it a DHCP assigned IP address. When you use the
              static IP address, you have to keep track of every IP address you
              assigned a computer and manually entered in the network settings.
              If you enable DHCP, your computers will be leased an IP address
              from a DHCP server. They'll automatically get IP addresses and you
              don't have to worry about manually setting addresses.
              <img src="image/4/2/dhcp.jpeg" alt="" />
              If you ever decide you need to expand your IP address range, you
              don't have to change anything on the client machines either it
              just happens automatically. To configure a DHCP server, you'll
              need to figure out which IP range you can use to assign IP
              addresses. If you want to integrate with DNS you need the address
              of your local DNS servers what gateway you should assign and the
              sub net mask that gets used. Once you saw the DHCP server software
              you have to configure the settings with this information.
              Different DHCP server software manufacturers have different
              configuration setting layouts. So you have to investigate the
              specific one you want to use. There are a lot of popular DHCP
              server software you can use for this. Windows Server versions come
              with the DHCP service built in. Once you turn on your DHCP server
              and your clients are set to receive DHCP addresses instead of
              static IP addresses you should have working the DHCP settings. In
              the last lesson, we talked about how DNS ties in with the DHCP.
              Well, in our DHCP configuration settings we can specify DNS server
              locations. The two service then sync up and when DHCP leases out
              new addresses, DNS updates his IP address mapping automatically.
              That's a super quick overview how DHCP services are configured.
              Hopefully you can now see why DHCP and DNS are critical network
              services for your organization.
            </div>
            <div class="article__part">
              <h4>Unable to Resolve a Hostname or Domain Name</h4>

              There will be times when you're working in an IT support role and
              you won't be able to resolve or get the IP address of a website
              name. This particular problem could be tricky to identify when you
              see it. You might just think that your network connection isn't
              working. Let's go ahead and try to navigate to google.com from our
              web browser. Oh, it doesn't look like we can get to google.com.
              Let's go over some of the tools that we learned in our networking
              class that can help. First up, if you are unable to resolve a
              domain name, check that your network connection is actually
              working. You can do a quick check and ping a website that you know
              is available in oldie, but goodie is to ping www.google.com. It's
              pretty rare that Google would be down although it can happen. Let
              me go into my terminal and type in ping, www.google.com. Looks
              like we're getting responses. Let's move on to isolating another
              problem, DNS. To verify that your DNS server is giving you a
              correct address for google.com, you can use NS lookup. Remember
              that NS lookup gives us the name server of a host or domain name.
              Let me go and do that on my terminal. From here, we can rule out
              if DNS is an issue by verifying that the hostname points to a name
              server. If we copy the IP address or the results and paste it into
              the web browser, it should resolve the website name if DNS is
              working. Let's go ahead and do that. I'm going to go ahead and
              copy the non-authoritative IP address. Open my web browser. Oh, I
              see that's working. What's going on? Looks like my DNS settings
              aren't working correctly. Let's look at my ping results again. I'm
              going to go ahead to my terminal and ping www.google.com.
              Riproduci il video a partire da :1:51 e segui la trascrizione1:51
              I see that it checks an IP address different from what I have
              here. If I go to this IP address, it doesn't take me anywhere. I'm
              going to take this IP address, copy this. Riproduci il video a
              partire da :2:8 e segui la trascrizione2:08 Remember that when a
              DNS query is performed, your computer first checks host file. Now,
              if I access my host file here, I can see that I have an entry for
              www.google.com, and it points to a fake IP address. If I remove
              this line right here where it says 127.1.1.3, and save that
              configuration file, and then restart my browser. If I type in
              www.google.com, here we go. We're there and the correct DNS
              setting should be applied to www.google.com. There are some
              situations where DNS can be tricky to navigate since there can be
              many contributing factors. But as with any troubleshooting
              scenario, remember to keep isolating the problem down until you
              can get to a root cause. With time and experience, you'll learn a
              lot more about DNS and how to troubleshoot it in the real world.
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Managing System Services</h3>
            <div class="article__part">
              <h4>What do Services Look Like in Action</h4>

              We've talked about lots of services, DNS, DHCP, NTP, and others.
              As an IT support specialist, it's important to understand how the
              programs that provide these services operate so that you can
              manage them and fix any problems that pop up. These programs run
              as background processes, also known as daemons or just services.
              This means that the program doesn't need to interact with a user
              through the graphical interface or the command line interface to
              provide the necessary service. The operating system ensures that
              the program is running. Each service has one or more configuration
              files that you, as a system administrator will use to determine
              how you want the service to behave. Some services may offer
              interactive interfaces that allow a user to edit the configuration
              and inspect the current status or the usage history. Other
              services may rely on the system infrastructure for this, which
              means you will need to edit the configuration files yourself. You
              have to know how to start and stop the service and how to go
              through its logs to see any current or previous activity. Services
              are usually configured to start when the machine boots, so that if
              there's a power outage or a similar event that causes the machine
              to reboot, you won't need a system administrator to manually start
              the service. If you want to decide yourself when the service
              starts, instead of starting upon boot, you need to change the
              software configuration to make it start when you want. Similarly,
              services are usually configured to restart if they crash
              unexpectedly. If this is not how you want to set up, you may need
              to change the system configuration that handles these properties.
              There are lots of services out there and each may require specific
              knowledge regarding how to configure it and when and how to use
              it, but the general concepts related to managing and configuring
              services are the same across the board. In the rest of this
              lesson, we'll look at examples of how to do this on both Windows
              and Linux.
            </div>
            <div class="article__part">
              <h4>Managing Services in Linux</h4>

              As a system administrator, you will need to know how to look at
              the status of a running service and how to stop, start, and
              restart running services. The exact way to do this will depend on
              the operating system you're using. But the concepts are the same.
              Let's look at a very simple service Network Time Protocol, NTP.
              We've called out before that NTP allows machines to synchronize
              their clocks. Ubuntu installations include a daemon that runs on
              the machine and is in charge of synchronizing the clock using NTP.
              We can check that there's an NTP Daemon running on this machine
              using the service command, service ntp status. We can see that
              there is an NTP service and the system tells us it's running. This
              service is keeping our clock on time without us even realizing it.
              If at any point it detects that the clock has drifted, it adjusts
              the time in a very small increment. It will add or remove 0.5
              milliseconds per second until it reaches the desired time. It uses
              very small increments so that other services which depend on the
              clock to perform their tasks won't be affected by a sudden
              adjustment of the time. Under normal operating conditions, a
              computer clock would only see very small drifts from the Standard
              Time. So these very small adjustments make sense. If the daemon
              detects the time has changed more than 128 milliseconds, it
              assumes that something else is going on and will not interfere.
              Let's test this by manually modifying the date of the system to a
              date in the past. I'm going to go ahead and type in sudo date and
              give it a specified date, 2017-01-01 Riproduci il video a partire
              da :1:54 e segui la trascrizione1:54 00:00:00. That specified date
              and Enter, and then type in date. We've set the date to January
              1st, 2017 at 12:00 A.M. if we check the date after a few seconds,
              it will still be set to January 1st, 2017. A few seconds past
              midnight, it does not get adjusted. The NTP Daemon saw the change,
              but since it's more than 128 millisecond threshold, it's not
              adjusting the clock. How do we make it catch up to the present?
              There's an option in the NTP daemon that allows it to drastically
              adjust the clock when it's starting. This is because the daemon is
              expected to start very early in the process when the machine is
              booting up. There shouldn't be any time dependent services running
              at that point. If we manually restart the service now, we'll see
              that the date and time get adjusted. Let's type in sudo service
              ntp stop, date, sudo service ntp start, typing date, then Enter.
              We use the Stop action to stop the service and the Start action to
              start it back up. Immediately after starting the service, we can
              see that the date and time are set back to the present. We use the
              sudo command to stop and start the service because any user can
              check the status of the service, but only an administrator can
              cause it to stop and start. An alternative that's available in
              most services is the restart action, which does a stop followed by
              a start. Let's see how that one looks. First, let's set the date
              back to January 1st, 2017 at 12:00 A.M, and then we'll restart the
              NTP service. I'm typing sudo date, specify the time. 2017-01-01
              Riproduci il video a partire da :4:6 e segui la trascrizione4:06
              00:00:00, hit Enter, then hit date, then hit sudo service ntp
              restart, then hit date. Now you've seen how to check the status,
              start, stop, and restart service in Linux. NTP is a very simple
              service, but you can also use the same commands to manage much
              more complex services. Required en ​
            </div>
            <div class="article__part">
              <h4>Managing Services in Windows</h4>
              Like Linux, Windows also allows the system administrator to manage
              the services that are running on the system. For this example,
              let's look at the Windows update service. This service is in
              charge of detecting software updates for either the operating
              system or other installed programs, downloading them and having
              them ready to be applied to the system. Let's get the status of
              the running service using the get service command. So I'm going to
              go ahead and open Power Shell and I'm going to go ahead and type
              in get service. I'm going to type in the shorthand form for
              Windows update service, which is this. So, wuauserv is a short
              name for the Windows update service. We can see that the windows
              update service is running and can get more information about it by
              running this next command. Which is going to be get service
              wuauserv and then I want to open format list. Riproduci il video a
              partire da ::55 e segui la trascrizione0:55 Asterix. This will
              show us what type of service it is and how it's configured to run.
              It's a good way to get additional information on the service
              you're interested in. As with Linux, any user can query the status
              of the service, but only administrators can start or stop a
              service. If you try to do the next steps with the normal user
              shell, you won't be able to run the commands. Now, let's try
              stopping the service and then checking the status. For this, I'll
              open an administrative power shell and run the stop service
              command. So I'm going to go into my start and instead of clicking
              on it, I'm going to right click and then type in run as
              administrator. Yes, to the security control. Riproduci il video a
              partire da :1:41 e segui la trascrizione1:41 So now, I'm going to
              go ahead and type in stop service wuauserve. Next I want to type
              in get service wuauserve. So, this service has been stopped. In
              order to start it back up, we execute the start service command,
              which I'm going to do start service wuauserve to start the
              service. And then I'm going to type in get service, Riproduci il
              video a partire da :2:19 e segui la trascrizione2:19 To show that
              the service is now running. Finally, you can list all services
              that are registered in the system using the get service command.
              Riproduci il video a partire da :2:32 e segui la trascrizione2:32
              We can also perform these same actions graphically using the
              services management console. So I'm going to go ahead and click
              and start. Riproduci il video a partire da :2:49 e segui la
              trascrizione2:49 This console shows us all the services in the
              system. The ones that are running will say running in the status
              column, while the ones that aren't running won't say anything in
              the status column. We can find the Windows update utility at the
              end of the list. We can stop it by right clicking on it. Riproduci
              il video a partire da :3:8 e segui la trascrizione3:08 And then
              clicking stop. Riproduci il video a partire da :3:11 e segui la
              trascrizione3:11 And then right clicking again, and hitting start.
              Riproduci il video a partire da :3:17 e segui la trascrizione3:17
              And there you have it. That's how you get the status, stop and
              start services in Windows. Required en ​
            </div>
            <div class="article__part">
              <h4>Configuring Services in Linux</h4>
              On top of knowing how to query the status stop and start services
              as a system administrator, you have to know how to configure
              services to meet the needs of your organization. For example, if
              you're running a DNS server, you'll need to configure the DNS
              zones that you want to serve. If you're running a web server,
              you'll need to configure the different sites and web applications
              that you would like to have enabled. And in general, you want to
              apply any specific security and backup policies to all your
              services. The details will depend a lot on the operating system
              and the service. But let's talk about the basics that you'll need
              to know for all services. Most services are enabled as soon as you
              install them. These are programs that are shipped with the
              defaults that are good enough to safely start serving right away.
              But not all services can provide default values that are suitable
              for everyone. In some cases you will need to edit the
              configuration files before the service can go live. On Windows
              most of the configuration is stored in the registry. This can be
              modified using graphical wizards or using the set service command.
              On Linux, the configuration files for the installed services are
              located in the /etc directory. And while some software may ship
              graphical configuration editors, you typically have to edit the
              configuration files with a text editor. Let's experiment with a
              simple ftp server called vsftpd. A service that gets enabled by
              default when installed. So I'm going to go ahead and type in sudo
              apt install vsftpd to install the service. Once it's done
              installing the service is already running. We can query the status
              of the service by running service vsftpd status and see that it's
              running. This tells us that the service is already running. We can
              also verify that it's running by connecting to the ftp server with
              an ftp client. To do this, I'm going to go ahead and type in lftp
              local host. Lftp is an ftp client program that allows us to
              connect to an ftp server. When we tell it to connect to local host
              will try to connect to the ftp server running on local host. Now,
              let's try to run the ls command to list the contents of the
              current directory. So when type in ls And then type in exit.
              Riproduci il video a partire da :2:31 e segui la trascrizione2:31
              This is failing because it's requiring a username and password and
              we aren't providing them. It makes sense that the default behavior
              of the ftp server is to be locked down. If we really want to
              enable anonymous connections will have to do that explicitly.
              Let's modify the configuration file to allow anonymous
              connections. To do that I'll edit the configuration file for this
              service that's located in the /etc/vsftpd.config to change the
              anonymous enable setting from No to Yes. So we go pseudo vim
              /etc/vsftpd.config. This will open up my config file and I'm going
              to go ahead and change and the anonymous enable from No to Yes.
              Save my configuration file. By changing the value of the anonymous
              enable setting from No to Yes, we're telling the ftp server
              program that we want to allow anonymous connections. We've made
              the change but we aren't done yet. If we try to connect again, ls
              will still fail. Ls it failed. No one had exit. This will also
              happen with other services because most services read their
              configuration when they start and then keep it in memory while
              they're running. In order for our service to re-read the
              configuration, we need to tell it to reload. Reloading means that
              the service re-read the configuration file without having to stop
              and start. That way ongoing connections aren't interrupted but new
              connections will use a new configuration. Let's do this for our
              ftp service. Someone typing sudo service, vsftpd reload. Once
              we've done this, we can try to connect again and this time
              executing ls will succeed. Let's see it worked.
            </div>
            <div class="article__part">
              <h4>Configuring Services in Windows</h4>

              We've now seen how to start, stop, restart, modify configuration,
              and reload this configuration for Linux services. Let's look at
              how you can do something similar using Windows. For this example,
              we'll be using Internet Information Services, the feature offered
              by Windows to serve web pages. First, we'll need to enable this
              feature. We'll use the turn features on and off option in the
              Windows Control Panel. Click on the 'Turn Windows features on and
              off". Riproduci il video a partire da ::36 e segui la
              trascrizione0:36 This opens the Server Manager, which we can now
              use to enable Internet Information Services. I'm going to go ahead
              and click on "Next", "Next" again, 'Next" again, so total of three
              times. I'm going to go ahead and scroll down and look for web
              server IIS. I'm going to click on that, and I'm going to click
              "Add Features". Click "Next". Click "Next" again, 'Next" again,
              "Next" again, and then hit "Install". Riproduci il video a partire
              da :1:14 e segui la trascrizione1:14 I selected the web server
              option to have this service enabled on this Windows Instance. It's
              now installing all the necessary pieces to enable a web server on
              this machine. Riproduci il video a partire da :1:28 e segui la
              trascrizione1:28 It's now done installing. When we close this
              window, we notice that there is a new option on the service
              manager called IIS. We see here that we have an IIS service
              running on this server. We can configure this service by
              right-clicking on the entry, and then selecting Internet
              Information Services Manager. Then I'm going to expand on our
              server, and then click on "Sites". These are the websites that are
              handled by this service. Currently, there's only one called
              default website. Let's see what this website looks like by
              navigating to local host. I'm going to go ahead and click on
              "Internet Explorer" and type in localhost. Riproduci il video a
              partire da :2:15 e segui la trascrizione2:15 Great. Our server is
              serving the default website. Now let's add a different website to
              it. We go back to our Windows Information Services. I've created
              an example site and stored in my documents folder. Now, I'll copy
              this example site into the inetpub directory, which is the
              directory normally used to serve websites when using IIS. I'm
              going to hit "Copy". Then I'm going to go into the inet directory,
              just C colon backslash inetpub. I'm going to paste that example
              folder for my documents to the inetpub directory. Then hit
              "Continue". Security control. I've copied my website. Now, let's
              enable it in the IIS Manager console. Let's go back to the
              console. I can add a new website by right-clicking on the list of
              websites and selecting the Add Website option. I am now presented
              with a bunch of options that I need to fill in. Let's select
              example as the name of my website. Let's select the folder that I
              just copied as the physical path for the website. Riproduci il
              video a partire da :3:33 e segui la trascrizione3:33 Finally,
              let's select 8080 as the port. This last one is so that the
              default website can run in the default port, port 80, while our
              example website can run in a separate port. I set up the new
              website. IIS tells me that the website is already up and running.
              Let's see if that's correct. I'm going to go ahead and click on
              "Internet Explorer", type up localhost, Riproduci il video a
              partire da :4:5 e segui la trascrizione4:05 and type in colon port
              8080, and hit "Enter". Success. We've added a second website to
              our web server. We've now seen how to install, manage, and
              configure Linux and Windows services. In the next quick lab
              exercises, you'll be able to try these actions yourself. Have fun.
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Configuring Network Services</h3>
            <div class="article__part">
              <h4>Configuring DNS with Dnsmasq</h4>

              In large enterprise deployments, you'll probably have different
              programs serving each of the networking services that we covered
              in this module. In smaller set-ups, you may be better off having a
              centralized solution that handles all services. Let's look at
              dnsmasq, a program that provides DNS, DHCP, TFTP, and PXE services
              in a simple package. This will let us do some hands-on
              configuration of these services even if it's not as complex as
              other networking solutions. Let's start by installing dnsmasq in
              this machine. I'm going to type in sudo apt install dnsmasq.
              Riproduci il video a partire da ::40 e segui la trascrizione0:40
              Once we've installed dnsmasq, it's immediately enabled with the
              most basic functionality. It provides a cache for DNS queries.
              This means that you can make DNS requests to it and will remember
              the answers, so your machine doesn't need to ask an external DNS
              server each time you make the query. In order to check this
              functionality, we'll use the dig command, which lets us query DNS
              surface and see their answers. Let's ask how a DNS server running
              in local host for the address of www.example.com. We do this by
              running digwww.example.com @localhost. Riproduci il video a
              partire da :1:24 e segui la trascrizione1:24 The part after the @
              sign indicates which DNS server we want to use. Here we have the
              reply from our query. Our DNS server is telling us the IP address
              for the domain, example.com. How do we know that this query was
              actually answered by the service the machine is running? We can
              run the service in debug mode, so we get more information about
              what's going on behind the scenes. This isn't how you would
              normally run the service, but it's useful for understanding what's
              happening, so let's stop the DNS mass service that's running, and
              the start it in debug mode, so now I'm going to type in sudo
              service, dnsmasq stuff. Then type in sudo dnsmasq the d flag and
              then pass the q flag. Bypassing d and q, we're telling dnsmasq
              that we want to run it in debug mode, and that we want it to log
              the queries that we execute. When it starts, it prints in the
              compilation options that are enabled and the configuration files
              that are used. Now, we can query again with our friendly dig
              command. If we run the command again, we'll get the same answer,
              and we'll see the debug output in the dnsmasq console. My second
              console, now, I'm going to go ahead and type in dig
              www.example.com @localhost. This is showing us that our dnsmasq
              service received the query, forward it to the configured DNS
              server, and then reply to the original machine. If we query for
              the same host name again, we'll see that instead of asking the
              other DNS server, dnsmasq replies with the cached query. Now my
              second console, I'm going to type in again, dig www.example.com
              @localhost. If I hit "Enter", for now, dnsmasq is operating as a
              simple caching DNS server, but we can make it do more than that.
              For example, we can give it a list of host names in IPs and have
              this service give authoritative answers for them. You might
              remember that when trying to resolve a host name to an IP there
              can be serviced, that store the information about the mappings,
              which can then provide the authoritative answers, while other
              servers will only be able to forward and delegate the queries to
              the server that had the information. These files have the same
              format as the ETC host file. I created this file that lists the
              internal host that I want to be able to resolve, so we type in
              cat, myhosts. As you see, it's a very simple format. You just have
              to list which IP is associated with each host. We use the h
              parameter to tell us dnsmasq that we want to include this list in
              the information being served. I'm going to cancel this, clear, and
              I'm going to type in sudo dnsmasq -d -q -H, myhost. Now that we
              have our list of host loaded, let's query with dig. Now my second
              console, I'm going to type in dig oxygen.local @localhosts. As
              dnsmasq is authoritative about this host, there's nobody to
              forward the question to. It also lists which file is using to get
              the information, me. Finally, let's see what the output looks like
              when we ask it for information that it doesn't have. I'm going to
              type in dig hydrogen.local @localhost. We see that it replied that
              the authoritative servers or the root servers, but that it
              couldn't find any results, and what did the running dnsmasq say?
              Since the requested name isn't in the list of hosts known to our
              DNS server, it forwards the query to the configured DNS server.
              The reply for that was NX domain, which means non-existent domain.
              While dnsmasq is as simple as it gets, you've now seen what a DNS
              server looks like in action. Cool.
            </div>
            <div class="article__part">
              <h4>Configuring DHCP with Dnsmasq</h4>

              We've seen how to use dnsmasq to serve DNS queries. But as we
              mentioned before, dnsmasq can also be used for other networking
              services. Let's look at how it can be used as a DHCP server. A
              DHCP server is usually set up on a machine or a device that has a
              static IP address configured to the network interface which is
              being used to serve the DHCP queries. That interface is then
              connected to the physical network that you want to configure it
              through DHCP, which can have any number of machines on it. In real
              life, the DHCP server and the DHCP client usually run on two
              separate machines. But for this example, we'll be doing a
              simulation on one machine. In this machine we have an interface
              called eth_srv. Let's configure it to be the DHCP servers
              interface. Now I'm going to type in IP address, show eth_srv. This
              command shows us that this interface has the 192.168.1.1 IP
              address. The /24 pot indicates that this IP is in a network that
              goes from 192.168.1.0 to 192.168.1.255. The interface also has an
              IPV6 address configured, but we won't go into IPV6 for this
              example. We also have an interface called eth_cli, which is the
              interface that we'll use to simulate a client requesting an
              address using DHCP. This interface doesn't have an IP configured
              yet. I'm going to type in IP address, show eth_cli. We can see
              that this interface doesn't have an IPV4 address configured. We'll
              change this by using our DHCP server. To do this, we need to
              provide additional configuration to dnsmasq. There are lots of
              things we can configure. We're going to use a very basic set of
              options. Let's look at the configuration file. I'm going to type
              in cat dhcp.config. The interface option tells dnsmasq that it
              should listen for DHCP queries on the eth_srv interface. The bind
              interfaces option tells it not to listen on any other interfaces
              for any queries. This allows us to have more than one dnsmasq
              server running at the same time, each on its own interface. The
              domain option tells the clients the networks domain name and will
              be used for querying host names. Then we have two different DHCP
              options, which are additional information that will be transmitted
              to DHCP clients when the IP is assigned. In this case, we're
              telling clients what to configure as a default gateway and which
              DNS servers should be used. There are a lot more options that we
              can set, but these two are the most common ones. Finally, we
              configure the DHCP range. This is the range of IP addresses that
              the DHCP server can handle. Depending on your specific setup, you
              may want to reserve some of the addresses in your network for
              machines that need to have a static address. We don't plan to do
              that, you can make the range larger, but make sure you don't
              include the address of the DHCP server itself. The last value in
              the DHCP range line is the length of the lease time for the IP
              address. In this case, it's 12 hours, which means that once an
              address is assigned to a machine, it will be reserved for that
              machine for those 12 hours. If the lease expires without the
              client renewing it, the address can be assigned to a different
              machine.
              <img src="image/4/2/configiring-dhcp.jpeg" alt="" />
              We've gone through the configuration file. Let's tell dnsmasq to
              start listening for queries using this config. Now I'm going to
              type in sudo dnsmasq -d -q -c dhcp.config and then hit "Enter." We
              can see in the output that dnsmasq is listening for DHCP queries
              on the eth_srv interface with the options that we set in our
              configuration file. Now, let's run a DHCP client on a second
              terminal. I'm going to open up the second terminal. Now, my second
              terminal, I'm going to type in sudo dhclient -i eth_cli, and then
              -v for verbose. Riproduci il video a partire da :4:50 e segui la
              trascrizione4:50 We're using DHCP client, which is very common
              DHCP client on Linux. We're telling it to run on the eth_cli
              interface and we're using the -v flag to see the full output of
              what's happening. Back in the networking course, we explain the
              whole process of DHCP client getting a DHCP lease. Here we see the
              packets being exchanged and how our client got the IP address
              192.168.1.80. We also see that the DHCP client expects to renew
              the address before it expires. Let's see how our interface looks
              now. Now I'm going to type in IP address, show eth_cli. Our
              eth_cli interface has successfully acquired an IP address. Now,
              let's look at what dnsmasq printed when the request was made. We
              see the same packet exchange that we saw from the client. But
              dnsmasq also shows that it now knows the hostname of the machine
              with the address 192.168.1.80 because dnsmasq also has DNS
              capabilities. This means it will also provide this as an
              authoritative answer for local queries. Now I'm going to type in
              dig @localhost instance-1. Riproduci il video a partire da :6:16 e
              segui la trascrizione6:16 With that, we've seen how dnsmasq can
              act not only as DNS server, but also as a DHCP server. This setup
              was a simulation to show you what you can do with dnsmasq. As
              mentioned earlier, in real life, you would have this on separate
              machines, physical or virtual. If you want to test a setup like
              this, you'd normally do that on a separate network from the
              production network. Remember, never test in production. We covered
              a lot of information in this module. You've learned about the
              overall services needed in IT infrastructure. On top of that, you
              learned about the physical infrastructure services like remote
              access and virtualization that help your organization run more
              efficiently. You even learned about essential network services
              like DNS and DHCP, along with the overall picture of what's needed
              to set up DNS for an organization and why you'd want to do that.
              <video controls width="400">
                <source src="video/4/configuring-dhcp.mp4" type="video/mp4" />
                Ваш браузер не поддерживает встроенные видео :(
              </video>
            </div>
          </div>
        </article>
      </section>

      <section class="section">
        <header class="section__header">
          <div class="container">
            <h2 class="section__title">Module 3</h2>
          </div>
        </header>
        <article class="article">
          <div class="container">
            <h3 class="article__title">Software Services</h3>
            <div class="article__part">
              <h4>Communication Services</h4>

              Software services include a wide range of functions. We'll cover
              the major ones here. First up, is communication services, which
              enable employees in a company to talk to one another. Then the
              security services which add a layer of security protection to our
              IT infrastructure. We'll also discuss user productivity services
              and some of the aspects of managing software in a business that
              you probably have to think about in your work. There's lots of
              software out there that's used for inter company communication
              like email or phone communication. These are important
              communication services. But, in this video we're only going to
              discuss software that's used in instant communication. Instant
              communication has drastically changed how we communicate in both
              our personal lives and in the workplace. We can have multiple
              conversations with different people in real time using chat
              applications. You probably use something like Facebook Messenger
              on your smartphones to chat with your friends. In a business
              setting, there are similar methods of instant communication. The
              first is Internet Channel Relay, or IRC, which is a protocol
              that's used for chat messages. IRC operates in a client server
              model, so lots of IRC client software can be used to connect to an
              IRC server. IRC was widely used in the 1990's as a way to
              facilitate all kinds of chats, group chats, individual chats, and
              more. It's not as widely used today, given the wave of social
              media instant chat messages. But if you're considering setting up
              an IRC, it is a free alternative to other chat applications. Paid
              for options are another method of instant communication. There are
              a lot more sophisticated and advanced chat applications out there
              that offer enterprise support. A few popular options are HipChat
              and Slack.(Oops! HipChat was discontinued in 2019, and its users
              migrated over to Slack. ) There are also other communication
              protocols called OpenIM protocols that are widely used and
              integrated into different communication applications. One of the
              most popular communication protocol is XMPP, or Extensible
              Messaging and Presence Protocol. It's an open source protocol used
              in instant messaging applications and social networking services.
              XMPP is even used in Internet of things applications, among other
              things. A few popular and free applications that use XMPP are
              Pidgin and ADM. Instant communication is a fantastic tool you can
              use to promote team collaboration and efficiency. When managing an
              IT infrastructure, it should be one of the communication services
              that you consider implementing for your organization. They will
              definitely thank you, maybe even over instant communication.
            </div>
            <div class="article__part">
              <h4>Email Protocols</h4>
              One communication service that you're almost guaranteed to use
              today is email. We use email for a wide range of communication. In
              an enterprise setting, it's important for us as admin or so IT
              support specialists to be able to configure email services for the
              company. To do this, you need to have a domain name setup for your
              company that you can use as your email domain like Devin at
              example.com. When you send or receive email, you want to use this
              email address. There are two ways to set up email for a company.
              The first is to run your own managed server. Using this option,
              you set up the email server software on a server. Then you create
              a DNS record for your mail server. There are different DNS
              records. Remember that the A record is used for host names, but
              for email servers, we use MX for the mail exchange record. Email
              server setup can be one of the most complicated service to set up
              for us as admin. You have to get the email to actually work.
              Protect your email addresses from spam, filter out viruses, and
              more. An alternative approach to setting up your own email servers
              is to use an email service provider like Google Suite. These
              service providers allow you to create email inboxes and more by
              paying a monthly fee for every user in your organization. This
              ties you into the Gmail webmail client and allows you to access
              your email from anywhere. As long as you're connected to the
              Internet, whatever option you choose, you will have to understand
              the differences between email protocols when you set up your email
              accounts. There are lots of email protocols out there, but we'll
              only do a rundown of the more common ones you hear about POP3,
              IMAP, and SMTP. Post Office Protocol or POP Version 3 is an email
              protocol that downloads email from an email server onto your local
              device and then deletes the email from the email server. If you
              want to retrieve your email through POP3, you can only view it
              from one device. There are a few reasons why you might want to use
              POP3 to get your email. If you need to keep your email storage
              under a certain quota, POP3 is a good way to maintain that storage
              limitation. Another benefit of POP3 is privacy, your email can
              only be seen from your local device. If storage limitations and
              security are a concern for you, you might want to consider using
              POP3 over something like IMAP.
              <img src="image/4/3/pop3.jpeg" alt="" />
              Speaking of IMAP or Internet Message Access Protocol allows you to
              download emails from your email server onto multiple devices. It
              keeps your messages on the email server. This email protocol is
              one of the more popular ways to retrieve email.
              <img src="image/4/3/imap.jpeg" alt="" />
              Last up is Simple Mail Transfer Protocol, or SMTP, which is a
              protocol used for sending emails while POP3 and IMAP, and other
              protocols can be used to retrieve email. There's really only one
              email protocol for sending email, SMTP.
              <img src="image/4/3/smtp.jpeg" alt="" />
              There are lots of different email protocols that can be
              implemented depending on the email software you choose. Email
              Service is critical for any organization. Companies need to be
              able to contact clients and business partners and communicate
              internally. If you work in an IT support specialist role where you
              are handling system administration tasks, you will need to weigh
              the pros and cons of a dedicated email server or cloud email
              service. Decisions, decisions, decisions.
            </div>
            <div class="article__part">
              <h4>Spam Management/Mitigation</h4>
              <p>
                In this reading, you will learn about common spam mitigation
                strategies. Spam is defined as any unsolicited message or call
                that is sent to a large number of recipients. Spam is a
                prevalent security concern for organizations. Cybercriminals use
                spam to steal important information from victims. Excessive spam
                can slow down mail servers and even cause the servers to crash.
                IT Support professionals must know how to mitigate and manage
                spam problems.
              </p>
              <h5>Types of spam</h5>
              <p>
                There are several different types of spam. Some spam is mass
                marketing from legitimate businesses. Legitimate spam is simply
                a nuisance, especially when it is unsolicited. Other spam can be
                malicious and criminal.
              </p>
              <ul>
                <li>
                  <p>
                    Phishing emails attempt to trick recipients into providing
                    personal information, credit card numbers, login
                    credentials, etc. One famous phishing racket is the Nigerian
                    royalty scam that asks victims to help a member of a royal
                    family to move a large amount of money out of Nigeria. The
                    story includes an excuse for why the royal person cannot do
                    this for themselves and needs the victim’s assistance. The
                    cybercriminal requests the victim’s bank account information
                    for the purpose of wire-transferring the fictional royal
                    money to the victim’s account. However, the cybercriminal
                    drains all of the money from the victim’s bank account
                    instead.
                  </p>
                  <p>
                    Phishing emails can also include clickbait links, which
                    offer the victims something enticing, such as celebrity
                    gossip, tabloid scandals, lottery winnings, etc.
                    Cybercriminals even use spam to lure in victims by appealing
                    to people’s vices. Once the recipient clicks on the emailed
                    clickbait link, they become victim to a number of malicious
                    attacks. The attacks can include exposure to malware,
                    ransomware, viruses, keyloggers, trackers, information
                    phishing, and more.
                  </p>
                </li>
                <li>
                  Text spam is another method used by cybercriminals to send
                  phishing scams. Text message spam is normally less elaborate
                  than email spam. The texts often contain a brief clickbait
                  message followed by a link.
                </li>
                <li>
                  Email spoofing is a type of phishing where emails appear to be
                  from reputable companies, like banks, well-known brand names,
                  government agencies, charities, etc. The “From” address of
                  spoofed emails is forged to look like it came from the
                  reputable company. Additionally, spoofed emails often use
                  stolen company logos, verbiage, and formatting to appear
                  authentic. A couple of common email spoofing scams include:
                  <ul>
                    <li>
                      Fake job opportunities - Cybercriminals send emails with
                      fake job opportunities and ask victims to provide all of
                      the personal information that is normally requested in a
                      job application and background check. This data may
                      include the victim’s social security number,
                      government-issued ID info (e.g., driver’s license or
                      passport), current and former addresses, current and
                      former employers, etc. The goal of the cybercriminal is to
                      obtain all of the information needed to steal the victim’s
                      identity.
                    </li>
                    <li>
                      Fake credit card charges - Cybercriminals send emails that
                      appear to be purchase receipts or alerts stating a
                      business will be charging a large amount of money to the
                      victim’s credit cards for items the victim never
                      purchased. The goal is to get the victim to reply or call
                      a fake customer service line listed in the email to
                      dispute the charges. The cybercriminal, posing as a
                      customer service representative, asks the victim for their
                      personal and credit card information to look up the bogus
                      charge and pretend to cancel the fake order. Then the
                      cybercriminal will either use the stolen credit cards or
                      sell the victim’s credit card information on the black
                      market.
                    </li>
                  </ul>
                </li>
                <li>
                  Tech support scams are used to trick people into creating a
                  security weakness for cybercriminals to hijack their
                  computers. The cybercriminals introduce themselves as
                  technical support for Microsoft, Dell, or other well-known
                  computer companies. They claim that the victim’s computer has
                  been sending the company alerts about some type of fictional
                  computer problem. The cybercriminal will direct the victims to
                  change system settings or even set up remote sessions for the
                  cybercriminals to change the settings themselves. The changed
                  system settings open a door for the cybercriminals to hijack
                  the computers to steal information, install ransomware or
                  malware, or even to use the victims’ computers as a vehicle to
                  commit other crimes.
                </li>
                <li>
                  <p>
                    Call spam or robocalls mimic telemarketing-type calls to
                    collect personal information, bank or credit card numbers,
                    and other criminally useful data from victims. Robocalls are
                    also used to test databases of phone numbers to determine
                    which are legitimate numbers. The phone numbers that are
                    answered by a live human are sold to telemarketers as
                    customer leads or on the black market to cybercriminals, who
                    use the numbers as lists of potential victims.
                  </p>
                  <p>
                    One of the largest spam call scams was based out of India
                    where 700+ employees in a call center in India were arrested
                    or detained for impersonating the United States Internal
                    Revenue Service (IRS). This criminal organization targeted
                    Americans with phone calls claiming that the victim owed
                    back taxes to the IRS and must pay hundreds or even
                    thousands of dollars immediately to avoid arrest. The
                    criminal organization stole up to $150,000 USD per day using
                    this extortion scam.
                  </p>
                </li>
              </ul>
              <h5>Spam mitigation and management solutions:</h5>
              <p>
                Fortunately, many cloud platforms offer services and tools to
                help minimize these types of attacks. The following security
                measures are offered by platforms like Google Workspace. Google
                Workspace Administration Help guides are listed with each item
                below. These guides provide more information, as well as the
                instructions for implementing these security measures in Google
                Workspace.
              </p>
              <ul>
                <li>
                  DomainKeys Identified Mail (DKIM): Helps to protect victims
                  against phishing, email spoofing, and other email spam by
                  preventing sender address forgery. DKIM attaches a header that
                  contains a cryptographic private key to each email sent. This
                  key is used to verify the identity of the sender and to detect
                  if the email message was manipulated while in transit across
                  the internet. Receiving email servers will usually designate
                  emails without legitimate DKIM keys as spam. For more
                  information and instructions to implement DKIM in Google
                  Workspace, please see the article: Help prevent spoofing and
                  spam with DKIM
                </li>
                <li>
                  Sender Policy Framework (SPF): Used to control which domains,
                  email servers, and IP addresses can send emails for an
                  organization. SPF is examined by the receiving email servers
                  to verify that the domains, email servers, and IP addresses
                  from incoming emails are from approved senders. For more
                  information and instructions to implement SPF in Google
                  Workspace, please see the article: Help prevent spoofing and
                  spam with SPF
                </li>
                <li>
                  Domain-based Message Authentication, Reporting, and
                  Conformance (DMARC): Defines how the receiver should treat
                  email messages depending on the results of DKIM and SPF
                  checking. For more information and instructions to implement
                  DMARC in Google Workspace, please see the article: Help
                  prevent spoofing and spam with DMARC
                </li>
              </ul>
            </div>
            <div class="article__part">
              <h4>User Productivity Services: Agreements and Licenses</h4>

              In any organization, the software that employees need to do their
              job is the software then IT support specialist managing IT
              infrastructure needs to provide. Depending on the organization you
              might need to get your users things like software development
              programs, word processing, graphical editors, finance software,
              and so on. Whatever software you provide, there are different
              things to consider when using it in a commercial setting that
              might not have crossed your mind when you used a similar software
              personally. When you use software, you're doing so under the
              agreement of the developers license. For example, when you use
              open source software, the license agreement usually says that it's
              free to use share and modify. When software is used as a consumer,
              agreements can say that only a specific person can use the
              software. In a business or commercial setting, most software
              distributors will have a separate agreement. In most cases, you
              can buy ten licenses and any ten people in your company can use
              it. If someone leaves the company or doesn't need the software
              anymore, you can take their license and give it to someone else in
              the company. When considering software licenses, it's important to
              review the terms and agreements, then move forward with whatever
              option works best for your company. Things get a little more
              complicated when it comes to cloud software services. You might
              have to deal with some of the same stipulations and also think
              through whether to purchase added features for businesses and
              enterprises, like dedicated customer support. Whatever method you
              use to provide software, whether it's installing software on every
              machine or utilizing cloud software services, there's one thing to
              keep in mind, software used as a consumer won't be the same as
              software used as a business.
            </div>
            <div class="article__part">
              <h4>Web Server Security Protocols</h4>

              The last software services that we'll discuss are security
              services. Security is super important to all organizations. It's
              integrated into pretty much all aspects of an IT infrastructure
              service. We'll dive deeper into this in the last course on IT
              security. For now, remember that there are lots of different
              security protocols that are put in place for all sorts of things,
              keeping data encrypted, authentication, etc. If you ever manage a
              web server that serves content to other users, you want to let
              them know that when they access your website, you're keeping their
              interaction with you as secure as possible. Let's say that you
              have an online bank account that you're logging into. The URL will
              most likely begin with an HTTPS. Remember that HTTP stands for
              hypertext transfer protocol, which is used to format and transfer
              web content around the Internet. When you enter in a URL, you
              notice that the HTTP comes before everything else. HTTPS or
              hypertext transfer protocol secure is the secure version of HTTP.
              It makes sure the communication your web browser has with the
              website is secure through encryption. HTTPS is also referred to as
              HTTP/TLS or HTTP/SSL. This is because there are two protocols that
              enable us to make our web servers secure. The first is transport
              layer security protocol or TLS, which is the most popular way to
              keep communications secure over a network. TLS is widely used to
              keep web browsing secure but it can be used in a lot of other
              applications too. The second protocol is secure socket layer
              protocol or SSL. It's a way of securing communication between a
              web server and client but it's pretty old and insecure, so it's
              been deprecated in favor of TLS. You may still see it's name being
              used to refer to the TLS protocol, like SSL/TLS. The two protocols
              are often used interchangeably. In fact, SSL version 3.0 was
              essentially TLS version 1.0. But TLS's new features and updates
              have made it more secure than SSL. So if you're managing an
              organization's website on a server, how do you enable TLS on the
              server so that the site can be using HTTPS? Well, you need to get
              a digital certificate of trust from an entity called a certificate
              authority. The certificate authority grants a certificate to your
              website saying that I trust that you control the web server and
              verifies that you are who you say you are. Once it does that, you
              can install the certificate on your web server. That way, when
              users visit your site, they'll see the HTTPS in the URL instead of
              just HTTP. Think of certificates as a way to verify that something
              is trustworthy. Security is an integral part of IT and it's not
              just the responsibility of security engineers. Everyone should be
              thinking about security and all layers of your infrastructure
              should have a layer of security built upon them.
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">File Services</h3>
            <div class="article__part">
              <h4>What are file services?</h4>
              In this lesson, we're going to run down some of the file services
              we can use that will allow us to be productive as an organization.
              Employees need to be able to share files with each other, whether
              that's to collaborate or exchange information. We talked about
              shared folders in Windows in the last course. But in this lesson,
              we're going to talk about more scalable and efficient ways to
              share data. Enter file storage services. File storage services
              allow us to centrally store files and manage access between files
              in groups. You can set up a file storage server that will let
              users access a shared directory to modify, or add files and much
              more. The other way to maintain a file storage service, is by
              using a Cloud file storage provider. There are lots of providers
              that offer secure and easily managed file storage.
            </div>
            <div class="article__part">
              <h4>Network File Storage</h4>

              In the last course, we mentioned that very few file systems can be
              used across all major operating systems. FAT32 is a popular file
              system that's compatible with Windows, Linux, and Mac OS's, but it
              has severe limitations on the amount of data you can store on a
              volume. What happens if you have multiple users that want to share
              files between each other? Well, they need to store the file
              somewhere and they need to be able to retrieve the files over a
              network. Network file system, or NFS, allows us to do this. It's
              protocol that enables files to be shared over a network. The file
              system is compatible on all major operating systems. The easiest
              way to set up an NFS server is by using a Linux environment. You
              can install NFS server software, then modify the configuration
              files for the directories that you want to allow shared access to.
              Once you do that, the NFS service will be running in the
              background of the server. On each client machine that wants to
              access the server, you just mount the file system the way you
              would any other file system except, you'd use the host name
              instead of a physical disk device. From there, you can access the
              shared directory like you would any other folder on your computer.
              <img src="image/4/3/nfs-server.jpeg" alt="" />
              NFS is a good solution to file-sharing within a network. But as
              with anything on a network, heavy usage will slow down the file
              system. While NFS works with all major operating systems, they're
              still interoperability issues with Windows. If your fleet consists
              mostly Windows machines, you might want to look at using something
              like Samba. Samba services are similar to NFS. You can centrally
              share and manage file services. Also, all major operating systems
              can use a Samba file share. The only reason you might want to
              consider Samba over NFS is because it works better with Windows
              operating systems. It also includes other services that can be
              integrated with your organization, like printer services. One
              thing to note is that you may hear the term Samba or SMB. These
              two are different. SMB is a protocol that Samba implements. Fun
              fact; when you create a window shared folder, it's actually using
              the SMB protocol. Samba itself is a software service suite used
              for file services. There are lots of other file storage services
              that you can use. A relatively affordable solution for file
              storage hardware is to use a network attached storage or NAS,
              pronounced NAS. Instead of setting up a dedicated server, like you
              would other services. NAS's are computers that are optimized for
              file storage. They usually come with an operating system that
              strip down in order just to serve files over a network. They also
              come with lots of storage space. Whatever method you choose,
              central file storage and management is an important part of IT
              infrastructure for any organization.
            </div>
            <div class="article__part">
              <h4>Mobile Synchronization</h4>
              <p>
                Mobile devices present some challenges for IT professionals.
                Mobile devices are easily lost, damaged, or stolen. With mobile
                synchronization, an IT professional can easily restore all the
                data stored on a lost or damaged device to a new device. This
                reading covers mobile synchronization on devices for
                collaboration with productivity platforms.
              </p>
              <h5>Mobile synchronization as backup</h5>
              <p>
                Most mobile OS platforms have built-in ways to backup mobile
                data to the cloud.
              </p>
              <p>
                These backup methods help preserve the data carried on a mobile
                device and exchange it with other devices. They preserve the key
                types of data that a user wants to have backed up:
              </p>
              <ul>
                <li>App data</li>
                <li>Call history</li>
                <li>Contacts</li>
                <li>Settings</li>
                <li>SMS messages</li>
                <li>Pictures and videos</li>
                <li>MMS messages</li>
              </ul>
              <p>
                As a backup method, mobile synchronization allows an IT
                professional to move the user’s data seamlessly to a new device.
              </p>
              <h5>
                Mobile synchronization for collaboration and productivity
                platforms
              </h5>
              <p>
                Mobile synchronization is essential to today’s collaboration and
                productivity platforms, such as Microsoft 365 and Google
                Workspace. These are account-based platforms that allow the user
                to link familiar productivity software and apps to a particular
                user or company profile. This way one username and password
                connects the user to the files, photos, people, and content
                needed to sync across different instances of the software or
                app.
              </p>

              <h6>Sync Microsoft 365 to a mobile device</h6>
              <p>
                To sync Microsoft 365 to a mobile device, the user needs to have
                a Microsoft account. With a Microsoft account, a user can set up
                Office apps and email on an iOS or Android mobile device. This
                setup process typically involves installing and setting up the
                Outlook mobile app for email, and the Office mobile app for
                other Microsoft tools, like Word, Excel, and PowerPoint.
              </p>
            </div>
          </div>
        </article>
        <article class="article">
          <div class="container">
            <h3 class="article__title">Print Services</h3>
            <div class="article__part">
              <h4>Configuring Print Services</h4>
              While our world is moving more and more into the digital space,
              there are still aspects of our lives that require good old
              fashioned paper. Many organizations still use printers, and as an
              IT support specialist, you have to manage them as you would any
              other device. If you have a printer at home, you probably connect
              it directly to your computer. Maybe you even print over your home
              network through Wi-Fi. Some small organizations can get away with
              this sacrament of management. But most large organizations have
              lots of printers that need to be managed and large volumes of
              information that need to be printed. When managing printer IT
              infrastructure, you need to have a place to centrally manage all
              your printers. You will probably be running commercial printers
              that also can report diagnostics information like low toner
              levels.
              <img src="image/4/3/print-server.jpeg" alt="" />
              Along with managing printer centrally, you'll also need to be able
              to deploy printer drivers software so that your users can print
              from their computers. There are a few different ways that printers
              can be managed. Setting them up really depends on how many
              printers you have and how many people are in your company. In a
              small company with less than 100 people, setting up one or two
              commercial printers should be more than enough. To set up a print
              server, all you have to do is install a print service on a server.
              Most server operating systems already come with the printer
              service readily available. For example, let's look at Windows, in
              the Windows Server operating system there's a print and document
              services that can be enabled. All you have to do is add your
              network printed to the service and install the drivers for those
              printers nice and simple,right? In Linux a common print server
              that usually pre installed on machines as CUPS or Common UNIX
              Printing System. Let me show you CUPS allows you to easily manage
              printers from a simple web URL. When your print server is set up,
              you need to add the printer to the client machine. Just search by
              the printer server name and connect to the device and start
              printing. There are lots of ways you can optimize this process.
              When you start learning about directory services, we'll take you
              through how to set rules up on machines so that the printer and
              their drivers are automatically installed on a client computer.
              Riproduci il video a partire da :2:14 e segui la trascrizione2:14
              Another way you can manage printers is by using a cloud service
              provider. This allows you to match your printers through a web
              browser. It also lets your users print through a web browser so no
              setup is involved on their machines. Printer setup is pretty easy
              to do, most of it depends on what printer service you decide to go
              with. We've learned a lot about software services in our IT
              infrastructure from important communication services to security
              and now printing.
            </div>
            <div class="article__part">
              <h4>Print Services</h4>
              <p>
                IT professionals are often responsible for adding and updating
                printer drivers and settings. This may occur when a printer is
                added to a network, moved to a new location, or there is a
                software update. Along with updating drivers and settings on
                printers, IT may also be responsible for adding network printers
                to employee computers. Correct printer configuration saves time,
                supplies, and effort. This reading covers printing languages,
                basic printer configuration settings, printer sharing, printer
                security, and network scan services.
              </p>
              <h5>Printing languages</h5>
              <p>
                When choosing a print driver or troubleshooting issues with one,
                it is important to know which printing language the printer and
                computer operating system are using. Printing languages describe
                images on a screen to a printing device, so the printed output
                matches what is on screen. Printing languages are also called
                page description languages. Two of the most common printing
                languages are Printer Control Language and PostScript.
              </p>
              <p>
                Printing languages can be either device-dependent or
                device-independent. Device-dependent means both the printer and
                computer are responsible for creating parts of the printed data.
                Device-independent means that the computer is solely responsible
                for creating the printed data. It is helpful for IT to know if
                the printing languages used are device-dependent or independent
                as it can help them troubleshoot whether printing errors are
                occurring because of the driver on the computer or the printer’s
                hardware.
              </p>
              <h6>Printer Control Language (PCL)</h6>
              <p>
                Printer Control Language (PCL) is a printing language created by
                Hewlett-Packard that is used by many printer brands and computer
                operating systems. PCL is printing device-dependent because both
                the printer and computer are responsible for creating parts of
                the printed data. Because PCL is device-dependent, the output
                may not be the same on every printing device.
              </p>
              <h6>PostScript (PS)</h6>
              <p>
                PostScript was created by Adobe and is a printing language used
                by many printer brands but most commonly used in Macintosh
                systems. Unlike PCL, PostScript does not use the printer to
                create data. PostScript is device-independent, and the output is
                the same on any printer. If an error arises when PostScript is
                used, then it is usually an error with the driver on the
                computer.
              </p>
              <h5>Sharing a printer on a network</h5>
              <p>
                Printers can be shared on a network allowing multiple computers
                to access one printer across the network instead of having to be
                wired to the computer directly. IT professionals maintain and
                set up networks that include shared printers. For more
                information on sharing printers on your network read the article
                in the reference section below.
              </p>
              <h5>Network scan services</h5>
              <p>
                Network scan services allow a printer with scanning capabilities
                to create a file of a scanned image and upload or send it to a
                location on the network or in the cloud, or attach the file to
                an email and send it. Employees often need IT support for ways
                to use this type of technology. The following network scan
                services can be used for fast file uploads or attachments.
              </p>
              <ul>
                <li>
                  Email scan service allows a document to be scanned directly
                  from the printer to email.
                </li>
                <li>
                  Server Message Block (SMB) protocol allows a document to be a
                  shared resource once scanned by the printer.
                </li>
                <li>
                  Cloud services enable a document to be scanned from the
                  printer and uploaded directly to the cloud.
                </li>
              </ul>
              <h5>Printer security</h5>
              <p>
                Printer security protects access and tracks the activity of a
                print device. Printer security aims to ensure that only
                authorized users can use a printer. Setting up and monitoring
                proper security permissions falls under the job of an IT
                professional.
              </p>
              <p>
                Some basic measures for limiting access to printers and tracking
                print activity are:
              </p>
              <ul>
                <li>
                  User authentication commonly requires a user to enter a
                  username and password before completing the print job.
                </li>
                <li>
                  Badges are usually a physical card a user must scan at the
                  printer to complete the print job.
                </li>
                <li>
                  Secured prints require a user to enter a user-created code at
                  the printer to complete the print job.
                </li>
                <li>
                  Audit logs track users that have accessed the printer,
                  including the date and time of use.
                </li>
              </ul>
              <h5>Key takeaways</h5>
              <p>
                IT support professionals are often responsible for printer
                management. It is helpful to know about printing languages,
                printer configuration, networking, and security.
              </p>
              <ul>
                <li>
                  Printer Control Language is device-dependent, while Postscript
                  is device-independent.
                </li>
                <li>
                  Some basic printer configuration settings are orientation,
                  print quality, tray settings, and duplex.
                </li>
                <li>
                  Having a printer on a network enables multiple users to share
                  printers.
                </li>
                <li>
                  Network scan services allow a printer with scanning
                  capabilities to create a file of a scanned image and upload or
                  send it to a location on the network, on the cloud, or email.
                </li>
                <li>
                  Printers have security and tracking features such as user
                  authentication, badges, secured print, and audit logs.
                </li>
              </ul>
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Platform Services</h3>
            <div class="article__part">
              <h4>Web Servers Revisited</h4>
              Platform services provide a platform for developers to completely
              build and deploy software applications, without having to deal
              with OS maintenance, server hardware, networking or other services
              that are needed to use the platform tools. A web server that we
              deploy our web applications to or the development software that we
              used to code our applications, are both examples of platform
              services. In this day and age, most businesses have a digital
              presence. Whether there's a website that promotes their business
              or even a website that is their business, businesses that run web
              services keep their services stored on a web server. A web server
              stores and serves content to clients through the Internet. You can
              access web servers using a domain name like google.com. A web
              server itself stores web bars and runs an HTTP service or HTTP
              server that processes HTTP requests. Remember that HTTP is how the
              web formats and transfers web pages. Riproduci il video a partire
              da :1: e segui la trascrizione1:00 You can think of the web server
              as the physical server that stores web files and the HTTP server
              software. When your web browser makes a request to fetch a web
              page from a URL, it sends an HTTP request that gets processed by
              the HTTP server. Then the HTTP server sends out an HTTP response
              with the content that you requested. There are a lot of popular
              HTTP server software out there, but the most widely used is the
              Apache HTTP Server, most commonly referred to as Apache. Apache is
              free and open source. It helps serve a large percentage of web
              pages on the Internet. Let's actually see how a web server serves
              content to the web. I'm going to install the Apache web server
              software on my Linux computer here. You don't have to understand
              the specifics of the setup, I just want you to see how easy it is
              to run a web service. So let me go ahead and install Apache. So
              I'm going to go ahead and do Sudo apt-get install apache2 and then
              hit the flag yes, to accept all my packages. Perfect, now, our web
              server service is running on our machine. We're actually able to
              start hosting web content. The machine that we're hosting our
              content on is, well, this machine right here. Remember that our
              computer has an IP address that's associated with itself 127.0.0.1
              or a host name of local host. Local host itself is reserved for
              this purpose. So it's not possible to get the domain name local
              host. So now that we know our machines location, let's enter it to
              the web browser. And here it is, our local web server content
              running on our machine. The files we see here come with the
              default Apache installation. But if we wanted to upload our own
              web content, we can just navigate to the directory where this is
              stored and replace it with our web content. Remember that since
              this content is hosted on our local machine, we will need to use
              DNS to let the world know that our web server exists. That's a
              quick rundown of how web servers work. System administrators
              aren't responsible for creating the content that gets served, but
              they might be responsible for making sure that content is
              available. If you're an IT Support specialist with a web service
              that needs to be managed, you should have a pretty good
              understanding of how it works.
            </div>
            <div class="article__part">
              <h4>Load Balancers</h4>
              <p>
                In this reading, you will learn about load balancers and their
                importance in cloud computing. You will become familiar with
                load balancing components and the benefits of utilizing load
                balancers.
              </p>
              <p>
                IT Support professionals who manage cloud environments and/or
                physical servers in enterprise networks will likely need to
                configure, manage, or troubleshoot load balancers. Load
                balancers monitor and route network traffic flowing to and from
                a pool of physical or virtual servers. Load balancers can be
                hardware (e.g., load balancing routers) or software (e.g.,
                Citrix ADC Virtual Platform). Load balancers distribute the
                traffic evenly, or by customized rules, across multiple servers.
                This function maximizes server performance and prevents the flow
                of traffic from overwhelming any one server and its resources.
                Basic server resources normally include CPUs, RAM, and network
                bandwidth. Servers can also offer other resources, like
                applications, file servers, database services, and more.
              </p>
              <p>
                Load balancers can also detect when a server has failed and can
                reroute and balance network traffic across the remaining
                servers. This important business continuity and reliability
                function is often referred to as high availability.
                Additionally, load balancers provide IT Support professionals
                with the ability to add and remove servers to the pool as
                needed.
              </p>
              <h5>Load balancing terminology</h5>
              <p>
                The following short glossary includes some common terminology
                for several concepts related to load balancers:
              </p>
              <ul>
                <li>
                  Client: A computer or program that sends requests to a server.
                  For example, a client could be a browser that requests a web
                  page from a web server. It could also be a workstation
                  requesting a file from a file server.
                </li>

                <li>
                  Host/node: A physical or virtual server that receives network
                  traffic from an Application Delivery Controller (ADC). The
                  server is identified by its IP address. Whether the server is
                  called a “host” or a “node” depends on the terminology used by
                  the vendor of the load balancing solution.
                </li>

                <li>
                  Member: A host/node that receives network traffic on a
                  specified TCP port. The host/node is identified by its IP
                  address plus the TCP port of the app that should receive
                  network traffic.
                </li>

                <li>
                  Pool/cluster/farm: A grouping of hosts/nodes or members that
                  offer similar services, such as application or web services.
                </li>

                <li>
                  Application Delivery Controllers (ADC): Physical appliances,
                  virtual appliances, or software that provide load balancing
                  services by managing traffic between clients and host/node or
                  member pools. ADCs can also provide other important services
                  such as security and encryption.
                </li>

                <li>
                  Path-based routing: Routes network traffic based on URL paths.
                </li>

                <li>
                  Listener: A software process that checks network traffic for
                  client requests and forwards them to target groups.
                </li>

                <li>
                  Open Systems Interconnection (OSI) model: Model that depicts
                  the seven layers of computer data communications:
                  7-application, 6-presentation, 5-session, 4-transport,
                  3-network, 2-data, and 1-physical.
                </li>

                <li>
                  Front end: In load balancing environments, the front end can
                  include the ADC system and any virtual servers that act as
                  proxies for client communications with the ADC system and the
                  back end servers.
                </li>

                <li>
                  Back end: In load balancing environments, the back end
                  normally includes the pool/cluster/farm systems. The back end
                  can also include disk storage systems.
                </li>

                <li>
                  Distributed applications: Software stored on cloud platforms
                  or physical servers that can run on multiple networked
                  computers at the same time.
                </li>

                <li>
                  Containerization: Isolated runtime environments that can
                  deploy and run distributed applications through application
                  virtualization. This method is faster and is more scalable
                  than older load balancing solutions.
                </li>

                <li>
                  Availability Zones (AZs): Regional data centers that host
                  cloud platforms and are configured for high availability.
                </li>

                <li>
                  Elastic Load Balancer (ELB): Enables the use of more than one
                  Availability Zone.
                </li>

                <li>SSL/TLS: Network protocols for encrypted communication.</li>
              </ul>
              <h6>Example ADC process for load balancing</h6>
              <img src="image/4/3/load-balancing-transaction.png" alt="" />
              <p>
                The following steps are an example of one possible load
                balancing configuration using an ADC solution:
              </p>
              <ol>
                <li>
                  [Blue arrows] The client sends a connection and an information
                  request to the ADC service.
                </li>

                <li>
                  [Blue arrows] The ADC listener detects and accepts the
                  connection. Then the ADC load balancing service analyzes the
                  best host (or member) routing path for the client request. The
                  ADC changes the destination IP to the address (and possibly
                  the TCP port) of the selected host (or member).
                </li>

                <li>
                  [Green arrows] The host or member approves the client
                  connection and routes a response to the client through the
                  ADC.
                </li>

                <li>
                  [Orange arrows] The ADC changes the source IP (and TCP port,
                  if applicable) to a virtual server IP (and port) before
                  forwarding the response to the client. The clients will
                  continue to use the IP address of the virtual server for
                  further communications.
                </li>
              </ol>
              <h6>Load balancing types</h6>
              <ul>
                <li>
                  Application Load Balancer: Operates at the application layer
                  (HTTP and HTTPS) of the OSI model. Application load balancers
                  also scan traffic for HTTP errors and coding bugs, as well as
                  guard applications against distributed denial-of-service
                  (DDoS) attacks.
                </li>

                <li>
                  Network Load Balancer: Operates at the transport layer
                  (TCP/UDP) of the OSI model. Network load balancers can route
                  millions of client requests per second and handle volatile
                  workloads. Network load balancers also support static IP
                  addressing and containerization, among other services.
                </li>

                <li>
                  Classic Load Balancer: Can operate at either the application
                  layer (HTTP/HTTPS) or the transport layer (TCP/SSL). Classic
                  load balancers use fixed ports for communication.
                </li>

                <li>
                  Gateway Load Balancers: Operates at the network layer (IP) of
                  the OSI model. Gateway load balancers have listeners on all
                  ports that scan every IP packet in the network traffic and
                  route each request to the target pools, as defined by the
                  listener configuration. A gateway load balancer is the only
                  point of entry and exit for network traffic.
                </li>
              </ul>
              <h6>Load balancers in cloud environments</h6>
              <p>
                In cloud environments, load balancing across virtual servers is
                configured through the cloud platform. A few of the load
                balancing options offered by several top cloud platforms
                include:
              </p>
              <ul>
                <li>
                  Google Cloud: Google offers an array of options for load
                  balancers, such as application and network level load
                  balancing, software-defined load balancing, multi-region
                  failover, and seamless autoscaling. Google Cloud also offers
                  external, internal, global, and regional load balancing. For
                  security measures, the load balancers are integrated with
                  Google Cloud Armor, which protects against distributed
                  denial-of-service (DDoS) attacks.
                </li>

                <li>
                  Amazon Web Services (AWS): AWS offers three ELB solutions: an
                  Application Load Balancer, a Gateway Load Balancer, and a
                  Network Load Balancer. AWS ELBs provide security through user
                  authentication, certificate management, and SSL/TLS
                  decryption.
                </li>

                <li>
                  Microsoft Azure: Operates at the transport layer of the OSI
                  model. Azure load balancer is the only front end point for
                  accepting client requests to route to the back end server
                  pools. The backend pool may consist of Azure Virtual Machines
                  (VMs) or instances running in Azure virtual machine scale sets
                  . Azure offers public load balancers for internet traffic and
                  private/internal load balancers for private virtual networks.
                  Azure’s Standard load balancer uses the zero trust security
                  model.
                </li>
              </ul>
              <h6>Load balancers in physical environments</h6>
              <p>
                In physical environments, such as server rooms and data centers,
                load balancing can be configured across multiple servers with
                operating systems like VMware. Network traffic loads can also be
                configured for smaller environments across two servers in a
                physical active-active cluster. In active-active clusters, both
                servers actively handle network traffic simultaneously.
              </p>
            </div>
            <div class="article__part">
              <h4>What is a database server?</h4>

              When you run a service that operates on the web you need to have a
              web server that serves web pages to clients that request it like
              we just covered. But you may also need to store information. Have
              you ever thought about what happens to your information when you
              create an account online for a website? Where do they store that
              info? Do they put it in a folder on a web server? If they do you
              need to stop using that service immediately. Customer information
              like news articles, videos large amounts of text image or audio
              files generally get stored in a database. Databases allow us to
              store, query, filter, and manage large amounts of data. When you
              build a web product you'll probably store the data in a database.
              Database service consists of database software that's running that
              you're able to read and write from. Common database systems like
              MySQL and PostgresSQL are widely used in application and web
              development and data analytics. These database systems usually
              require knowledge of special languages or syntaxes to be able to
              parse and filter through large amounts of data. Administrating and
              managing a database can be incredibly complex, losing precious
              data could cost the company dearly. There's actually an entire job
              specialization within IT that deals with databases just like that
              called database administrators.
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Troubleshooting Platform Services</h3>
            <div class="article__part">
              <h4>Is the website down?</h4>

              Web service and service in general are prone to breakage. Just
              like any other machine, troubleshooting web server could involve
              lots of different variables. We won't discuss a specific
              troubleshooting scenario in this lesson, but we'll talk about some
              easy troubleshooting tools you can use to diagnose a 40 web server
              or browser, called HTTP status codes. When we want to go to
              google.com, our browser is sending an HTTP request to the HTTP
              server on the web server. In turn we get an HTTP response.
              Sometimes this response returns the content that we want. Almost
              all the time, it will return a status message of the response.
              HTTP status codes are codes or numbers that indicate some sort of
              error or info messages that occurred when trying to access a web
              resource. Knowing common HTTP status code, comes in handy when
              you're troubleshooting a website error. They usually tell you
              useful information that can help you isolate the root cause.
              Here's a common HTTP status code you might recognize, the dreaded
              404 not found. A 404 error indicates that the URL you entered
              doesn't point to anything. Let's see what happens if I type in
              google.com/asdf. I get this error message. The requested URL/asdf
              was not found on this server. That's exactly what I expected to
              happen. I typed in an address I knew didn't exist and the website
              confirmed it for me. But how do we know it's a 404 error code?
              Depending on the website, HTTP error messages could be displayed
              right on the page when you try to access it. However, to be
              absolutely sure, you can just view the HTTP response itself. To do
              that, we'll have to do a bit of work. Browsers today have built in
              tools that help people diagnosed issues with the web browser or
              website itself. Since I'm using chrome, I'm going to use the
              chrome developer tools. Let me go ahead and do that. So I want to
              click on this. I get into Tools, and then click on Developer
              Tools. This will open up the developer tools side by side to my
              web browser. Developer tools is a great resource for testing and
              debugging issues with the website or browser. We just want to see
              the HTTP response code. To get to that, I'm going to go to the
              Network tab here, and refresh my page. Riproduci il video a
              partire da :2:23 e segui la trascrizione2:23 If I try to go to
              google.com/asdf, I'll see the request I made in the left hand side
              here. If I click that, I'll see the status code says 404 not
              found. Pretty neat, right? HTTP status codes that start with 4xx
              indicated an issue on the client side. The client tried to do
              something that I couldn't, like enter a bad URL. Access something
              it wasn't authorized to do etcetera. The other common HTTP status
              codes you might see start with 5xx. These errors indicate an issue
              on the service side, the web server that hosts this web content is
              experiencing issues and hopefully there's some administrators are
              looking into it. HTTP status codes tell us more than just errors.
              They can also tell us when our request is successful, which is
              denoted by the codes that begin with 2xx. HTTP status codes can
              tell us a lot about an issue with the website. If you encounter
              one that you aren't familiar with, just look it up. It will
              probably tell you exactly what the issue is. Required en ​
            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Managing Cloud Resources</h3>
            <div class="article__part">
              <h4>Cloud Concepts</h4>

              Now that you know more about the different services you can host
              on a Cloud provider, let's talk about how to make the most of the
              Cloud for your organization. As we've mentioned in previous
              lessons, when we say that a service is running in the Cloud, we
              mean that it's running somewhere else, either in a data center or
              in other remote servers. These data centers, house a large
              assortment of machines, and different types of machines are used
              for different services. For example, some machines may have local
              solid-state drive, SSD for increased performance, while others may
              rely on virtual drives mounted over the network to lower costs.
              When you use software as a Service or SaaS, the software is
              already pre-configured and the user isn't deeply involved in the
              Cloud configuration. If you choose a Cloud email solution like
              Gmail, a Cloud storage solution like Dropbox, or a Cloud
              productivity suite like Microsoft Office 365, there are only a
              small number of options for you to select or customize. The Cloud
              provider manages everything related to the service for you,
              including deciding whether VMS are hosted, ensuring that has
              enough capacity to serve your needs, falling back as frequently
              and reliably and more. When you use Infrastructure as a Service or
              IAAS, on the other hand, you're hosting your own services in the
              Cloud. You need to decide how you want the infrastructure to look,
              depending on what you want to run on it. For example, you need to
              decide which of the many available machine types you'll use and
              what kind of storage they'll need. Pro tip; start small, then
              select more powerful instances as needed. When you set up Cloud
              resources, you need to consider regions. A region is a
              geographical location containing a number of data centers. Each of
              these data centers is called a zone, and each zone is independent
              of the others. If one of them fails for some reason, the others
              are still available and services can be migrated without visibly
              affecting users. Large Cloud providers usually offer their
              services in lots of different regions around the world, and which
              region you choose will mostly depend on where your users are
              located. Smaller Cloud providers may offer fewer regions, but they
              may be the only provider with the availability in your city or
              country. It doesn't matter where exactly the data center is
              located, but users may experience more latency if it's further
              away from them. You may also hear about public, private and hybrid
              Clouds. The Public Cloud is what we call Cloud services provided
              to you by a third party. The name refers to the fact that the
              Cloud providers offer his services to the public. When your
              company owns the services and the rest of your infrastructure-
              whether on-site or in a remote data center, we call that the
              private cloud, and the hybrid cloud is a mixture of both public
              and private Cloud. Some workloads are run on servers owned by your
              company, while others are run on servers owned by a third party.
              The trick to making the most of the hybrid Cloud is ensuring that
              everything is integrated smoothly, so you can access, migrate, and
              manage data seamlessly no matter where it's hosted.
            </div>
            <div class="article__part">
              <h4>Typical Cloud Infrastructure Setups</h4>

              Let's say you have a web server providing a website to clients. In
              a typical setup for this kind of service running in the cloud, a
              number of virtual machines will be serving this same website. A
              load balancer ensures that each VM receives a balanced number of
              queries. Whenever there's a request for your website, a different
              VM will be picked to serve the response. These types of services
              are usually configured to spin up more virtual machines when there
              are a lot of queries and to shut down some of the VMs when the
              number of queries goes down. This capability is called
              autoscaling. It allows the service to increase or reduce capacity
              as needed, while the service owner only pays for the cost of the
              machines that are in use at any given time. Since some machines
              will shut down when the demand is lower, their local disks will
              also disappear and should be considered ephemeral or short lived.
              If you need data persistence, you have to create separate storage
              resources to hold that data and connect that storage to the VMs.
              Usually VMs operating websites or web services are connected to a
              database also running in a cloud. This database is also served by
              multiple machines behind a load balancer, but this is managed by
              the cloud provider and doesn't concern the cloud user. To make
              sure the service is running smoothly, you can set up monitoring
              and alerting. When you do this, you can detect and correct any
              problems with your service before your users even notice. Most
              cloud providers include monitoring and alerting solutions as part
              of their services. You can configure when and how you want to be
              alerted if the monitoring infrastructure detects performance
              issues. It may seem tricky to set up cloud resources, but most
              providers make them easy to configure.
            </div>
          </div>
          <div class="article__part">
            <h4>When and How to Choose Cloud</h4>

            As an IT support specialist, you might have to decide when a Cloud
            service is a better choice than using your own physical hardware.
            We've touched on this already, but let's dive deeper into what to
            consider when you're making this decision. Using Cloud
            infrastructure doesn't require a large upfront investment. It's a
            good choice if you aren't sure how long you need it. If you're
            setting up temporary infrastructure or trying something that might
            not last, a Cloud service may be your best option. Choosing to use
            Cloud infrastructure also makes sense when you have demand that
            varies greatly throughout the year. If you operate a website that
            gets a lot of traffic during one season, but much less for the
            remainder of the year, you wouldn't want to invest a lot of
            infrastructure just to have it sit idle most of the time. Another
            reason to choose the Cloud is geographical location. If your users,
            employees of your company or external users of your services are
            distributed around the world, having all of your servers on-site
            won't satisfy their needs. You'll want to use a provider that has
            data centers in or close to the locations that you want to serve.
            Let's say you decided that your use case fits the Cloud model. How
            do you decide which are the many Cloud providers to use? Take a look
            at your specific needs and compare the services offered by the
            various providers, and then figure out which one best suits your
            needs. Most Cloud providers offer free trials. It's a good idea to
            test them out to see if they meet your needs, to check how well your
            company's infrastructure integrates with the cloud providers.
            Finally, the technology in this space is evolving quickly. There are
            more services and solutions offered by cloud providers every year to
            make sure you're up-to-date about the latest changes in the field
            before making a decision. That's it. Now you've got a better idea of
            what to think about when you want to move your services to the
            Cloud.
          </div>
          <div class="article__part">
            <h4>Common Cloud Models</h4>
            <p>
              The cloud is a part of everyday life in the modern internet world.
              It gives users a place to work, access, and store data from any
              system plugged into the cloud. Being able to work with the cloud
              is a vital skill in IT. This reading will cover common cloud
              services and four types of cloud computing.
            </p>
            <h5>Types of cloud services</h5>
            <p>
              Companies use cloud services to provide access to internal tools,
              develop software, store data, and more. The three primary cloud
              services are Software as a service, Platform as a service, and
              Infrastructure as a service. The Google Cloud Platform is a prime
              example of a system that employs all three types of cloud
              services.
            </p>
            <img src="image/4/3/cloud-models.jpg" alt="" />

            <h6>Software as a Service (SaaS)</h6>
            <p>
              SaaS providers allow users to use their software with an internet
              browser or application instead of having to download software to a
              specific device. Users access information from any device through
              a login. The SaaS vendor stores all user data and files online
              instead of on the user’s physical equipment. SaaS typically uses a
              subscription model for its services. Hacking is a concern when
              using this service since the full-service run in the cloud.
            </p>

            <h6>Platform as a Service (PaaS)</h6>
            <p>
              PaaS offers computer hardware and software in the cloud that
              allows users to develop and deploy applications or cloud based
              services. PaaS makes buying, developing, configuring, managing,
              and installing software and hardware unnecessary.
            </p>

            <h6>Infrastructure as a Service (IaaS)</h6>
            <p>
              IaaS provides an IT infrastructure to a company over the internet
              and on-demand. IaaS provide access to things like virtual
              machines, containers, networks, and storage. This service reduces
              the need to purchase expensive hardware. IaaS allows companies to
              centralize infrastructure for faster disaster recovery.
            </p>
            <h5>Additional cloud services</h5>
            <p>
              The following cloud services are more narrow in focus and are
              designed to solve unique problems.
            </p>
            <h6>VPN as a Service (VPNaaS)</h6>
            <p>
              VPNaaS secure networks through a cloud-based connection between
              the employee and the organization’s network. Using this approach
              eliminates the need for a physical VPN endpoint.
            </p>

            <h6>Function as a Service (FaaS)</h6>
            <p>
              FaaS is an event-based service that lets developers do the
              building, running, and managing functions directly in the cloud
              without needing to maintain a server. Event-based systems use an
              event, such as a website click, to trigger communication within a
              system.
            </p>

            <h6>Data as a Service (DaaS)</h6>
            <p>
              DaaS provides data access as a service to a business. It manages
              the data companies generate and uses APIs to deliver data from
              various sources on demand. DaaS allows companies to organize and
              access the data they need. DaaS monetize by providing access to
              data. By increasing accessibility to data, DaaS can lower the cost
              of data-driven decision making, remove personal bias in data
              collection, and innovation.
            </p>

            <h6>Blockchain as a Service (BaaS)</h6>
            <p>
              BaaS is a newer and increasingly mainstream cloud model that uses
              a non-centralized system. This model uses encrypted, connected
              blocks of information for higher security than standard cloud
              services. BaaS is used to store smart contracts and high-security
              documents. This model authenticates users without needing
              additional applications. SaaS services may adapt BaaS as a
              standard feature to address the risk of hacking.
            </p>
            <h5>Four types of cloud computing</h5>
            <p>
              Cloud computing is the delivery of computing services like the
              cloud services mentioned above. There are four main types of cloud
              computing:
            </p>
            <ol>
              <li>
                Public clouds: cloud environments created from IT infrastructure
                owned by a provider such as Google Cloud or Amazon Web Services.
                Public clouds host the data of multiple companies. Be aware that
                public clouds do not provide absolute security for the
                information it stores.
              </li>

              <li>
                Private clouds: serve a single business or organization. The
                cloud runs behind an internal firewall. Private clouds can be
                deployed and managed by a third-party vendor.
              </li>

              <li>
                Multiclouds: involve using more than one cloud service from more
                than one vendor. These can be private or public.
              </li>

              <li>
                Hybrid clouds: blend at least two public or private cloud
                services and connects them with internal networks, such as local
                area networks or VPNs.
              </li>
            </ol>
            <p>
              Cloud services and cloud computing work together to meet the needs
              of companies and organizations.
            </p>

            <h5>Key Takeaways</h5>
            <p>Companies use the cloud for many tasks and services.</p>
            <ul>
              <li>
                The three primary cloud services are SaaS, PaaS, and IaaS.
              </li>

              <li>
                Additional cloud services include VPNaaS, FaaS, DaaS, and BaaS.
              </li>

              <li>
                Four main types of cloud computing are public clouds, private
                clouds, multiclouds, and hybrid clouds that deliver cloud
                services.
              </li>
            </ul>
          </div>
          <div class="article__part">
            <h4>Managing Cloud Resources</h4>
            <p>
              If you are considering hosting some services in the Cloud, you’ll
              need to learn what the different terms used to configure the
              services mean.
            </p>

            <p>
              When deploying a service to the cloud, you will typically create a
              number of virtual machines that will be the servers in charge of
              hosting your service. In the usual case, you would start by
              creating a single machine that will run the service, creating the
              configuration associated with the machine, verifying that it
              works, and then turning this into a template that can be used for
              the creation of many machines as needed.
            </p>

            <p>
              In order to do this, you’ll make use of both Autoscaling and Load
              Balancing. Autoscaling means being able to automatically create
              new instances when the load increases and automatically turn them
              down when the load decreases. In order for this to be possible,
              you need to ensure that your instances can be completely
              configured automatically, and that there’s no data being kept in
              the instances themselves (data can be stored in a database, or in
              separate drives).
            </p>

            <p>
              Load Balancing means distributing the load among many servers.
              There’s different approaches to doing load balancing, but the main
              concept is that there’s a load balancing service that will route
              traffic to the servers in a way that they each get to serve a
              portion of users, without the users realizing that they are
              connecting to different machines. In other words, the users will
              access a single address (e.g.
            </p>
            http://www.example.com ), which can be served by different servers,
            in different parts of the world, without the users having to care
            about that.

            <p>
              Once you have your service set up to scale automatically and
              balance the load, you’ll want to also setup Monitoring and
              Alerting for it. Monitoring means checking that the service is
              healthy, that it’s responding to queries as expected and not
              generating unusual errors. Alerting means sending alerts when
              things don’t happen as expected.
            </p>

            <p>
              For a simple service, you might go with the monitoring that is
              already built in by the cloud provider, which will allow you to
              check that your instance is healthy, but is likely not going to go
              into much detail as to whether the content is being served
              correctly. If your service is more complex, you might want to
              invest more time into making it possible to monitor additional
              parameters of your service.
            </p>

            <p>
              Depending on the specific service you are deploying, there might
              be more concepts that you need to understand before you can
              actually do it. We recommend reading the documentation offered by
              the cloud provider you have chosen to figure out what you need to
              do.
            </p>
          </div>
        </article>
      </section>

      <section class="section">
        <header class="section__header">
          <div class="container">
            <h2 class="section__title">Module 4</h2>
          </div>
        </header>
        <article class="article">
          <div class="container">
            <h3 class="article__title">Pioneers in Computing and IT</h3>
            <div class="article__part">
              <h4>What is Directory Service</h4>

Have you ever looked up someone's phone number in a phone directory or use a directory listing at a shopping mall to find a specific store, a directory server essentially provides the same functionality. A directory server contains a look up service that provides mapping between network resources and their network addresses. It's used to organize and look up organizational objects and entities ranging from things like user accounts, user groups, telephone numbers and network shares. Instead of managing user accounts and computer information locally on every machine, all that information can be stored on a directory server for easy access and management. The ideal enterprise quality directory server should support replication. This means that the store directory data can be copied and distributed across a number of physically distributed servers but still appear as one unified data store for acquiring and administering. Why is replication important? It provides redundancy by having multiple service available simultaneously. So there will be minimal disruption to the service in the event that one of the server explodes, replication also decreases latency when you access the directory service. By having replicas of your directory survey located in each office, you're able to answer directory service queries more quickly. The directory service should also be flexible allowing you to easily create new object types as your needs change. Access to the information stored in the directory server database should be accessible from a variety of OS types and from the designated areas of the corporate network. Directory services are useful for organizing data and making it searchable for an organization. This is achieved through the use of a hierarchical model of objects and containers. The containers are referred to as organizational units or OUs and they can contain objects or more organizational units. This is similar in organizational structure to a file system. OUs are like folders which can contain individual files or objects for directory service. OUs can also contain additional folders. The management benefits of this structure are pretty clear. Can you imagine trying to keep your music library organized if there was no such thing as sub folders? Crazy. This hierarchal structure can be used to convey additional information about what's stored within.
<img src="image/4/4/directory-services.jpeg" alt="">
Riproduci il video a partire da :2:23 e segui la trascrizione2:23
Take your directory structure as an example, you may have an OU called users which contains all user accounts. Within this OU there could be additional use which represent the actual team structure of your organization. The users OU could contain additional OUs like sales engineering, marketing which include the user account objects for the individuals that belong to these teams. This structure can be used to convey differences between these sub values of users. For example, we could enforce stricter password requirements from members of engineering without affecting sales and marketing. Sub members inherit their characteristics of their parent OU. So any changes made to the higher level users OU would affect all sub values including sales, marketing and engineering. Someone with the responsibilities of a systems administrator, whether that's a system admin or IT support specialist would be responsible for the setup, configuration and maintenance of the directory server. This includes the OS itself on which the directory service would run. Standard OS management tasks are involved here, like ensuring that updates are installed and configuring standard services. Other responsibilities include the installation and configuration of the directory service itself, so installing the service and configuring any related services. If multiple servers are used in a replication setup, this needs to be configured too. It's very likely that the hierarchy and overall structure of the director itself would also be up to the sys admin to design and implement. Well, that covers the high level overview of what exactly a directory service is.
Required
</div>
<div class="article__part">
  <h4>Implementing Directory Services</h4>

Directory services became an open network standard for interoperability among different software vendors. In 1988, the X.500 directory standard was approved and included protocols like directory access protocol or DAP. Directory system protocol or DSP, directory information shadowing protocol or DISP and directory operational bindings management protocol or DOP. Alternatives to DAP were designed to allow clients to access the X.500 directory. The most popular of these alternatives was lightweight directory access protocol or LDAP. Since these are open standards for communication and access for directory services, a bunch of different implementations of these services cropped up. There are offerings from Apache, Oracle, IBM and Red Hat, but we'll cover two in more detail later in this module.
Riproduci il video a partire da :1:1 e segui la trascrizione1:01
The first is Microsoft implementation which is referred to as Active Directory or AD. It has some customization and added features for the Windows platform. There are also open source implementations of directory services using LDAP. A popular example of this is open LDAP. Open LDAP supports a wide range of platforms like Windows, UNIX, Linux and various unique derivatives. In addition to the service software, there are also client tools used for accessing and administering a directory server. Microsoft Office Active Directory Users and Computers or ADUC, which works well with Microsoft's active directory server. There are also other more open tools that can be used to interface with a lot of other directory server implementations. Along with clients for administering and managing a directory server, there are also client applications that can interface with and query a directory server. All major OS platforms support integrating into a directory server for login and authentication purposes. The advantage here is that this allows for centralized management of user accounts.
</div>
</div>
</article>

<article class="article">
  <div class="container">
    <h3 class="article__title">Centralized Management</h3>
    <div class="article__part">
      <h4>What is centralized management?</h4>

The job of the systems administrator is to well, administer systems. Sysadmins have a set of systems that are responsible for, and they have to manage those systems so they're available to serve their function to the organization. For example as a sysadmin, I might be responsible for making sure that all of the service in my network are kept up to date with security patches and application updates. Should I go around and log into each server checking each one at a time? What if I need to manage user accounts on end user devices? Should I go to each employee's desk and set their account up that way? I guess I could, but that would be super time consuming and probably inconsistent. Instead, what I want to do is use centralized management. A central service that provides instructions to all of the different parts of my IT infrastructure. Directory services are one of these services. Remember in earlier lessons when you created accounts and gave them access to resources on your computer. Imagine that you work for an organization that has dozens, hundreds, or even thousands of computers and people who use them. You can't possibly go into each of those computers to set them up. Directory services provides centralized authentication, authorization, and accounting, also known as AAA. When computers and applications are configured to use directory services for AAA services, decisions about granting or denying access to computers, file systems, and other IT resources are now centralized. Now, you can create a user account once and it's available for the entire network at once, easy, well, sort of. Now, let's go one step further, let's say you have a network file system that you need to give everyone in the IT department access to. You could set up the network share then give it a list of user accounts to grant access to the share. But what happens when someone new joins the IT department? What about when someone leaves? Instead of granting access based on who you are, what if you granted access based on what you do. In most organizations access to computer and network resources is based on your role in the organization. When you manage access to resources on a computer and on the network, you'll often grant and deny access based on user groups. User groups can be used to organize user accounts in all sorts of ways. You might create groups with buildings that people work out of, or the person's role in the organization, or really almost anything else. What's important is that you use groups to organize accounts based on the way that you will manage them. If you are a systems administrator, then you might have permission to do things like creating user accounts and resetting passwords. You're allowed to do that because of your role as a systems administrator. If you add another systems administrator to your organization, you don't want to find out all of the things that a sysadmin should have access to, then grant them individual account access to each of those resources, that would just take forever. Instead, we'll create a group for sysadmins and add all system administrators to that group. Then we can give the system administrators group access to any resources they need. If you or another person change roles in the company, then all you have to do is change the groups that you're part of, not the rights that you have to directly access resources. We call this role-based access control, or RBAC. Controlling access to resources isn't all you can do. You can also centralize configuration management, just like you don't want to run around to every computer to configure user accounts. You wouldn't want to do that to set up printers, configure software, or mount network file systems. By centralizing the configuration management of your computers and software, you can create rules about how things should work in your organization. There are many ways to centralize your configuration management. An easy way to get started is with as simple a tool as log on scripts that run each time someone logs on to a computer.
    </div>
  </div>
</article>

<article class="article">
  <div class="container">
    <h3 class="article__title">LDAP</h3>
    <div class="article__part">
      <h4>What is LDAP?</h4>

Before we jump into directory services, let's talk about the underlying protocol that's used in directory services called LDAP or Lightweight Directory Access Protocol. LDAP is used to access information in directory services like over a network. Two of the most popular directory services that use LDAP are active directory and open LDAP, which we'll talk about more in upcoming lessons. There are lots of different operations you can use in LDAP. You can add a new entry in the directory server database, like creating a new user objects called Christie. You can delete an entry in the directory server database. You can modify entries and much more. When we say entry, we're referring to the LDAP entry format or LDAP notation for records in the directory service. An LDAP entry is just a collection of information that's used to describe something. Take a look at this example. Don't worry too much about what this says. The format of LDAP entries basically has a unique entry name, denoted by dn or distinguished name, then attributes and values associated with that entry. So CN is the common name of the object. In this case, since it's a person, we use Devan Sri-Tharan as the name. OU is the organizational unit such as a group and in this case, Sysadmin is used. DC is domain component. So example.com is split into example, then com.
<img src="image/4/4/ldap.jpeg" alt="">
Again, it's not necessary to remember these attributes. You can reference them in the next reading. The takeaway here is that LDAP donotation is used for entries in directory services to describe attributes using values.
    </div>
    <div class="article__part">
      <h4>What is LDAP Authentication?</h4>

Trascrizione interattiva - Attiva la modalità di trascrizione di base premendo il tasto ESC
Puoi navigare nella trascrizione utilizzando la scheda. Per salvare una nota per una sezione di testo, premi CTRL + S. Per espandere la selezione puoi utilizzare CTRL + tasto freccia. Puoi contrarre la selezione utilizzando i tasti Maiusc + CTRL + tasto freccia. Per i lettori di schermo che non sono compatibili con l'uso dei tasti freccia per le scelte rapide, puoi sostituirli con i tasti H J K L. Alcuni screen reader potrebbero richiedere l'uso di CTRL insieme al tasto alt.
Riproduci il video a partire da :: e segui la trascrizione0:00
If you were around when phone books were used, you might remember that these big old books contain the names, addresses, and phone numbers of people in your neighborhood or community who wanted their information to be publicly listed. This is way different from the phone book or contact list you have in your mobile phone. The people who are in your contacts directory gave you then phone numbers for your use only. When using LDAP, there are different authentication levels that can be used to restrict access to certain directories similar to those big public phone directories or those private mobile phone directories. Maybe you have a directory that you want to make public so anyone can read the entries in the directory or maybe you just want to keep that data private to only those who need it. We'll discuss how LDAP does this authentication and what methods it uses. We talked about the different operations you can do with LDAP, like add, remove, or modify entries in a directory. Another operation that you can perform is the bind operation, which authenticates clients to the directory server. Let's say you want to log into a website that uses a directory service. You enter your account login information and password. Your information is then sent back to the website. It will use LDAP to check if that user account is a user directory and that the password is valid. If it's valid, then you'll be granted access into that account.
<img src="image/4/4/authentication.jpeg" alt="">
You want your data to be protected, encrypted when it's completing this process. There are three common ways to authenticate. The first is anonymous, then simple, and the last is SASL or simple authentication and security layer. When using anonymous binding, you want actually authenticating at all. Depending on how it's configured, anyone could potentially access that directory, just like our public phone book example. When you use simple authentication, you just need the directory entry name, and password. This is usually sent in plain text, meaning it's not secure at all. Another authentication method that's commonly used is SASL authentication. This method can employ the help of security protocols like TLS, which you've already learned about and Kerberos, which we'll discuss in a minute. SASL authentication requires the client and the directory server to authenticate using some method. One of the most common methods for this authentication is using Kerberos. Kerberos is a network authentication protocol that is used to authenticate user identity, secure the transfer of user credentials and more. Kerberos by itself can be a complex topic that we'll revisit in the IT Security course. Once the client has successfully authenticated with the LDAP server or directory service, the user will be authorized to use whatever access levels they have.
    </div>
  </div>
</article>

<article class="article">
  <div class="container">
    <h3 class="article__title">Active Directory</h3>
    <div class="article__part">
      <h4>What is Active Directory?</h4>

Welcome back. In this lesson, we'll learn more about Active Directory or AD, the native directory service for Microsoft Windows. Active Directory has been used to centrally manage networks of computers since it was introduced with Windows Server 2000. If there are computers running Windows in your organization, then AD probably has a huge role. Active Directory works in a similar fashion to open LDAP. It actually knows how to speak the LDAP protocol and can inter-operate with Linux, OSX and other non Windows hosts using that protocol. When you use Active Directory to manage a fleet or Windows servers and client machines, it does a lot more than just provide directory services and centralized authentication. It also becomes the central repository of group policy objects or GPOs, which are ways to manage the configuration of Windows machines. Now, let's take a look at a typical Active Directory domain and see what it contains. Active Directory administration relies on a whole suite of tools and utilities. We're going to use a tool called the Active Directory Administrative Center or ADAC. ADAC is a tool that we'll use for lots of the everyday tasks that you'll learn in this course. It's great for getting work done and for learning how things work behind the scenes as you'll see. Remember that, much like fast systems, directory services are hierarchical. Everything that you see in Active Directory is an object. Some objects are containers which can contain other objects. Several of the default containers are just called containers and they serve as default locations for certain types of objects. Another type of container is called an organizational unit or OU. You can think of an OU like a folder or directory for organizing objects within a centralized management system. Ordinary containers can't contain other containers, but OUs can contain other OUs. That's a little confusing. To show you the hierarchical structure of AD better, I've clicked this button of the left-hand pane to switch ADAC to tree view. There are lots of things listed here. ADAC tells us what object each of these are and gives us a description for some of them. We're not going to work with all of these, but we want to call out some parts of the directory that are more common to work with. The very first node in this tree is our domain. A domain will have a short name like, example, and the DNS name like, example.com. Objects, particularly computers in the domain, will be given a DNS name that lives in the domain's DNS zone. There's actually one level of hierarchy above a domain that we don't see in this tool. That's a forest. If you look at the logical shape of a domain, it looks like a tree. The name, even makes sense. A forest contains one or more domains. Accounts can share of resources between domains in the same forest. In our example environment, example.com is the only domain in the forest. Our next example that we will look at is computers. This container is where new AD computer accounts are created. If I go here, you can see my computers. Computer accounts are created when a computer is joined to the AD domain. The next thing that we'll look at is domain controllers. This container is where domain controllers are created by default. Next, we'll look at users. This container is where new AD users and groups are created by default. The servers that host copies of the Active Directory database are called domain controllers or a DCs. Domain controllers provides several services on the network. They host a replica of the Active Directory database and group policy objects. DCs also serve as DNS service to provide name resolution and service discovery to clients. They provide central authentication through a network security protocol called Kerberos. Domain controllers get to decide when computers and users can log onto the domain. They also get to decide whether or not they have access to shared resources like file systems and printers. This allows system administrators to make changes to the network really quickly and easily. If someone new joins the organization, sys admins can create a user account for them and almost immediately, every device on the network knows who that person is. If someone changes jobs in the org or leaves, a sys admin can disable or delete their account and within seconds their access to devices adjust. It's common for most domain controllers in Active Directory network to be the read, write and replicas. This means that each have a complete copy of the database and are able to make changes to it. Those changes are then replicated to all other copies of the database on other DCs. Replication is usually quick and the last change wins in almost all cases. This isn't perfect, but it works for most tasks. Some changes to the AD database can only be safely made by one DC at a time. We task those changes to a single domain controller by granting it a flexible single master operations also known as FSMO. If your job will involve domain controller management, you will need to understand how to assign FSMO roles and recover from DC failure. In order for computers to take advantage of the central authentication service of AD, they have to be joined or bound to Active Directory. Joining a computer to Active Directory means two things. The first is that AD knows about the computer and has provisioned a computer account for it. The second is that the computer knows about the Active Directory domain and authenticates with it. From that point forward, the computer can authenticate to Active Directory just as any users who log on to the computers are able to.
Required
en
​

    </div>
    <div class="article__part">
      <h4>Managing Active Directory</h4>

Managing Active Directory isn't just a big topic, it's a huge topic. There are system administrators who spend all their time just managing AD. We're going to spend some time showing you some of the most common tasks that assist admin will need to do in an active directory environment. When an active directory domain is first set up, it contains a default user account, administrator and several default user groups. Let's do a run down of the most important groups. So I want to first get into my active directory window, and as you can see on an example.com, and we'll run through the user's. Domain admins are the administrators of the active directory domain. The administrator account is the only member of this group in a new domain. Remember how a local administrator or root, on the computer is able to make any changes they want to the operating system. Users in the domain admin group can make any changes they want to the domain. Since the domain can control the configuration of all of the computers that are bound to it, domain admins can become local administrators of all of those machines too. This is a huge amount of power and responsibility, so don't add accounts to this group lightly. Enterprise admins administrators of the active directory domain. They also have a permission to make changes to the domain that affect other domains in multi domain forest. The administrator account is the only member of this group in a new domain. Enterprise admin accounts should only be needed on a rare occasion, like when active directory forest is being upgraded to a new version. Domain uses is a group that contains every user account in the domain. If you want to give access to a network resource to everyone in the domain, you don't need to grant access to every individual account, you can use domain users. Each computer that's joined to the domain has an account too, so we have a default group for them also. Domain computers contains all computers joined to the domain except domain controllers. Domain controllers contains all domain controllers in the domain. I'm going to be able to do everything in this lesson because I'll be playing the role of a domain admin in my example organization. As a systems administrator our IT support specialist, you might also be a domain admin or enterprise admin. Because of the power that gives you to make changes in active directory, you should never use a domain admin account as your day to day user account. It's too easy to make a mistake that affects the entire organization. Domain admin accounts should only be used when you deliberately making changes to active directory, got it. Your normal user account should be very much like other user accounts in the domain. Where your permissions are restricted just to those resources that you need to have access to all the time. If there are some administrative tasks that you need to perform a lot as part of your day to day job, but you don't need to have broad access to make changes in AD. Then delegation is for you, just like you can set NTFS ACLs to give accounts permission in the file system, you can set up ACLs on active directory objects.
    </div>
    <div class="article__part">
      <h4>Managing Active Directory Users and Groups</h4>
      As we mentioned in an earlier lesson, a common Sys Admin task is managing the lifecycle of user accounts. User accounts are important part of IT. They identify who you are and they're used to control what kind of access you have to IT resources in your organization. Poorly managed user accounts can prevent people from doing what they need to do, which can waste time and get really frustrating. User accounts that have too much access or are no longer needed, create risk for the organization. All of this together means I'm making sure that user accounts are created, maintained, and eventually deleted is one of the most important tasks that Assist administrator can have. So let's dig in. Let's create a user account for Kristi using the Active Directory Administrative Center and put in the default user container. If we right-click on users, right here, and then click on User, we get this dialogue to create a user. There's a whole bunch of stuff here. Do we really have to fill all of this in? Thankfully, not. Only the fields that have the asterisks next to them are required. Let's go ahead and enter a full name and account name for Kristi. The full name is a person's real name. But what's users SAM account name log on. That's a very long way of saying username. The security account manager or SAM, is a database in Windows that stores the usernames and password. This is where SAM account name comes from. So let me go ahead and type in Kristi first,
Riproduci il video a partire da :1:34 e segui la trascrizione1:34
and then I'm going to type in her user account, will also name her Kristi.
Riproduci il video a partire da :1:42 e segui la trascrizione1:42
You can see that the domain is going to be part of the accounts full name. Each user account has to have a unique username within the domain. We're just going to use Kristi's first name here. Now, let's set a password for the new user account. We don't want to know Kristi's password, so we're going to make sure the option user must change password at the next login is checked, and then choose a random password for her. Like so. All right, the AD administrative console lets us create user accounts one at a time. But eventually, we're going to need to create a whole bunch of accounts at once. Imagine registration time at school or university where hundreds or thousands of new user accounts will need to be created in just a few days. You wouldn't want to do that by clicking through ADAC forms. If you know how to create a user account in the command line interface, then you can learn how to write a script that will run those commands for you over and over again. We're going to talk a lot more about scripting and automation in an upcoming lesson. For now, we just want to be able to see the commands that we'll need to do what ADAC does for us. It turns out that everything that you do in ADAC is actually done in PowerShell. Down at the bottom of the console is a Windows PowerShell history pane, that we can expand to see the commands that are being run by ADAC. It looks like ADAC run a few commands when we create a Kristi's user account. There's one other thing that I like to call out. When we need to describe the full path of an object in AD will often use L dot notation. You can see that in several of the parameters in this PowerShell history. Okay. Next, let's look at active directory groups. Remember our early discussion about groups used to grant permissions to roles, let's create a group based on Kristi's role in this fictional org. Kristi is a researcher in the Active Directory Administrative Center, right-click on the users container and select new group. We're going to call it this group researchers. So let's enter that into the group name field. See the SAM account name field again. It fills automatically when the name as we type it. You can create a group with different full name and SAM account name, but it's usually not useful to do that. What is a good idea is to provide a description of the group. That way everyone understands what the group's for. We can also add an initial set of users from the screen. But we're going to say that for the next step. For now, let's click ''Okay'', and the group will be created. The group now shows up in ADAC, and the description is right there to remind us of what the groups used for. Excellent. Now let's check a work and see how to do this in PowerShell. So I'm going to go ahead and open PowerShell and paste my command. There it is. You can see all of the settings that we filled in fields for in the create group dialogue in ADAC.
    </div>
    <div class="article__part">
      <h4>User Accounts and Groups</h4>

What are group category and group scope? If you look back at the new group window, you'll see that there are two mandatory settings that we just left as default, group type and group scope. So what are they? There are two categories of group in active directory. The most common one, and the only one that will deal with in this module is called a security group. Security groups can contain user accounts, computer accounts or other security groups. The default groups that we talked about before like domain users and domain admins are security groups. They're used to grant or deny access to IT resources. Let's say you create a human resource group, and then give that group access to a shared photo specifically for folks in human resources. The other type of group is called a distribution group. A distribution group is only designed to group accounts and contacts for email communication. You can't use distribution groups for assigning permission to resources. One reason you might use a distribution group in service security group is to create an email list that included people from outside of your domain. What about group scope? Group scope has to do with the way that group definitions are replicated across domains. Keeping a lot of large groups synchronized in very large network is a complicated problem. So active directory gives us different types of groups to manage that complexity. Most commonly, group scopes are used like this. Domain local, this is used to assign permission to a resource. An example of this would be to create a domain local group that has read access to a network share called Research Share Readers and another with write access called Research Share Writers. Global, this is used to group accounts into a role. Our example researchers group is a global group. You could have other role-based groups like sales or management. Universal, this is used to group global roles in a forest. Domain local and global groups aren't replicated outside of the domain that they're defined in, but universal groups are. In a multidomain forest, you might have a global research shared readers group in each domain and or research shared readers universal group that contains each global group as members. Universal groups are replicated to all domains in a forest. With domain local resource groups and global role groups, you can create very easy-to-understand group memberships. They very clearly describe what kind of access each role is supposed to have to each resource. So, I can add managers to the research share readers group, and researches to the research share writers group. And that's all very easy to understand. Now if we add to our new researchers group, she can write to the research chef shared folder. It's not because her user account was given direct access to the files there, but because she's a researcher. All right, let's add user accounts to our new researchers group. We can do this from the user account or from the group. Let's start from user account. In the active directory administrative center, right click on her account and select active groups.
Riproduci il video a partire da :3:8 e segui la trascrizione3:08
Now, in the enter the object names to select field, type researchers and then click OK.
Riproduci il video a partire da :3:17 e segui la trascrizione3:17
That's it. What did ADAC do in the background? ADAC used a PowerShell command that takes an active directory security principle like a user account or security group and added to its group membership. Let me add myself to the researchers group and see how that works. I'll right click on the researchers group, and then click on properties.
Riproduci il video a partire da :3:39 e segui la trascrizione3:39
Okay, and if I click on members, I can see that is a member of this group. Now, if I click on this Add button off to the right,
Riproduci il video a partire da :3:53 e segui la trascrizione3:53
It'll bring up a similar dialogue window as you saw before. Now, I'll enter my account name and click OK.
Riproduci il video a partire da :4: e segui la trascrizione4:00
System administrator, hit OK.
Riproduci il video a partire da :4:6 e segui la trascrizione4:06
Just like we made changes from the group instead of the user, so did ADAC in PowerShell. This time around, we used the PowerShell command to set or make changes to an existing AD Group. We added my account to the researchers group. Now, I'm not really a researcher in this Org. So let's go ahead and remove my account using ADAC. This time of course, I use the remove button instead of the add button, and we're done. As you can see, the only thing that changed in the PowerShell command was a single parameter to remove instead of add a member. Most people in an organization have more than one role. In our fictional company, researchers are part of the research and development department. Some network resources will be shared with all of R and D where some resources will only be made available to researchers. It makes sense for us to create a parent group for the R and D department. Let's create a global security group for this. So what we're going to do is we're going to go ahead and right click users, click new group, and they're going to go ahead and name our group name Research and Development. In the description, we're going to go ahead and write all of the members of the Research and Development group, and then we're going to hit OK. Now, our researchers are part of the R and D department. In addition to being researchers, so instead of adding each researcher independently to the research and development group, we can add the researchers group as a member. If I type research like this and hit OK, I get a list of all of the users and groups that start with research. Let's go ahead and select research and development to add the researchers group.
Riproduci il video a partire da :6:1 e segui la trascrizione6:01
What happened at the Windows PowerShell history?
Riproduci il video a partire da :6:5 e segui la trascrizione6:05
Remember, user accounts and groups of both security principles. So we use the same PowerShell command to change group membership here like we did before. So, we created a user and added them to some new groups
    </div>
    <div class="article__part">
      <h4>Managing Active Directory User Passwords</h4>
      One of the key advantages of central authentication is that it simplifies the management of passwords. Did you know that up to 25% of people forget their password at least once a day? Helping users with password issues is a common task for an IT support specialist. Once a user changes their password in active directory that changes effective on every machine that they are permitted to log onto. With certain exceptions AD doesn't store the user's password instead it stores a one way cryptographic hash of the password. The basic idea is that passwords can be easily turned into a hash but hashes can't be easily turned back into passwords. You can look up the user password even if you wanted to. That's actually a really good thing. As a general rule you should avoid ever knowing the password of another person's account. If there's more than one person who can authenticate using the same user name and password then auditing becomes difficult or even impossible. To audit your infrastructure in this sense means to analyze who perform specific actions in your IT infrastructure. In a security audit or troubleshooting scenario it's important that only one person can authenticate with their account. So keeping that in mind, how can you assist with the account passwords? The most common issue is that a person forgets the password. When this happens if you have this admin responsibilities you may be authorized to reset their password for them. This should only be done when you're absolutely sure that the person requesting the password reset is allowed to do so. Many organizations will have policies and procedures that require the request to be made in person or that the person otherwise prove that they are, who they say they are. Once you know that the password reset is authorized, you've done the hard part. The rest is simple. So let's imagine that Mateo reports that he can't sign into his account because he forgot his password. In the Active Directory Administrative Center, which I'll show you right here.
Riproduci il video a partire da :1:55 e segui la trascrizione1:55
We're going to right click on his user account.
Riproduci il video a partire da :1:59 e segui la trascrizione1:59
Let's find his user account first and then right click his account and click reset password.
Riproduci il video a partire da :2:9 e segui la trascrizione2:09
The password that we enter here will be temporary because we're going to make sure that user must change password at next login is checked. So I'm going to go ahead and create a temporary password and I want to make sure that the user must change password at next log on option is checked and then hit OK. Again we don't want to know the user's password. So I'm not going to ask Matteo for a temporary password. He might just tell me a variation of his usual password and I don't want that. Some orgs will have policies about how to generate and distribute temporary passwords. You want to make sure you know if those exist. In this example, I'm just going to generate a random password and enter it here. What's this?
Riproduci il video a partire da :2:53 e segui la trascrizione2:53
User accounts can be locked by Active Directory. If someone enters the wrong password for that account too many times it seems that this is what's happened to Matteo. Once an account is locked, nobody can authenticate with the account until the account is unlocked by an administrator. Active directory password policies control how many attempts are allowed before an account is locked out. We'll take a look at password policies here in a bit when we go over group policy objects or GPOs back to the problem at hand. I want Matteo to be able to log in and change his password so I'll make sure the option unlock account is checked and click OK. If you look at what is happening in power shell, you'll see some familiar commands along with a new one for unlocking Matteo's account. When you change your password you have to provide the current password as well as the new one. This is the sort of thing that happens when the user changes their own password. When you reset a password as an administrator, you only provide the new password and your administrative authorization overrides the existing password. If this person has used the NTFS, encrypting file system or EFS feature to encrypt files, they can lose access to those files if their password is reset.
Required
en
​

    </div>
    <div class="article__part">
      <h4>Joining an Active Directory Domain</h4>

If you have systems administrative responsibilities, you might be involved in joining machines to the Active Directory domain. Remember from our introduction to AD that computers can be joined or bound to Active Directory. Joining a computer to Active Directory means two things that AD knows about the computer and has provisioned a computer account for it. The computer knows about the Active Directory domain and authenticates with it. Over here, I'm logged into a Windows computer that isn't joined to a domain. This is called a workgroup computer. The name comes from windows work groups, which are a collection of stand-alone computers that work together. Windows work groups aren't centrally administered, so they'd become harder and harder to manage as the size of the network grows. We want central administration and authentication in our network. Let's join this computer to the domain. Let's look at the query for this first, then PowerShell. I'm going to go ahead and click "Computer" then "System properties." As you can see, this computer is under a workgroup. We need to do is we need to join this machine to the domain. To do that, I'm going to go ahead and click "Change settings." Click on "Change." In the computer name and domain changes window. You can see the computer can either be a member of a domain or a workgroup, but not both at the same time. I'm going to go ahead and select the "Domain" right here. I'm going to go ahead and enter our domain name, which is example.com. Now, when I click "Okay" this computer will reach out on the network to find a domain controller for my AD domain. Once it finds the DC, I'll be asked for a username and password to authorize the computer to be joined to the domain. I put in my domain admin, username, and password. Here, it says wallah, there you go. My machine is now joined to my domain. The domain controller creates a computer account in the domain for this computer, and this computer reconfigures itself to use AD authentication service. This will require a reboot. Let's jump over to the Active Directory Administrative Center to see what it looks like on that end. I'm at my Active Directory window and I'll go ahead and click "Computers."
Riproduci il video a partire da :2:11 e segui la trascrizione2:11
That is, I can see my computer in the computer's container. Now, my new computer will use this Active Directory domain for authentication. I can use group policy to manage this machine. We can join computers to the domain from PowerShell too. I've got this computer over here that also needs to be joined to the domain, so let's use the CLI this time. I'm going to go ahead and type in Add-Computer and DomainName,
Riproduci il video a partire da :2:41 e segui la trascrizione2:41
example.com, and server. I'm going to connect to dc1.
Riproduci il video a partire da :2:50 e segui la trascrizione2:50
That was nice and simple. Now I'm prompted for my credentials again.
Riproduci il video a partire da :2:58 e segui la trascrizione2:58
That's it. By default, this command won't automatically reboot the machine to complete the domain join. If I add the restart parameter, the command will take care of that too. One final thing. Over the years, there have been several versions of Active Directory. We refer to these versions as functional levels. An Active Directory domain has a functional level that describes the features that it supports. If you administer Active Directory. You need to know what your domain and forest functional levels are, and may someday need to upgrade your Active Directory forest or domain support new features. Let's look at the properties on this domain. This domain is a version 2016. We can also find this from PowerShell like this, some typing, get AD Forest,
Riproduci il video a partire da :3:49 e segui la trascrizione3:49
and then get AD Domain. See the forest moon and domain mode properties. Now that you know what your domains functional level is, you can find out what AD features it supports.
Required
en
​

    </div>
    <div class="article__part">
      <h4>Group Policy: Group Policy Object (GPO)</h4>

All right, now that we've joined all these computer start domain, what are we going to do with them? In this lesson, we're going to talk about how to use Active Directory Group Policy to configure computers and the domain itself. Like we mentioned before, directory service are databases that are used to store information about objects. The objects represent things in your network that you want to be able to reference or manage. One of these object types in AD is group policy object or a GPO. What's a GPO? It's a set of policies and preferences that can be applied to a group of objects in the directory. GPOs contain settings for computers and user accounts. You may want different software preferences for the marketing team, the legal team, and the engineering team. Using group policy would help standardize the user preferences for each of these teams and help make it more manageable for you to configure. Using group policies, you can create login and log off scripts and apply them to users and computers. You can configure the event log, telling the computer what events should be logged and where the logs should be sent. You can say how many times someone can enter the wrong password before their account is locked. You can install software that you want to be available and block software that you don't want to run. You heard the boss. And this is just the beginning. You can create as many group policy objects as you want, but they don't do anything until they're linked to domains, sites, or OUs. When you link a GPO, all of the computers or users under that domain, site, or OU will have that policy applied. You can use other tools like security filtering and WMI filters to make group policies apply more selectively. We'll get into that a bit. A Group Policy Object can contain computer configuration, user configuration, or both. These are applied at different times. Computer configuration is applied when the computer signs into the active directory domain. This will happen each time the computer boots into Windows unless it's disconnected from the network at the time it's booted up. User configuration is applied when the user account is logged onto the computer. In each case, once the GPO is in effect, is checked and enforced every few minutes.
<img src="image/4/4/group-policy-object.jpeg" alt="">
Remember when I said that GPOs contained policies and preferences? What's the difference? Policies are settings that are reapplied every few minutes, and aren't meant to be changed even by the local administrators. By default, policies in the GPO will be reapplied on the machine every 90 minutes. This ensures that computers on the network don't drift from the configuration that system administrators defined for them. Group policy preferences on the other hand, are settings that, in many cases are meant to be a template for settings. System administrators will choose settings that should be the default on computers that apply the GPO. But someone using the computer can change the settings from what's defined in the policy and that change won't be overwritten. How do domain joined computers actually get the GPOs? When a domain joined computer or user signs into the domain by contacting a domain controller, that domain controller gives the computer a list of group policies that it should apply. The computer then downloads those policies from a special folder called SYSVOL. That's exported as a network share from every domain controller. This folder is replicated between all of the domain controllers and can also contain things like log in and log off scripts. Once the computer has downloaded its GPOs, it applies them to the computer. Lastly, many policies and preferences in GPOs are represented as values in the Windows registry. The Windows registry is a hierarchical database of settings that Windows, and many Windows applications use for storing configuration data. The GPO is applied by making changes to the registry. The Windows operating system and Windows applications read the registry settings to determine what their behavior should be. Group policy management is another huge topic. Will only cover the basics of it in this course. Now, that you understand a little bit about what Group Policy Objects are, let's dig in and see how you use them to manage active directory and AD joined computers.
    </div>
    <div class="article__part">
      <h4>Group Policy Creation and Editing</h4>

The most important tool we'll use for creating and viewing Group Policy Object is called the Group Policy Management Console or GPMC. You can find this in the Tools menu of Server Manager or by running gpmc.msc from the command line. You can see that the layer of GPMC is similar to other management tools that we've used in Active Directory. On the left, we see the structure of Active Directory. GPMC had several containers to it's GUI. These are AD containers I'll use. There are management interfaces that only show up in GPMC. The Group Policy Objects container will hold all of the GPOs that are defined in the domain. The WMI filters container is used to define powerful targeting roles for your GPOs. These filters use properties of Windows Management Instrumentation or WMI. Objects to decide whether or not a GPO should apply to a specific computer. Group Policy results is a troubleshooting tool that's used to figure out what Group Policies apply to computer and user in your network. You would use this tool to check on Group Policies that are already applied to a computer or user. On the flip side, Group Policy modeling is used to predict which Group Policies will apply to a computer or user in your network. You use this tool if you wanted to test a change to your GPOs, OUs, or WMI filters before making real changes in your Active Directory. Remember that the users and computer containers are not Organizational Units. Group Policy Objects can only be linked to domain sites and OUs. If computer and user objects are in the default containers, they can only be targeted with GPOs that are linked to domains and sites. It's a good practice to organize your user and computer accounts in OUs so they can be targeted with a more specific Group Policies. Now, let's get started with Group Policy Objects not in GPMC, and take a quick look at a GPO that already exist. In a brand new Active Directory domain, there'll be two GPOs that are automatically created; the Default Domain Controller Policy and the Default Domain Policy. The Default Domain Policy is, as you might guess, a default GPO that's linked to the domain. It applies to all of the computers and users in the domain. The Default Domain Controller Policy is linked to the Domain Controllers OU, and applies, you guess that, to the domain controllers. What we're looking at here is a settings report for the Default Domain Policy. This GPO is designed to enforce policy decisions that we want to make for the entire domain. For example, the minimum password length policy prevents users from setting passwords that are too short. The audit account logon events policy says that the computer should create a Windows event for each successful and failed logon attempt. There are thousands of settings that can be controlled with GPO. It can take some research to find the right setting to change in a Group Policy Object to make a change that you want. Group Policy has been around through several versions of Windows, and sometimes things aren't exactly where do you expect to find them. Don't despair. There are lots of documentation online about Group Policies and where to find specific settings. Pro tip. Some thing that you might find super useful are the Group Policy settings reference that Microsoft releases with each new version of Windows. This reference is a spreadsheet that details a GPO Policies and preferences that are available and where to find them. Next, let's try changing one of the settings in our Default Domain Policy. Before we get started, I'm going to make a backup at the GPO. I'll right-click on the "Policy" and choose Backup. I've created a GPO backup folder on my desktop. But in a real environment, we'd want to create a network folder that's locked down to only allow Domain Administrators to access it. I can add a description here too to help me remember why I made the backup, then I complete the backup wizard and I'm done. Now, I know that if I make a mistake, I can restore the policy from backup. I'll right-click on the "Policy" again. But this time I'm choosing Edit. This will open up the Default Domain Policy into the Group Policy Management Editor. You can see over in the left-hand pane that the GPO is divided into two sections; computer configuration and user configuration. Each of these is divided into policies and preferences. Inside this tree of policies and preferences is every individual GPO setting that GPMC knows about. Whether it's been configured or not. Every GPO has access to the same settings that every other GPOs access to. There aren't special GPOs. Even so, it's a good practice to make different GPOs that each address a specific category of need. For example, you might have a GPO that handles all of the settings for a specific group of users, or one that handles security policies for the whole domain. With specific GPOs, for specific solutions, you can link your GPOs to only the computer or users that need that policy. Since you're working with the entire universe of Group Policy in every GPO, it can be very difficult to tell from the Editor what settings are actually configured in this GPO. Refer back to the settings report in GPMC for that information. It looks different. But you might notice that the settings report is laid out in the same hierarchal fashion as the GPO Editor. I can see that the account lockout threshold is configured to zero invalid logon attempts. Let's take a look at that policy and the GPO Editor. I'm going to use a settings report as a roadmap to finding that policy in the Editor. Let me show you how. I'm going to go ahead and right-click "Default Domain Policy, " hit "Edit," and then I'll have this to the side, so we can look at our roadmap. As you can see, computer configuration. I click "Computer Configuration," then click "Policies," Click "Windows Settings," I going to click "Security Settings," and then "Account Policies" because we're interested in the lockout policy. You can see that there are three policies under account lockout policy. The policy column tells us the name of the policy and the policy setting tells us the current configuration of the policy. If a policy is not defined, then this GPO won't make any changes to that setting on the computers that it's applied to. The policy name is pretty easy to understand, but I'm not sure that I understand all of the consequences of changing those values. If I double-click on any of these policies, it will open up the properties dialogue for that policy. Oh, what's this? There's an Explain tab here. Awesome. The Explain tab will tell us what the policy configures. It may also tell us what to expect if the policy is not defined and what the default value of the policy is if it's enabled but not customized. Looking at the explanation of the account lockout threshold policy, I see that by having it set to zero, accounts will never be disabled for failed login attempts. That's not what I want in my domain, so I'm going to change this value. Oh, interesting. It looks like this policy has some dependencies on other policies. I'm going to accept these defaults and now I'll see that all three of these policies in the account lockout policy had been configured. How do we save these changes? As soon as you hit "Apply" or "Okay" in a group policy management editor dialog, the change is made in the GPO immediately, almost right away, computers can receive the update and start applying it. That might not be what you wanted. When you need to make changes to a production group policy, you should test them first. For example, I was playing around with a default domain policy which is linked to the whole domain. I've just immediately made it so all user accounts will be locked if they enter their password incorrectly once. Whoops, where's the Undo button? Guess what? There isn't one. Don't worry. This is why we made a backup before starting to work on this policy. Let's restore the policy from backup and undo this catastrophe waiting to happen. Back in the group policy management console, I'm going to right-click on Default Domain Policy in the Group Policy Objects and then select Restore from Backup. This wizard remembers the last place that I backed up a GPO and assumes that's where I want to restore from. So intuitive. Now, it lists each of the GPO backups that are in the folder that we choose. The name of the policy and the time that it was backed up are listed here along with any descriptions that we provided when we did the backup. If I click on View Settings, it will launch my web browser with the settings report of the backup. Cool. I need to get this policy restored so my users don't get locked out of their accounts. The summary dialog shows me what I'm about to do. Let's go there. This all looks right so I'm going to click "Finish" and make sure that my policy has been restored. Perfect. My backup has been restored and my mistake has been undone. As you've seen in this example, before making changes to a GPO, you should always back it up. But what's another way I could have prevented this mistake? That's right. I could have tested my changes. There are lots of ways to do this. Some organizations will have established best practices for testing GPO changes in their environment. If that's the case, then you should follow those standards. You might need to follow a change management process too, in order to notify others in the organization about the changes that you are about to make. What I'm going to show you is just one way of adding some safety to GPO changes. Let's say I have a GPO code, example policy, like a name. I want to make changes to example policy, but I want to test the changes first to make sure that I don't break production machines. First, I set up a testing OU that contains test machines or user accounts. If example policy is usually linked at example.com finance, then computers, then I can create example.com finance computers test and put testing machines in the test OU. This lets the test machines keep all of the existing production GPOs, but gives me a place to link a test GPO that'll override production. Let me go and show you how I do that. I click "Example", click "New", click "OU" and type in finance, and click "Okay". I look for another OU for my computers.
Riproduci il video a partire da :10:46 e segui la trascrizione10:46
Then underneath that, I'm going to go ahead and make a test OU, so I can test my GPO and hit "Okay". Next, I make a copy of the GPO that I want to change and call it something like test example policy. Let me show you how I do that. This is one policy that I have. I'm going to hit "Copy", go to my Group Policy Objects, hit "Paste". Now, let's say we use default permission for the GPOs because we want to make a copy of course, and hit "Okay".
Riproduci il video a partire da :11:20 e segui la trascrizione11:20
You can see it's called Copy of Master. I'm going to rename this to Test Example Policy, Enter. Now, I can make the changes that I want to test in Test Example Policy and link it to my test OU. Let me show you how I link that. I'm going to go into my OU Finance, Computers, then right-click Test. Now, I say link an existing GPO, which is going to be my Test Example Policy right here, and then hit "Okay". After I've confirmed that my changes were the way that I expected, I can make a backup of the test policy then input the backup of Test Example Policy to the production example policy. This makes some extra work for me since I'm a systems administrator, but I also benefit from added safety and peace of mind. By testing my changes on a copy of the GPO on test machines, I make it much harder to accidentally break production machines. Your organization might be using advanced group policy management or AGPM, which is a set of add-on tools from Microsoft that give you some added revision control abilities in GPMC. If you do use AGPM in your organization, you should follow best practices for GPO version control using AGPM.
    </div>
    <div class="article__part">
      <h4>Group Policy Inheritance and Precedence</h4>
      If you follow the practice of creating specific GPO s to address specific categories of needs, you can end up with a whole lot of policies linked at many levels of your active directory hierarchy. Group policy objects that control security settings are really commonplace where this can happen. Systems administrators are responsible for protecting the security of the IT infrastructure. So it's a good practice to create a very restrictive GPO that uses very secure conservative security policies and link that to the whole domain. This gives you a secure default policy, but some uses all computers might not be able to do what they need to with those very conservative policies in place. The Finance Department might need to use Excel macros that are disabled in your default security policy, for example. So we can create GPO's that relax some of the security settings or policies in the OUs that contain those computers or users. Another example might be that we have a group policy object that standardized the desktop wallpaper of all computers, but we have computers that are public access kiosks that need to have a different wallpaper. In any of these cases, you can have computers or user accounts with multiple gps assigned to them that contradict one another by design. So what happens when there are two or more contradictory group policy objects that apply to the same computer? When computers processing the group policy objects that apply to it all of these policies will be applied in a specific order based on a set of presidence rules. GPOs are applied based on the containers that contain the computer and user account GPOs that are linked to the least specific or largest container are applied first. GPOs are linked to the most specific or smallest container are applied last. First, any GPOs linked at the AD site are applied then any linked at the domain and then any OUs in order from parent to child. If more than one policy tries to set the same policy or preferences, then the most specific policy wins. To see what I mean, let's look at this AD structure. As you can see my structure, I have multiple OUs. We have my IT OU. We have my sales OU. I also have my research OU and I also have my sites in Australia, India and North America. If you have a computer in the India site and they example dot com sales computer OU, then active directory would apply group policy objects that are linked to the India site the example dot com domain the sales OU and the computer's OU in that order. That's not although you can actually link multiple GPOs to the same container. How does AD decide which order to apply the GPOs in? If there are more than one in a container, each container has a link order for the GPOs that are linked to it. So let's look at our sales OU. The sales OU in our example domain has a GPO for network drive mapping and the GPO for configuring network printers. The link order of each policy determines which GPO takes precedence. The highest number is the lowest ranked GPO. So its settings are applied first. Network printers, sales is applied first and network drives sales is applied last. If anything, the network drive policy contradicts the network printer policy and the drive policy wins out. Let's summarize this so far. The highest numbered link order in the least specific container is applied first. And the lowest number link order in the most specific container is the last GPO applied. The last GPO to modify any specific setting wins, in the group policy management console, we can see the precedence rules in action. I'm going to switch to the computers OU in group policy management. I can see that there's a policy linked here called computer security policy to increase this. By switching from the linked group policy objects tab to the group policy inheritance. Like so I can see that objects in this, so, you will actually have quite a few policies applied. The precedence column tells us which policy will win if there are conflicting settings and the location column tells us where the policy was linked. You might have noticed that there are no site link policy listed here. That's because you can have computers from many different 80 sites in the same OU. So site based GPO links aren't represented in the summary. When you add all of the group policies together for a specific machine and apply precedents rules to them. We call that the resultant set of policy ORSOP for that machine. When you're troubleshooting group policy will often compare an RSOP report, pronounce RSOP to what you expect to be applied to that computer. There are a lot of ways to get RSOP report, we'll use the group policy management console for now and look at the other methods when we start troubleshooting. Let's check on what group policy objects will apply when one of our sales staff logs onto their computers. All right, click on the group policy results not in GPMC and select group policy results wizard. So let me go and do that. This wizard will walk me through generating a resultant set of policy report for the computer and user my choice. The computer that I'm using to run this report will make a remote connection to that computer and ask you to run the report. The report will then be visible in my local GPMC. I'd like to see that our sub when Emmett is logged into his computer which is Emmett pc 01. Let me go and do that one hit next. I want to search for his computer so I hit another computer. Hit browse and type in Emmettpc01 and hit OK. The group policy results in wizard is super simple. First I enter Emmet pc 01 as a computer that I want to run the report on, by default the wizard will only run in our report for computer configuration. Since I want to see the user configuration for Emmett to I'll select display policy setting for him which is actually already selected by default. So what we're going to do is we're going to hit next and it's going to take us to the summary of selections and then we're going to hit next. And to generate the report, we hit finish, you can only select users from this list who have already logged on to this computer in the past. That's it, I'll review my selection in the summary dialog. Then finish the wizard. I'm left with a new item under the group policy resultant nod in GPMC and it contains a resultant set of policy report that I just requested. Great, this R-SOP report contains everything that we need to understand what policies apply to a computer or user. It includes a whole lot of detail about where the computer and use are located in AD what their security group memberships are and more. I'm going to set that aside for the moment and focus on the setting sections of the report. This looks a lot like the information you see in the settings tab of GPO but instead of only showing you the settings modified by single GPO, you can see the combined effect of all of the applied GPO's. The winning GPO column tells you which GPO ultimately took precedence for each policy and preference. Amazing, right? Remember I'm making a remote request from my group policy management console to Emmett pc 01 to run this report. There are a bunch of reasons that this could fail to work. Emmet pc 01 may be powered off, it could be disconnected from the network or have firewall rules that prevent me from running the report remotely. If I am not a local administrator on the machine, I won't be able to run the report. In any of these cases, if I need that R-SOP report for troubleshooting, I might have to run commands locally on Emmett pc 01.
    </div>
    <div class="article__part">
      <h4>Group Policy Troubleshooting</h4>
      As a systems administrator or IT support specialist, you might be called on to troubleshoot issues related to Active Directory. Let's go through some of the most common troubleshooting tasks that you may encounter. This lesson will introduce you to tools that will help you troubleshoot these scenarios. Keep in mind, these are only examples. Since we're working with complex systems, there are lots and lots of ways for things to not work. Your greatest tool is to learn about these systems and understand how they function. Thoughtful troubleshooting and research are your friends. One of the most common issues you might encounter is when a user isn't able to log into their computer or isn't able to authenticate to the Active Directory domain. There are many reasons this might happen. They may have typed the password with cap locks button on. They may have locked themselves out of their computer. Accidentally changed a system setting, or it could be a software bug. It's important to think about the steps to troubleshoot and remember to ask questions about what happened. Make sure to look at the exact conditions under which the failure occurs, and any error message that accompany the failure. This should be enough information to get you started down the right path to troubleshoot. Let's just talk for a moment about the most common types of failures that can lead to a user account authentication issue. If a user enters a wrong password several times in a row, their account may be locked out. People sometimes just forget their passwords and need the assistance of an administrator to sort things out. If a domain computer isn't able to locate a domain controller that it can use for authentication, then nothing that relies on Active Directory authentication will work. Anytime you troubleshoot an issue, start with the simplest solution first. This could be a network connectivity issue and nothing specific to Active Directory at all. If the computer isn't attached to a network that can route communications to the domain controller, then this must be fixed. Any networking issue that would prevent the computer from contacting the domain controller or its configured DNS servers, which is used to find the domain controller, could be an issue. Now, why is DNS so important? In order for the computer to contact a domain controller, it needs to find one first. This is done using DNS records. The domain computer will make a DNS request for the SRV records matching the domain that has been bound to. If the computer can't contact his DNS servers, or if those DNS servers don't have the SRV records that the computer is looking for, then it won't be able to find the domain controller. The SRV record that we're interested in are _adapt._ tcp. dc._ msdcs.DOMAIN.NAME,
Riproduci il video a partire da :2:43 e segui la trascrizione2:43
where domain name is the DNS name of our domain. I'm going to go ahead to my PowerShell, and I want to go ahead and type in Resolve-DNSName Type
Riproduci il video a partire da :3:6 e segui la trascrizione3:06
SRV Name ldap._tcp.dc._ msdcs.example.com. Well, that looks good. I should see an SRV record for each of my domain controllers, and I do. Perfect. Now, if I can't resolve the SRV records for my domain controllers, then my DNS servers maybe misconfigured. How might they be misconfigured? Well, my domain computers need to use the DNS servers that host my Active Directory domain records. This will often be one or more of my domain controllers, but it can be a different domain server. Either way, the appropriate DNS servers to use for your domain computers should be known and documented. Compare the configuration of the machine to the known good configuration and see if it needs to be adjusted. On the flip side, if you're resolving some SRV records, but they appear to be incomplete or incorrect, then in-depth troubleshooting may be required. One more thing to call out. Depending on the configuration of your domain and your computers, it's common that local authentication will continue to work. For a little while, at least. Once someone logs into a domain computer, information required to authenticate that user is copied to the local machine. This means that after the first login, you'll be able to login to the computer, even if the network is disconnected. You won't be authenticated to the domain or authorized to access any domain resources like shared folders. Just because someone is able to login doesn't mean that they're able to find the domain controller. Another issue that can prevent users from authenticating has to do with the clock. Kerberos is the authentication protocol that AD uses, and is sensitive to time differences. I'm not talking about local time zones here. I mean the relative UTC time. If the domain controller and computer don't agree on the UTC time, usually within five minutes, then the authentication attempt will fail. Domain computers usually synchronize their time with domain controllers with the Windows time service. But this can sometimes fail. If the computer is disconnected from a domain network for too long, or if the time is changed by software or a local administrator to be too far out of sync, then the computer may not automatically resync with a domain controller. You can manually force a domain computer to resync by using the w32tm/rsync command.
Required
en
​

    </div>
    <div class="article__part">
      <h4>Group Policy Troubleshooting: Common Issues</h4>

Now, let's talk a bit about troubleshooting group policy issues. A common issue that you might have to troubleshoot is when a GPO-defined policy or a preference fails to apply to a computer. You might learn about this failure in a number of ways. Like a person in your organization telling you that something on their computer is missing or not working. If you're using GPO to manage configuration on your machines, then maybe there'll be a piece of software that should be present. Or there may be a mapped network drive that's missing or a number of things. The common factor will be that something that you created a GPO to configure won't be configured on one or more computers. Let's look at the three most common reasons that this might happen. The first and possibly most common type of GPO failure has to do with the way group policies are applied. Depending on how your domain is configured, the Group Policy Engine that applies policy settings to the local machine may sacrifice the immediate application of some types of policies in order to make log on faster. This is called fast log on optimization. It can mean that some GPO changes take much longer to be automatically applied than you might expect. Also, the Group Policy Engine usually tries to make GPO application faster by only applying changes to a GPO instead of the whole GPO. In either of these examples, you can force all GPOs to be applied completely and immediately with gpupdate/force. If you want to be really thorough, you can run a gpupdate/force/sync.
Riproduci il video a partire da :1:37 e segui la trascrizione1:37
Adding the slash sync parameter will make you log off and reboot the computer. Some types of group policy can only run when the computer is first booted or when a user first logs on. A log off and reboot is the only way to make sure that a forced updated GPO has a chance to apply all of the settings. Replication failure is another reason that a GPO might fail to apply as expected. Remember that when changes are made to Active Directory, those changes usually take place on a single domain controller. Those changes then have to be replicated out to other domain controllers. If replication fails, then different computers on your network can have different ideas about the state of Directory objects, like group policy objects. The log on server environment variable will contain the name of the domain controller that the computer used to log on. Remember that you can see the contents of the variable with this command in PowerShell, which is dollar sign environment call on log on server, and shows me DC1. You can also get the same results using command prompt, which uses percentage login server percentage. Knowing which domain controller you're connected to is useful information to have if you suspect a replication issue. From the Group Policy Management Console, we can check on the overall health of the group policy infrastructure. I'm going to select my domain and take a look at the Status tab. This tab will summarize the Active Directory and assist vault replication status for the domain. It may be showing result from a recent test, so I'm going to force it to run a new analysis by clicking on "Detect Now." What we want to see is that all of our domain controllers are listed under domain controllers with replication in sync. If they are, then we can be sure that there are no replication issues that will affect our group policy objects. If we do see any domain controllers in the domain controller with replication in progress list, then we may have a replication issue. Depending on the size and complexity of your Active Directory infrastructure and the reliability and throughput of the network links between your AD sites, it's possible for replication to take a few minutes to complete. If replication doesn't complete in a reasonable amount of time, you may need to troubleshoot Active Directory replication. If you're trying to work out why a particular GPO isn't applying to a computer, the first thing to do is to run the resultant set of policy or RSoP. You can use the Group Policy Management Console, or you can run a command on a computer directly to generate the report. The GP result command will help us out there. If I run gpresult forward /R, you can see that I get a summary report in my terminal. Let me go and show you. Gpresult forward /R. Report is being created, and I get this report. If I want the full report, like I get for my GPMC, I can run gpresult forward /H FILENAME.html. I want to do gpresult forward /H, and then test.html. This will give me a report that's an HTML web page that I can open in my browser. Let me go and get that. With this report in hand, I want to look for some things. Is the GPO that I want to apply listed? Was it linked to an OU that contains the computer that I'm troubleshooting? Is the GPO that I care about listed under applied GPOs or under denied GPOs? If it was denied, what was denied reason? Did another GPO win for the policy of preference that I'm trying to configure? Each GPO can be configured with an alkyl called a security filter. Is the security filter set to something besides authenticated users? If so, then that may mean that you have to be in a specific group in order to read or apply the GPO. Each GPO can also be configured with a WMI filter. A WMI filter lets you apply a GPO based in the configuration of the computer. WMI filters are powerful but expensive and easy to misconfigure. This is because they look at Windows Management Instrumentation values to decide if a policy should apply or not. For example, you could create a GPO that instills a piece of software, but only if a WMI reports that a specific piece of hardware is present. These filters are expensive because they require the Group Policy Engine to perform some query or calculation on every computer that is linked to the policy. But then only apply the GPO to computers that match the filter. Many policies and preferences can be configured to apply to the computer or to use it as they log-on. Did you mean to configure a computer setting but accidentally configure a user setting or the reverse? We've really covered a lot out here. If you aren't clear on any of the concepts we've covered, that's okay, just make sure to re-watch the lessons. Remember though, that the more you work with Active Directory and the Group Policy, the more familiar you become with them. If you use what you've learned about these systems combined with your research skills, you can troubleshoot just about anything.
Let's look at the three most: aggiunto alla selezione. Premi [CTRL + S] per salvare come nota.
Required
en
​

    </div>
  </div>
</article>

<article class="article">
  <div class="container">
    <h3 class="article__title">OpenLDAP</h3>
    <div class="article__part">
      <h4>What is OpenLDAP?</h4>
      In the last lesson, you dove headfirst into the popular directory service, Active Directory. You learnt how to add users password, groups, and even modify access level for groups using group policies. Another popular directory service that's used today is the free and open source services, OpenLDAP, which stands for Lightweight Directory Access Protocol. Operates very similar to Active Directory. Using LDAP notation or LDAP data interchange format or LDIF, you can authenticate, add, remove users, groups, computers, and so on in a directory service. Open LDAP can also be used on any operating system, including Linux, macOS, even Microsoft Windows. However, since Active Directory is Microsoft's proprietary software for directory services, we recommend that you use that on windows instead of opening an LDAP. But it's helpful to know that open LDAP is open source, so it can be used on a variety of platforms. There are a few ways you can interact with an open LDAP directory. First, you can use the command line interface and passing commands to create and manage directory entries. You can also use a tool like phpLDAP admin, which offers you a web interface that you can use to manage your directory data. Much like the Windows, Active Directory GUI that you're familiar with, in this lesson, we'll give you a high-level overview of the operations you can do in open LDAP via commands and how they work. To begin, we'll just open the open LDAP package using this command. I want to get into my Linux environment, and type of this command, sudo apt-get install slapd ldap-utils,
Riproduci il video a partire da :1:47 e segui la trascrizione1:47
my password in and accept. Once you install the packages, it will prompt you to enter in an administrator password for LDAP. Let's go ahead and do that. Then hit "Okay". Then confirm your password, then hit "Okay".
Riproduci il video a partire da :2:9 e segui la trascrizione2:09
Now that it's installed, we're actually going to reconfigure the slapd package so that we can fine tune our setting. To do that, we're going to run the following command. I'm going to clear my window and then run sudo dpkg-reconfigure slapd. This is going to ask us a bunch of questions about our new setup. The first option is omit open LDAP server configuration. I'm going to go ahead and say no. Next, DNS domain name, is similar to Windows AD. This is our organization domain, let's use example.com, and then hit "Okay". Organization name, let's use example. Administrator your password, it's just the same thing that we entered before. Through the database, let's use MDB. Do you want the database to be removed when slapd purge, let's go ahead and say no. That's asking us if you would like to move to the old database, we're going to say yes for now. They'll say allow LDAP Version 2 protocol. We're going to say no. That's it. Now you have a running open LDAP server. We're really cooking now, let's keep going.
    </div>
  </div>
</article>
</section>

<section>
  <header class="section__header">
          <div class="container">
            <h2 class="section__title">Module 5</h2>
          </div>
        </header>
        <article class="article">
          <div class="container">
            <h3 class="article__title">Planning for Data Recovery</h3>
            <div class="article__part">
              <h4>What is Data Recovery?</h4>

What exactly is data recovery? If you've ever broken a cell phone, you probably lost some good pictures along with the phone itself. Data recovery is a process of attempting to recover the data that's lost from the broken phone. But this is just one example of attempting to recover from unexpected data loss. Data recovery, in general terms, is the process of trying to restore data after an unexpected event that results in data loss or corruption. Maybe a device that contains data was physically damaged, or attacker perform malicious actions, or malware deleted critical data. Whatever the cause, the effect is the same. You suddenly lost some really important data and you need to figure how to get it back. How you go about trying to restore this lost data depends on a few factors. One is the nature of the data loss. If a device has been damaged, you might be able to recover data from the damaged hardware. This could involve using data recovery software, which can analyze failed hard disks or flash drives and try to locate and extract data files. Another factor that would affect your data recovery is the presence of backups. If you're lucky or you had the foresight to plan for the unexpected, you have data backed up, and you can restore the data that was lost. Data recovery is an important part of an IT system or organization. Since data is critical component of any business operations, as an IT support specialist, part of your role is to ensure that this data is available and protected from corruption or loss. So if something goes wrong, the organization can continue with their business operations with minimal disruptions. That's why it's critical to be able to recover from unexpected events that could impact your business data. When an unexpected event occurs, your main objective is to resume normal operations as soon as possible while minimizing the disruption to business functions. By the end of this module, you'll have practical tools and methods that you can use to protect your data. One of the most important techniques you'll learn is how to effectively back up your data. The best way to be prepared for a data loss event is to have a well-thought-out disaster plan and procedure in place. Disaster plans should involve making regular backups of any and all critical data that's necessary for your ongoing business processes. This includes things like customer data, system databases, system conflicts, and financial data. You'll learn more about how to design and implement a data disaster plan throughout this module. And lastly, you'll learn more about what IT folks call a post-mortem. Imagine that something did go wrong with your systems and you had to use a disaster plan. You might have discovered issues when recovering your data that wasn't covered in the disaster plan. A post-mortem is a way for you to document any problems you discovered along the way, and most importantly, the ways you fix them so you can make sure they don't happen again. Being unprepared for a major data loss event can and has severely impacted businesses.
            </div>
            <div class="article__part">
              <h4>Backing Up Your Data</h4>

You want to protect your organization from critical data loss, good instincts but where do you start? Let's run down some of the key things to keep in mind when designing a data backup and recovery plan. The first thing to figure out is what data you need to backup. In a perfect world, you should only be backing up data that's absolutely necessary for operations and can be found in another source. Things like emails, cells, databases, financial spreadsheets, server configurations, and databases should all be included. But what about the downloads directory on your laptop isn't really necessary to backup all those cat pictures too? Probably not.
<img src="image/4/5/backup-system.jpeg" alt="">
Backing up data isn't free every additional file you backup takes up a little more disk space, increasing the overall costs of your backup solution. Once you've figured out what data do you like to backup, you should find out how much total data you currently have. But it's not enough just to think about what your backup storage requirements are right now, your organization may continue to grow and your backup needs should grow with it. Make sure that you account for future growth and choose a solution that's flexible enough to easily accommodate increases in data backups. Data can be backed up either locally to systems on-site or the backup data can be sent off-site to remote systems. Both approaches have positives and negatives and can help reduce different risks. The advantage of on site backup solutions is that the data is physically very close. This makes accessing the data a lot quicker, you won't need as much outbound bandwidth since you aren't sending the data out of your internal network. If you need to restore data from backups, that shouldn't happen pretty quickly since the data is close at hand. But what if the unexpected event is a building fire? Now the systems we were backing up along with the backup server had been lost in the fire. Yikes. We've lost everything, this is why off-site backups are strongly recommended. This involves making backups of critical data than sending the backup data off-site to remote systems in a different physical location. This could be another backup server that you control in a different office or a Cloud hosted backup service. But there are trade-offs. Off-site backups better prepare us for catastrophic events that can wipe out data from an entire office. But sending data offsite means that you need to transmit the data outside of your network, this means you need to consider things like encryption and bandwidth. Your Internet connection will be used to transmit the backup data depending on how much data you're sending off-site, and how fast the Internet connection is. This could take a lot of time. Another important thing to consider is encryption of backups. Since backups will often contain sensitive and confidential business data, it's important the data is handled securely and stored in a way that prevents unauthorized access. When sending data offsite is especially important to make sure that data is being transmitted securely, preferably encrypted via TLS. But that's not all, the resulting backup data that's stored should also be encrypted at rest. This is just good security practice.
            </div>
            <div class="article__part">
              <h4>Backup Solutions</h4>
              You're looking to bring a backup solution into your organization, but how do you choose between a DIY backup system or one of the many cloud providers? Well, let's start by looking at the trade-offs between the two. On-site or self-managed backups could be as simple as buying commercial NAS device, loading it with a bunch of hard drives and sending data to it over the network. This would definitely work, but it might not be the best long-term solution. How do you grow the disk capacity when you need more storage space? How do you handle the failed hard disk? Because hard disks will fail eventually. By the way, it's important to call out these are options aren't mutually exclusive. There's nothing stopping you from implementing both on-site and off-site backups actually is often recommended to have both if it's within your organizations budget. One more thing that you should consider when evaluating the backup strategy for an organization is backup time period. How long do you need to hang onto backups for? This answer will impact your long-term storage needs and overall cost to maintain a backup system. One approach which balances cost with convenience is to archive older data using a slower but cheaper storage mechanism. The standard medium for archival backup data storage is data tapes. These are a lot like audio cassette tapes, since they use pools of magnetic tape run through machines that allow data to be written to and read back from the tape. Tape storage is cheap, but isn't as easy or quick to access as data stored on hard drive or solid state drives. This storage system is usually used for long term archival purposes where data isn't likely to be needed. If it is needed, some delay in getting the data isn't a concern. There are dozens and dozens of backup solutions available. We won't cover specific ones since they're way too many, but we'll cover some common tools and give you some examples of backup solutions available. One is the command line utility rsync. Rsync isn't explicitly a backup tool, but it's very commonly used as one. It's a file transfer utility that's designed to efficiently transfer and synchronize files between locations or computers. Rsync supports compression and can use SSH to securely transfer data over a network. Using SSH, it can also synchronize files between remote machines, making it super useful for simple automated backups. Apple has a first-party backup solution available for their Mac operating systems called Time Machine. It operates using an incremental backup model. Time machines supports restoring an entire system from backup or individual files. It even allows restoring older versions of backup files. Microsoft also offers a first-party solution called backup and restore. This has two modes of operation, is a file-based version where files are backed up to a zip archive or there's the system image where the entire disk saved block-by-block to a file. File-based backup support either complete backups or incremental ones. System image backup support differential mode, only backing up blocks on the disk that have changed since the last backup.
            </div>

            <div class="article__part">
              <h4>Testing Backups</h4>

There's one last super important topic when it comes to backups, testing them. The field of IT is littered with tragic tales of IT support specialist and sysadmins attempting to restore data from a backup after a data loss incident, only to discover that their backups aren't valid. That's not just embarrassing, it's completely terrifying. The takeaway here is that it isn't sufficient to just set up regular backups. That's only half of the equation. The other half is a recovery process, and that process needs to be tested regularly. Restoration procedures should be documented and accessible so that anyone with the right access can restore operations when needed. You don't want your time off to be interrupted because your colleague back at the office doesn't know how to restore the SQL database from the backup, right? Of course not. Document the procedure and make sure you regularly test documentation to make sure it works now and in the future. This process is called Disaster Recovery Testing and is critical to ensuring a well-functioning Recovery System. Disaster recovery testing should be a regular exercise that happens once a year or so. It should have different teams, including IT Support Specialists going through simulations of disaster events. They'll test and evaluate how well-prepared or unprepared your organization is for lots of unexpected events. Whatever the scenario, it'll help your IT teams test their emergency procedures and figure out what works and most importantly, what doesn't. These simulated events are the perfect way to discover any gaps in your planning. If you discover that you aren't protected from data loss in any given scenario, it's an opportunity to learn and fix this gap without risking real data loss. Sounds like a win-win, doesn't it?
            </div>
            <div class="article__part">
              <h4>Types of Backup</h4>

So we've talked about how important backups are and why you should be backing up any important data, and some tools that can use to help you back up data. But how exactly do you decide when and how to back up data? Well, let's explore those options. There's a couple of ways to perform regular backups on data that's constantly changing. You can do a full backup on a regular basis which involves making a copy of the data to be fully backed up. The full unmodified content of all files to be backed up is included in this backup mechanism whether the data was modified or not. In the case of data that doesn't change very often like operating system configuration files, this approach can be inefficient. You're backing up multiple copies of data that isn't changing which way space and uses bandwidth unnecessarily. That doesn't seem like the best idea, does it? A more efficient approach is to only backup files that have changed or been created since the last full backup. This is called a differential backup. The advantage is that you aren't storing backups of duplicated unchanging data. Only the files that changed are backed up, saving us some storage space and time to form the backup. But you wouldn't want to completely stop taking full backups. Over time, you wind up tracking and storing lots of copies of files that change a lot, which will also take up more and more disk space over time. To avoid this, is a good practice to perform infrequent full backups while also doing more frequent differential backups. How often you perform a full backup will depend on how far back you want changes to be tracked. Let's say we perform full backups once every week and differential backups daily. The worst case scenario would lose close to 24 hours of data changes. That's not bad. Another efficient way to back up changing data is to perform regular incremental backups. While the differential backup backs files that have been changed or created, an incremental backup is when only the data that's changed in files is backed up. This is even more efficient in terms of both disk space and time required compared to differential backups. Again, you want to use frequent incremental backups along with less frequent full backups. But because this approach only source differences in the files that have changed since the last incremental backup, it's possible that all incremental backups are needed to fully reconstruct the files. If one of these incremental backups is missing or corrupt, it might not be possible to recover data any more recently than the last full backup. Another drawback is that recovery might be more time consuming. This is because the most recent version of backed up data has to be recreated by integrating the last full backup with each incremental backup that follows. The super large files that are changing frequently, this could require a lot of time to process. One more thing backup systems can do to help save space is file compression. When creating a backup, all the files and folder structures will be copied and put into an archive. Archives are useful for keeping files organized and preserving folder structure. Besides archiving the files, backups can also be compressed. This is a mechanism of storing the same data while requiring less space by using complex algorithms. Those are way too complicated to go into detail here. But it's supported to call out that not all data types lend themselves to being compressed. This means that space savings from compression will depend on what you're backing up. Another thing you should know about compressing backups is the expense of restoration. To recover data from a backup, it needs to be decompressed first. Depending on the size of your backups, this could take a lot of time and this space to expand. You can use a commercial NAS device or configure a fast server with a large amount of disk space. Wherever you choose to store your backup data, you need a lot of space. You could go out and buy a giant ten-terabyte hard disk which could work for a little while. But, what do you do once your backup data grows to fill that one disk? Are they even making disks larger than ten terabytes yet? Another thing to worry about is what you do if that one disk coding or your backed up data fails. Yikes, that wouldn't be good. These are issues a RAID array can address. RAID stands for redundant array of independent disks. It's a method of taking multiple physical disks and combining them into one large virtual disk. There are lots of types of RAID configuration called levels. Depending on the characteristics desired from the array, various RAID levels prioritize features like performance, capacity, or reliability. RAID arrays are a great inexpensive way of creating a lot of data capacity while minimizing risk of data loss in the event of disk failure. They can even be flexible enough to allow future growth in disk capacity. I want to stress the fact that RAID isn't a backup solution. It's a data storage solution that has some hardware failure redundancy available in some of the RAID levels. But storing data on a RAID array doesn't protect against accidentally deleting files or malware corrupting your data. This is so important that I want to say one more time, RAID is not a replacement for backups.
Required
en
​

            </div>
            <div class="article__part">
              <h4>User Backups</h4>

As an IT support specialist working closely with the users in your organization, the topic of user backups is sure to come up. We've already covered backing up mission critical operational data, but what about the spreadsheets and PDFs on Carly's laptop? She's going to want to make sure that she doesn't lose those if her laptop gets stolen. While it's important to have a backup solution for infrastructure and critical systems, you also have to think about your users and their valuable files. Ensuring reliable backups for client devices is a bit more challenging than infrastructure devices. There are likely to be lots of more client devices to back up compared to infrastructure ones. Plus, there are laptops, phones and tablets that won't be in the office all the time. One solution to user backup is to use a cloud service designed for syncing and backing up files across platforms and devices. Some examples of these are things like Dropbox, Apple, iCloud and Google Drive which are simple and straightforward to use. There's no complicated scheduling or configuration compared to infrastructure backups. They make it easy for users to configure what files or folders they want to have backed up and then ensure the files are synchronized with what's stored in the Cloud. As an IT support specialist, this is especially relevant. When users accidentally spill a cup of coffee on their laptop, they're going to come to you, helping their precious family photos can be saved. Getting users setup with an easy to use an effective backup system for their files is a great way to avoid this situation.
Required
en
​

            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Disaster Recovery Plans</h3>
            <div class="article__part">
              <h4>What's a Disaster Recovery Plan?</h4>

We've all experienced an accident or made a mistake before. We're only human. As an IT support specialist, it's important that you're prepared for a disastrous accident or mistake. Maybe an electrical surge fried the hard drive in a database server or someone deleted the wrong folder. These things happen and you need to be prepared for them. This is where the disaster recovery plan comes in. A disaster recovery plan is a collection of documented procedures and plans on how to react and handle an emergency or disaster scenario from the operational perspective. This includes things that should be done before, during, and after a disaster. The goal of the disaster recovery plan is to minimize disruption to business and IT operations by keeping downtime of systems to a minimum and preventing significant data loss. Despite the name, a disaster recovery plan will actually cover preventive measures and detection measures on top of the post-disaster recovery approach. Preventive measures cover any procedures or systems in place that will proactively minimize the impact of a disaster. This includes things like regular backups and redundant systems. Anything that's done before an actual disaster that's able to reduce the overall downtime of the event is considered preventative. For example, a standard for critical and network infrastructure or service to have redundant power supplies. They're often fed from different power sources like battery backup. This is designed to minimize the downtime that would be caused by one power supply failing or a power outage. That would be a preventative measure. Detection measures are meant to alert you and your team that a disaster has occurred that can impact operations. Obviously, you need to be aware of a disaster in order to take appropriate steps to reduce the impact on your organization. Timely notification of a disaster is critical since some steps of the disaster recovery plan might be time-sensitive to ensure there's no data loss or equipment damage. If there's a power outage, for example, critical systems should fall back to battery power, but battery backup power will only keep the systems online for so long. To avoid potential data loss or damage, these systems should be gracefully shut down before they completely lose power. This is why lots of systems that support redundant power supplies also have a function to send alerts on power loss events. Other things that should be monitored to help head off any unexpected disasters include environmental conditions inside server or networking rooms. Flood senses can alert you to water coming into the server room. Temperature and humidity centers can watch for dangerously high temperatures or sub-optimal moisture levels in the air. This can alert you to failed cooling in the server room. Smoke detectors and fire alarms are also critically important. Remember that it isn't just the technology that's necessary for a business to continue to operate, you also need the people working there. The specifics of building evacuation usually fall to the building management team. But as an IT support specialist who will likely work closely with some members of this team on things like power delivery, heating and cooling systems, and building evacuation, if there's a fire and the building needs to be evacuated, you should be prepared to set up temporary accommodation so people can still work effectively. This might be as simple as sending everyone home for the day and having them work from home, but you should be prepared for the situation ahead of time and have a plan in place to ensure that everyone is able to work from home effectively. Finally, corrective or recovery measures are those enacted after a disaster has occurred. These measures involve steps like restoring lost data from backups or rebuilding and re-configuring systems that were damaged. Once the disaster has been detected and steps have been taken to either prevent an outage or at least minimize downtime, work should begin on restoring full operations of everything affected. Usually, a disaster will take out one system that's part of a redundant pair or a replication scheme, which would prevent a complete service outage. But that would mean you aren't prepared for another disaster. When one system in a redundant pair suffers a failure, it's called a single point of failure. This is because it only takes one failure now to completely take the system down. That's not a scenario you want to be in. We've covered the overall elements of a disaster recovery plan. Why don't you take a little break to recover yourself then join me in the next video.
            </div>
            <div class="article__part">
              <h4>Designing a Disaster Recovery Plan</h4>

So, what exactly goes into an effective disaster recovery plan? Well, it depends. Keep in mind there's no one size fits all for disaster recovery plans. The mechanisms chosen and procedures put in place will depend a lot on the specifics of your organization and environment. But we can start by covering the three types of measures in more detail. We'll also go over some examples to help give you an idea of what to think about.
<img src="image/4/5/disaster-recovery-plan.jpeg" alt="">
A good way to understand what to plan for is to perform a risk assessment. This involves taking a long, hard look at the operations and characteristics of your teams. A risk assessment allows you to prioritize certain aspects of the organizations that are more at risk if there's an unforeseen event. Risk assessment can involve brainstorming hypothetical scenarios and analyzing these events to understand how they'd impact the organization and operations. When you look into preventive measures, pay attention to systems that like redundancy. If it's something critical to permit operations, it should probably have a redundant spare just in case. Make sure you have a sound backup and a recovery system along with a good strategy in place. Ideally, you should have regular automated backups to backup systems located both on site and off site. It's also critical that you have data recovery procedures clearly documented and kept up to date. Data recovery following a disaster is hopefully something you'll rarely do. It's important to make sure that these procedures are updated since systems change and evolve throughout their lifetime. Redundancies shouldn't be limited only to systems, anything critical to operations should be made redundant whenever possible. This includes power delivery or supply, communication systems, data links and hardware. Think through the impact that a disaster affecting each of these aspects would have on your day to day operations, what would happen to your network if the building lost power? Can you continue to work at the fiber optic data line for the building gets damaged by a nearby construction work? Your call rautive for the office just burst into flames. Okay, the last one is a little far fetched, but I think you get where I'm going. Another super important preventive measure that should be evaluated and verified is operational documentation. Make sure that every important operational procedure is documented and accessible. This includes things like setting up and configuring critical systems in infrastructure. Any steps or specific configuration details that are needed to restore 100% functionality to core systems and services should be documented in detail. It's also important that this documentation is kept up to date. An effective way to do this is periodically verify that the steps documented actually work. You don't want to find yourself in a situation where you need to reconfigure a system following incident only to discover that the documentation is wrong. When looking at detection measures, you'll want to make sure you have a comprehensive system in place that can quickly detect and alert you to service outages or abnormal environmental conditions. If uptime and availability is important for your organization, you'll likely have two internet connections, a primary and a secondary. You'll want to monitor the connection status of both of these links ideally, they should be configured to automatically fail over if one goes down, but you'll still want to be alerted when this happens. So you can investigate why the link went down and work on getting it back as soon as possible. The way to think about designing detection measures is to evaluate what's most critical to the day to day functioning of the organization. What infrastructure services and access are absolutely vital? Those are the ones you should be closely monitoring and it's not just outages or complete failures you should be watching for. Of course, you'll want to monitor for those, but you also want to monitor the conditions that indicate that a problem is likely to occur. If we can avoid a catastrophic failure by being alerted to an overheating server before it fails, that would be much better, wouldn't it? So, you want to monitor conditions of service and infrastructure equipment, things like temperature, CPU load and network load. For a service monitoring for error rates and requests per second will give you insight into the performance of the system. You should investigate any unusual spikes or unexpected increases. These early warning systems allow you to head off disaster before it brings operations to a halt. And of course, you absolutely must test these systems, simulate the conditions of your monitoring systems are designed to catch, make sure the detection thresholds actually fire the alerts like they're supposed to. This testing goes beyond just the monitoring systems too, you'd also want to test your reactions and responses to these alerts. If you're monitoring systems reliably trigger alerts, but everyone just ignores them, they aren't super useful, are they? You want to conduct regular disaster test to make sure systems are functioning and that your procedures to handle them are also up to the task. Corrective or recovery measures include actions that are taken to restore normal operations and to recover from an incident or outage. This includes steps like restoring a corrupted database from a backup or rebuilding and reconfiguring a server. Your disaster plan should include reference or links to documentation for these types of tasks, anything and everything that would be required to restore normal operations following some disaster. This is where the steps for restoring various systems and data from backups should be. The disaster recovery doc doesn't need to contain the details of the operations. Links and references are sufficient, but it's important to be prepared for a situation where typical documentation access methods are unavailable. Let's imagine you keep all your operational documentation in a wiki on a dedicated server. What happens when that server suffers an outage? It's important that critical documentation is accessible if an emergency scenario or disaster strikes.
Required
en
​

            </div>
          </div>
        </article>

        <article class="article">
          <div class="container">
            <h3 class="article__title">Post-Mortems</h3>
            <div class="article__part">
              <h4>What's a post-mortem?</h4>

Making mistakes is part of being human. We all make them from time to time. And hopefully each mistake is a learning opportunity. We look at what happened and try to understand why it did to avoid it in the future. Think of a toddler learning the stove is hot, they touch it once and it burns them. They quickly learn that touching the stove hurts so they stop doing it. It's important that we're able to learn from our mistakes, or else we'll constantly be burning ourselves. That's the purpose of a post-mortem report. We create a post-mortem after an incident, an outage, or some event when something goes wrong, or at the end of a project to analyze how it went. This report documents in detail what exactly happened leading up to during and after the event or project. It tries to highlight things that went well and things that didn't. The purpose of a post-mortem is to learn something from an event or project, not to punish anyone or highlight mistakes. Sure there will likely be some mistakes that will be addressed, but the intention isn't to punish or shame. It's meant to understand the root cause of why the mistakes happen and how to prevent them from happening again. The post-mortem process isn't quite over yet. Once the report is written up, it needs to be communicated. These type of reports have value beyond the immediate people involved in the incident, investigation and writing of the report. Sharing post-mortems with other teams at an organization helps encourage a culture of learning from mistakes. It shows that your team is willing to acknowledge when you mess up. But you don't let mistakes hold you back. It's just a good example for others to follow. And the content in the report might trigger a thought for some other team and make them realize they have a similar problem in their infrastructure. You may also identify areas that could be improved, that are the responsibility of teams that weren't involved in the incident. When it comes to post-mortems, sharing is caring. If you embrace post-mortems, they tend to foster a culture where it's okay to make mistakes. This is a healthy attitude to have in any organization. If people are afraid to make mistakes, then decision making will be very conservative. It's hard to push the boundaries, and try new things if everyone is afraid of screwing up. But, if the prevailing culture says that mistakes are okay as long as we can learn from them, then you'll have an organization that's willing to take risks and try out new and exciting ideas.
            </div>
            <div class="article__part">
              <h4>Writing a Post-Mortem</h4>

Now that we know the benefits of a post-mortem, let's take a closer look at what goes into one.
<img src="image/4/5/summary-timeline-case.jpeg" alt="">
A typical post-mortem for an incident begins with a brief summary, just a short paragraph that summarizes the incident. It should include what the incident was, how long it lasted, and what the impact was, and how it was fixed. Be mindful of time zone when listing times and dates in the post-mortem. Always clearly state the time zone to be absolutely clear. Next, you need a detailed timeline of key events. This should include everything that happened throughout the event, like when it started and when people involved were notified or realized what was going on. Every action taken in an attempt to resolve the situation should also be identified. These could contain times and dates along with time zones and who did what. The timeline should wrap up with the actions taken that resolve the outage and restored services signaling the end of the event. Next, a very detailed and honest account of the root cause is covered. This should go into detail explaining what led to the issue. It could be something like a configuration change that was pushed live without proper testing or a command that was typod. Remember that the intent isn't to blame or shame, it's to be honest with what went wrong so that a lesson can be learned from it. If the cause stemmed from a lack of testing, this might indicate areas for improvement in task verification. If it was from a typod command, this may reveal the need to automate a manual process. A more detailed explanation of the resolution and recovery efforts should be documented next. This is similar to the timeline covered earlier and should include the dates, times, and time zones. But it should go into more detail about what steps were taken to recover the rationale and the reasoning behind those actions, and what the outcome of each step was. Including the rationale gives those reading the report more context on how the event played out. Lastly, close out the report with a list of specific actions that should be taken to avoid the same scenario from happening again. This should include any actions or efforts aimed at improving the response handling too. Steps to reduce response timeline enhancements to monitoring would help. When you spin up a list of things to improve, lookout for things like improvements to monitoring systems. Maybe the incident investigation revealed a gap in visibility into critical systems, or the cause investigating it turns out that automation system isn't functioning as intended. While it's outside the scope of a postmortem report to come up with solutions to these gaps, there should be listed areas for improvement. Based on these discoveries, new parties can start to address the deficiencies found. One thing that often gets overlooked in post-mortem is what went well, but this is just as important as analyzing what went wrong. During the post incident analysis, it's also good to highlight things that went well. These include fail-safe or fail-over systems that work as designed and prevented a large outage or minimized the severity of the outage. This helps to demonstrate the effectiveness of our systems in place. For some folks, like those in finance, this is good news. It justifies any cost associated with these systems by clearly demonstrating a tangible benefit. This is important for any preventative system since they are frequently viewed as unnecessary costs by those that may not fully understand their benefit. These examples of systems working to prevent or reduce the impact of outages make that benefit very clear. Hopefully, you're better prepared now to learn from any mistakes you might make in your career, and mistakes we made because you're human, and after all, own them, learn from them and do better next time. Such is true in IT, work and in life.
Required
en
​

            </div>
          </div>
        </article>
</section>
en
​

            </div>
          </div>
    </main>
  </body>
</html>
